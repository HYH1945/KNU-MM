#!/usr/bin/env python3
"""
ë©€í‹°ëª¨ë‹¬ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ - ì›¹ìº  ë²„ì „
ìŒì„± + ì›¹ìº  ì‹¤ì‹œê°„ ì˜ìƒì„ í•¨ê»˜ ë¶„ì„í•˜ì—¬ ë” ì •í™•í•œ ìƒí™© íŒë‹¨

ì‚¬ìš©ë²•:
  python tests/test_multimodal_webcam.py
"""

import sys
import os
from pathlib import Path
from datetime import datetime
import json
import time

# src í´ë”ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ (config í´ë”ì—ì„œ)
env_path = Path(__file__).parent.parent / 'config' / '.env'
load_dotenv(env_path)

import speech_recognition as sr
from core.multimodal_analyzer import MultimodalAnalyzer
from core.video_capture import VideoMonitor
from core.alert_manager import get_alert_manager

# ê¸€ë¡œë²Œ alert manager ì´ˆê¸°í™”
alert_manager = get_alert_manager()

# ë¡œê¹… ë””ë ‰í† ë¦¬ ìƒì„±
LOG_DIR = Path(__file__).parent.parent / 'data' / 'logs'
LOG_DIR.mkdir(parents=True, exist_ok=True)

# ì›¹ìº  í”„ë ˆì„ ë””ë ‰í† ë¦¬ ìƒì„±
WEBCAM_DIR = Path(__file__).parent.parent / 'webcam_frames'
WEBCAM_DIR.mkdir(parents=True, exist_ok=True)

# ì‹¤í–‰ ì‹œì‘ ì‹œê°„ìœ¼ë¡œ ë¡œê·¸ íŒŒì¼ëª… ìƒì„±
SESSION_START = datetime.now().strftime("%Y%m%d_%H%M%S")
LOG_FILE = LOG_DIR / f'multimodal_webcam_history_{SESSION_START}.json'

# ìµœì‹  ì›¹ìº  í”„ë ˆì„ ì €ì¥ìš©
latest_frame = None
latest_frame_time = None

def save_conversation_log(turn, text, frame_path, analysis):
    """ëŒ€í™” ë‚´ìš©ê³¼ ë¶„ì„ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ íŒŒì¼ì— ê¸°ë¡"""
    # ê¸°ì¡´ ë¡œê·¸ ë¡œë“œ (ìˆìœ¼ë©´)
    if LOG_FILE.exists():
        with open(LOG_FILE, 'r', encoding='utf-8') as f:
            history = json.load(f)
    else:
        history = []
    
    # ìƒˆ ê¸°ë¡ ì¶”ê°€
    log_entry = {
        "turn": turn,
        "timestamp": datetime.now().isoformat(),
        "transcribed_text": text,
        "webcam_frame_path": frame_path,
        "analysis": analysis
    }
    history.append(log_entry)
    
    # íŒŒì¼ì— ì €ì¥
    with open(LOG_FILE, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)
    
    print(f"   ğŸ’¾ ê¸°ë¡ ì €ì¥ (ì´ {len(history)}ê±´): {LOG_FILE.name}")

def on_webcam_frame(frame, timestamp):
    """ì›¹ìº  í”„ë ˆì„ ì½œë°±"""
    global latest_frame, latest_frame_time
    latest_frame = frame.copy()
    latest_frame_time = timestamp

# ìš°ì„ ìˆœìœ„ë³„ ìƒí™© ì„¤ëª…
SITUATION_GUIDE = {
    'CRITICAL': {
        'description': 'ğŸš¨ ê¸´ê¸‰ ìƒí™© - ì¦‰ì‹œ ëŒ€ì‘ í•„ìš”',
        'examples': ['ì„œë²„ ë‹¤ìš´', 'ë³´ì•ˆ ì¹¨í•´', 'ì‹œìŠ¤í…œ ì˜¤ë¥˜'],
    },
    'HIGH': {
        'description': 'âš ï¸ ë†’ì€ ìš°ì„ ìˆœìœ„ - ë¹ ë¥¸ ëŒ€ì‘ í•„ìš”',
        'examples': ['ì„±ëŠ¥ ì €í•˜', 'ì‚¬ìš©ì ë¶ˆë§Œ', 'ì˜¤ë¥˜ ë°œìƒ'],
    },
    'MEDIUM': {
        'description': 'ğŸ“Œ ì¤‘ê°„ ìš°ì„ ìˆœìœ„ - ì¼ë°˜ì  ëŒ€ì‘',
        'examples': ['ì¼ë°˜ ì§ˆë¬¸', 'ì •ë³´ ì œê³µ', 'ê¸°íƒ€'],
    },
    'LOW': {
        'description': 'â„¹ï¸ ë‚®ì€ ìš°ì„ ìˆœìœ„ - ë°°ê²½ ì •ë³´',
        'examples': ['ì¸ì‚¬', 'ì¼ë°˜ ëŒ€í™”', 'ì°¸ê³  ì‚¬í•­'],
    }
}

print("=" * 70)
print("ğŸ¥ ë©€í‹°ëª¨ë‹¬ ë¶„ì„ (ìŒì„± + ì›¹ìº )")
print("=" * 70)
print("ğŸ’¡ íŒ: ë§í•  ë•Œ ì›¹ìº ì—ì„œ ì‹¤ì‹œê°„ í”„ë ˆì„ì„ ìº¡ì²˜í•˜ì—¬ í•¨ê»˜ ë¶„ì„í•©ë‹ˆë‹¤")
print("ğŸ’¡ íŒ: 'quit' ë˜ëŠ” 'exit'ì„ ë§í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤\n")

# 1. ëª¨ë“ˆ ë¡œë“œ
print("1ï¸âƒ£ ëª¨ë“ˆ ë¡œë“œ ì¤‘...")
try:
    multimodal_analyzer = MultimodalAnalyzer()
    recognizer = sr.Recognizer()
    video_monitor = VideoMonitor(camera_id=0)
    print("   âœ… ëª¨ë“  ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    print(f"   âŒ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    exit(1)

# 2. ì›¹ìº  ëª¨ë‹ˆí„°ë§ ì‹œì‘
print("\n2ï¸âƒ£ ì›¹ìº  ëª¨ë‹ˆí„°ë§ ì‹œì‘...")
try:
    video_monitor.start_monitoring(
        on_frame_callback=on_webcam_frame,
        frame_interval=0.5,  # 0.5ì´ˆë§ˆë‹¤ í”„ë ˆì„ ì—…ë°ì´íŠ¸
        show_preview=False  # í”„ë¦¬ë·° ì°½ ë¹„í™œì„±í™” (í•„ìš”ì‹œ Trueë¡œ ë³€ê²½)
    )
    
    # ì›¹ìº ì´ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
    print("   â³ ì›¹ìº  ì¤€ë¹„ ì¤‘...")
    for _ in range(10):
        if latest_frame is not None:
            break
        time.sleep(0.5)
    
    if latest_frame is None:
        print("   âš ï¸  ì›¹ìº  í”„ë ˆì„ì„ ë°›ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰í•˜ì§€ë§Œ ì´ë¯¸ì§€ ì—†ì´ ìŒì„±ë§Œ ë¶„ì„ë©ë‹ˆë‹¤.")
    else:
        print("   âœ… ì›¹ìº  ì¤€ë¹„ ì™„ë£Œ")
except Exception as e:
    print(f"   âš ï¸  ì›¹ìº  ì‹œì‘ ì‹¤íŒ¨: {e}")
    print("   ê³„ì† ì§„í–‰í•˜ì§€ë§Œ ì´ë¯¸ì§€ ì—†ì´ ìŒì„±ë§Œ ë¶„ì„ë©ë‹ˆë‹¤.")

# 3. ë¬´í•œ ë£¨í”„ ëª¨ë‹ˆí„°ë§
print("\n" + "=" * 70)
print("âœ… ì‹œì‘ ì¤€ë¹„ ì™„ë£Œ! ë§ˆì´í¬ë¡œ ë§ì”€í•´ì£¼ì„¸ìš”")
print("=" * 70 + "\n")

turn = 1
while True:
    try:
        print(f"\nğŸ“ [íšŒì°¨ {turn}] ë§ˆì´í¬ì—ì„œ ì…ë ¥ ë°›ëŠ” ì¤‘...")
        print("   ğŸ’¬ ì§€ê¸ˆë¶€í„° ë§ì”€í•´ì£¼ì„¸ìš” ('quit' ë˜ëŠ” 'exit'ìœ¼ë¡œ ì¢…ë£Œ)")
        
        with sr.Microphone() as source:
            # ë°°ê²½ ì†ŒìŒ ì ì‘
            recognizer.adjust_for_ambient_noise(source, duration=1)
            
            # ìŒì„± ì…ë ¥ (ìµœëŒ€ 30ì´ˆ)
            audio = recognizer.listen(source, timeout=None, phrase_time_limit=30)
        
        # Google Speech Recognition (ë¬´ë£Œ)
        print("   ğŸ”„ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘...")
        text = recognizer.recognize_google(audio, language='ko-KR')
        
        # ì¢…ë£Œ ì¡°ê±´
        if text.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'ë']:
            print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤")
            break
        
        print(f"\n   ğŸ“ ì¸ì‹ëœ í…ìŠ¤íŠ¸:")
        print(f"      '{text}'")
        
        # ì›¹ìº  í”„ë ˆì„ ì €ì¥
        frame_path = None
        if latest_frame is not None:
            print("\n   ğŸ“¹ ì›¹ìº  í”„ë ˆì„ ìº¡ì²˜ ì¤‘...")
            frame_path = str(WEBCAM_DIR / f"webcam_{SESSION_START}_{turn:03d}.jpg")
            
            import cv2
            cv2.imwrite(frame_path, latest_frame)
            print(f"      âœ… ì›¹ìº  í”„ë ˆì„ ì €ì¥: {frame_path}")
        else:
            print("\n   âš ï¸  ì›¹ìº  í”„ë ˆì„ ì—†ìŒ (ìŒì„±ë§Œìœ¼ë¡œ ë¶„ì„)")
        
        # ë©€í‹°ëª¨ë‹¬ ë¶„ì„ (ìŒì„± + ì›¹ìº )
        print("\n   ğŸ¤– ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì¤‘ (ìŒì„± + ë¹„ë””ì˜¤)...")
        if latest_frame is not None:
            analysis = multimodal_analyzer.analyze_with_video_frame(text, latest_frame)
        else:
            # ì›¹ìº  í”„ë ˆì„ ì—†ì„ ì‹œ ìŒì„±ë§Œ ë¶„ì„ (í´ë°±)
            from core.voice_analyzer import VoiceAnalyzer
            voice_analyzer = VoiceAnalyzer()
            analysis = voice_analyzer.analyze_with_llm(text)
        
        print("      âœ… ë¶„ì„ ì™„ë£Œ\n")
        
        priority = analysis.get('priority', 'LOW')
        is_emergency = analysis.get('is_emergency', False)
        
        # ğŸš¨ ê¸´ê¸‰ ìƒí™© ê°ì§€ ë° ì•Œë¦¼ (alert_manager ì‚¬ìš©)
        if is_emergency or priority == 'CRITICAL':
            alert_manager.trigger_alert({
                'is_emergency': is_emergency,
                'emergency_reason': analysis.get('emergency_reason', 'ì•Œ ìˆ˜ ì—†ëŠ” ê¸´ê¸‰ ìƒí™©'),
                'priority': priority,
                'situation_type': analysis.get('situation_type', 'ë¯¸ë¶„ë¥˜')
            })
        
        # ìƒí™© ë¶„ì„
        situation_text = analysis.get('situation', 'ë¯¸ë¶„ë¥˜')
        print(f"   ğŸ¯ ìƒí™© ë¶„ì„:")
        print(f"      {situation_text}")
        
        # ì‹œê°ì  ë‚´ìš©
        visual_content = analysis.get('visual_content', 'N/A')
        if visual_content and visual_content != 'N/A':
            print(f"\n   ğŸ‘ï¸  ì‹œê° ì •ë³´:")
            print(f"      {visual_content}")
        
        # ìŒì„±-ì‹œê° ì¼ì¹˜ë„
        consistency = analysis.get('audio_visual_consistency', 'N/A')
        if consistency and consistency != 'N/A':
            consistency_emoji = {
                'ì¼ì¹˜': 'âœ…',
                'ë¶ˆì¼ì¹˜': 'âš ï¸',
                'ë¶€ë¶„ì¼ì¹˜': 'ğŸ”¶'
            }.get(consistency, 'â“')
            print(f"\n   {consistency_emoji} ìŒì„±-ì‹œê° ì¼ì¹˜ë„: {consistency}")
        
        situation_type = analysis.get('situation_type', 'ë¯¸ë¶„ë¥˜')
        print(f"\n   ğŸ“Œ ìƒí™© ìœ í˜•: {situation_type}")
        
        action = analysis.get('action', 'ë¯¸ë¶„ë¥˜')
        print(f"   ğŸ”§ ê¶Œì¥ ì¡°ì¹˜: {action}")
        
        # ìƒí™© ê°€ì´ë“œ (ì°¸ê³ ìš©)
        if priority in SITUATION_GUIDE:
            guide_info = SITUATION_GUIDE[priority]
            print(f"\n   {guide_info['description']}")
            print(f"   ğŸ“‹ ì°¸ê³  ì˜ˆì‹œ: {', '.join(guide_info['examples'])}")
        
        # ğŸ’¾ ê¸°ë¡ ì €ì¥
        save_conversation_log(turn, text, frame_path, analysis)
        
        print("\n   " + "=" * 65)
        turn += 1
        
    except sr.UnknownValueError:
        print("   âš ï¸  ìŒì„±ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        turn += 1
    except sr.RequestError as e:
        print(f("   âŒ ìŒì„± ì¸ì‹ ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}")
        break
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤")
        break
    except Exception as e:
        print(f"   âŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        turn += 1

# ì›¹ìº  ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
print("\nì›¹ìº  ëª¨ë‹ˆí„°ë§ ì¤‘ì§€ ì¤‘...")
try:
    video_monitor.stop_monitoring()
except:
    pass

print("\n" + "=" * 70)
print(f"âœ… ì´ {turn - 1}íšŒ ë¶„ì„ ì™„ë£Œ")
print("=" * 70)
