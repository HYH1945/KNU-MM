#!/usr/bin/env python3
"""
ë©€í‹°ëª¨ë‹¬ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ëª¨ë“ˆ
ì˜¤ë””ì˜¤ + ì´ë¯¸ì§€/ë¹„ë””ì˜¤ë¥¼ í•¨ê»˜ ë¶„ì„í•˜ì—¬ ë” ì •í™•í•œ ìƒí™© íŒë‹¨
ìŒì„± íŠ¹ì„± ë¶„ì„ìœ¼ë¡œ ì‘ê¸‰ ì‹ í˜¸ ì‹ ë¢°ë„ ê²€ì¦

ì‚¬ìš©ë²•:
    analyzer = MultimodalAnalyzer()
    
    # ì´ë¯¸ì§€ + ì˜¤ë””ì˜¤ ë¶„ì„ (ìŒì„± íŠ¹ì„± í¬í•¨)
    result = analyzer.analyze_with_image(
        audio_text="ë„ì™€ì£¼ì„¸ìš”!",
        image_path="screenshot.jpg",
        audio_file_path="audio.wav"  # ì„ íƒì‚¬í•­
    )
"""

import os
import sys
import json
import base64
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, Union
import numpy as np

try:
    from openai import OpenAI
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

try:
    import cv2
    OPENCV_AVAILABLE = True
except ImportError:
    OPENCV_AVAILABLE = False
    print("âš ï¸  OpenCVê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    print("âš ï¸  Pillowê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ì²˜ë¦¬ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

# ì„¤ì • ê´€ë¦¬ì ì„í¬íŠ¸
try:
    from core.config_manager import get_config, get_prompt, get_openai_config, get_api_key
    CONFIG_AVAILABLE = True
except ImportError:
    CONFIG_AVAILABLE = False
    # ê¸°ë³¸ê°’ í•¨ìˆ˜ë“¤
    def get_config(*keys, default=None):
        return default
    def get_prompt(prompt_type='system'):
        return ""
    def get_openai_config(key, default=None):
        return default
    def get_api_key(service='openai'):
        return os.getenv('OPENAI_API_KEY')

# ìŒì„± íŠ¹ì„± ë¶„ì„ê¸° ì„í¬íŠ¸
try:
    from core.voice_characteristics import VoiceCharacteristicsAnalyzer
    VOICE_ANALYSIS_AVAILABLE = True
except ImportError:
    VOICE_ANALYSIS_AVAILABLE = False


class MultimodalAnalyzer:
    """ë©€í‹°ëª¨ë‹¬ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ê¸° (ì˜¤ë””ì˜¤ + ë¹„ì „)"""
    
    # ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (configê°€ ì—†ì„ ë•Œ ì‚¬ìš©)
    DEFAULT_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ìŒì„±ê³¼ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ë¶„ì„í•˜ëŠ” ìƒí™© ë¶„ì„ AIì…ë‹ˆë‹¤. ê°ê´€ì ìœ¼ë¡œ ìƒí™©ì„ íŒŒì•…í•˜ê³  ë¶„ì„í•˜ì„¸ìš”.

ë‹¤ìŒì„ JSONìœ¼ë¡œë§Œ ë°˜í™˜í•˜ì„¸ìš”:
{
  "context": "ë§¥ë½ ì„¤ëª…",
  "urgency": "ìœ„ê¸‰ë„ (ë‚®ìŒ/ì¤‘ê°„/ë†’ìŒ/ê¸´ê¸‰)",
  "situation": "ìƒí™© ë¶„ì„",
  "situation_type": "ìƒí™© ìœ í˜•",
  "is_emergency": true/false,
  "priority": "CRITICAL/HIGH/MEDIUM/LOW",
  "action": "ê¶Œì¥ ì¡°ì¹˜"
}"""
    
    def __init__(self, model: str = None):
        """
        ë©€í‹°ëª¨ë‹¬ ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            model: ì‚¬ìš©í•  OpenAI ëª¨ë¸ (Noneì´ë©´ configì—ì„œ ë¡œë“œ)
        """
        # ëª¨ë¸ ì„¤ì • (ì¸ì > config > ê¸°ë³¸ê°’)
        self.model = model or get_config('model', default='gpt-4o-mini')
        
        # API í‚¤ ë¡œë“œ (í™˜ê²½ë³€ìˆ˜ > .env > config.yaml)
        self.api_key = get_api_key('openai')
        
        if not self.api_key:
            raise ValueError(
                "âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
                "   ë‹¤ìŒ ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ ì„¤ì •í•˜ì„¸ìš”:\n"
                "   1. config/config.yamlì˜ api_keys.openaiì— ì…ë ¥\n"
                "   2. config/.env íŒŒì¼ì— OPENAI_API_KEY=sk-... í˜•ì‹ìœ¼ë¡œ ì…ë ¥\n"
                "   3. í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •: export OPENAI_API_KEY=sk-..."
            )
        
        self.client = OpenAI(api_key=self.api_key)
        
        # ìŒì„± íŠ¹ì„± ë¶„ì„ê¸° ì´ˆê¸°í™”
        if VOICE_ANALYSIS_AVAILABLE:
            self.voice_analyzer = VoiceCharacteristicsAnalyzer()
        else:
            self.voice_analyzer = None
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (configì—ì„œ ë¡œë“œ)
        self.system_prompt = get_prompt('system') or self.DEFAULT_SYSTEM_PROMPT
        
        # OpenAI API ì„¤ì •
        self.max_tokens = get_openai_config('max_tokens', default=800)
        self.temperature = get_openai_config('temperature', default=0.3)
        self.image_detail = get_openai_config('image_detail', default='low')
    
    def encode_image_to_base64(self, image_source: Union[str, np.ndarray], max_size: int = 1024) -> str:
        """
        ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”© (í¬ê¸° ìµœì í™” í¬í•¨)
        
        Args:
            image_source: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” numpy array (OpenCV ì´ë¯¸ì§€)
            max_size: ìµœëŒ€ ì´ë¯¸ì§€ í¬ê¸° (í”½ì…€). ì´ë³´ë‹¤ í¬ë©´ ë¦¬ì‚¬ì´ì§• (ê¸°ë³¸ê°’: 1024)
        
        Returns:
            base64 ì¸ì½”ë”©ëœ ë¬¸ìì—´
        """
        if isinstance(image_source, str):
            # íŒŒì¼ ê²½ë¡œì¸ ê²½ìš° - PILë¡œ ì—´ì–´ì„œ ë¦¬ì‚¬ì´ì§•
            if not PIL_AVAILABLE:
                # PILì´ ì—†ìœ¼ë©´ ì›ë³¸ ê·¸ëŒ€ë¡œ ì¸ì½”ë”©
                with open(image_source, "rb") as image_file:
                    return base64.b64encode(image_file.read()).decode('utf-8')
            
            # PILë¡œ ì´ë¯¸ì§€ ì—´ê¸°
            img = Image.open(image_source)
            
            # í¬ê¸° ì¡°ì • (ê¸´ ìª½ì´ max_sizeë¥¼ ë„˜ìœ¼ë©´ ë¦¬ì‚¬ì´ì§•)
            if max(img.size) > max_size:
                ratio = max_size / max(img.size)
                new_size = tuple(int(dim * ratio) for dim in img.size)
                img = img.resize(new_size, Image.Resampling.LANCZOS)
            
            # JPEGë¡œ ë³€í™˜í•˜ì—¬ base64 ì¸ì½”ë”©
            from io import BytesIO
            buffer = BytesIO()
            img.convert('RGB').save(buffer, format='JPEG', quality=85)
            return base64.b64encode(buffer.getvalue()).decode('utf-8')
        
        elif isinstance(image_source, np.ndarray):
            # numpy array (OpenCV ì´ë¯¸ì§€)ì¸ ê²½ìš°
            if not OPENCV_AVAILABLE:
                raise ImportError("OpenCVê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install opencv-python")
            
            # í¬ê¸° ì¡°ì •
            height, width = image_source.shape[:2]
            if max(height, width) > max_size:
                ratio = max_size / max(height, width)
                new_width = int(width * ratio)
                new_height = int(height * ratio)
                image_source = cv2.resize(image_source, (new_width, new_height), interpolation=cv2.INTER_LANCZOS4)
            
            # BGR to RGB ë³€í™˜ (OpenCVëŠ” BGR, PILì€ RGB)
            if len(image_source.shape) == 3 and image_source.shape[2] == 3:
                image_rgb = cv2.cvtColor(image_source, cv2.COLOR_BGR2RGB)
            else:
                image_rgb = image_source
            
            # numpy arrayë¥¼ JPEGë¡œ ì¸ì½”ë”© (í’ˆì§ˆ 85%)
            encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 85]
            _, buffer = cv2.imencode('.jpg', image_rgb, encode_param)
            return base64.b64encode(buffer).decode('utf-8')
        
        else:
            raise TypeError("image_sourceëŠ” íŒŒì¼ ê²½ë¡œ(str) ë˜ëŠ” numpy arrayì—¬ì•¼ í•©ë‹ˆë‹¤")
    
    def analyze_with_image(
        self, 
        audio_text: str, 
        image_source: Union[str, np.ndarray],
        additional_context: Optional[str] = None,
        audio_file_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ì˜¤ë””ì˜¤ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ë¶„ì„ (ìŒì„± íŠ¹ì„± ë¶„ì„ í¬í•¨)
        
        Args:
            audio_text: ìŒì„±ì—ì„œ ë³€í™˜ëœ í…ìŠ¤íŠ¸
            image_source: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” numpy array
            additional_context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ (ì„ íƒ)
            audio_file_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ìŒì„± íŠ¹ì„± ë¶„ì„ìš©, ì„ íƒ)
        
        Returns:
            ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            base64_image = self.encode_image_to_base64(image_source)
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ êµ¬ì„±
            user_message = f"""**ìŒì„± ì…ë ¥:** "{audio_text}"

**ë¶„ì„ ìš”ì²­:** ìœ„ ìŒì„±ê³¼ í•¨ê»˜ ì œê³µëœ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ì „ì²´ ìƒí™©ì„ íŒë‹¨í•´ì£¼ì„¸ìš”."""
            
            if additional_context:
                user_message += f"\n\n**ì¶”ê°€ ì •ë³´:** {additional_context}"
            
            # OpenAI API í˜¸ì¶œ (Vision ì§€ì›)
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": self.system_prompt
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": user_message
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}",
                                    "detail": self.image_detail  # configì—ì„œ ë¡œë“œ
                                }
                            }
                        ]
                    }
                ],
                max_tokens=self.max_tokens,  # configì—ì„œ ë¡œë“œ
                temperature=self.temperature  # configì—ì„œ ë¡œë“œ
            )
            
            # ì‘ë‹µ íŒŒì‹±
            content = response.choices[0].message.content
            
            # ì•ˆì „ ì •ì±… ê±°ë¶€ ê°ì§€
            if content and ("I'm sorry" in content or "I can't assist" in content or "I cannot" in content):
                print(f"âš ï¸  OpenAI ì•ˆì „ ì •ì±…ìœ¼ë¡œ ì¸í•œ ë¶„ì„ ê±°ë¶€")
                print(f"   ì›ë³¸ ì‘ë‹µ: {content}")
                return {
                    'context': 'ì´ë¯¸ì§€ ë‚´ìš©ì„ ë¶„ì„í•  ìˆ˜ ì—†ìŒ (ì•ˆì „ ì •ì±…)',
                    'urgency': 'ë‚®ìŒ',
                    'urgency_reason': 'OpenAI ì•ˆì „ ì •ì±…ìœ¼ë¡œ ì¸í•œ ë¶„ì„ ì œí•œ',
                    'situation': 'ì´ë¯¸ì§€ê°€ ì•ˆì „ ì •ì±…ì— ìœ„ë°°ë˜ê±°ë‚˜ ë¶„ì„ì´ ì œí•œë˜ì—ˆìŠµë‹ˆë‹¤.',
                    'situation_type': 'ë¶„ì„ ì œí•œ',
                    'emotional_state': 'ì¤‘ë¦½',
                    'visual_content': 'ë¶„ì„ ë¶ˆê°€',
                    'audio_visual_consistency': 'N/A',
                    'action': 'ë‹¤ë¥¸ ì´ë¯¸ì§€ë¡œ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ì´ë¯¸ì§€ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”',
                    'is_emergency': False,
                    'emergency_reason': None,
                    'priority': 'LOW',
                    'error': 'ì•ˆì „ ì •ì±… ê±°ë¶€',
                    'raw_response': content
                }
            
            # JSON íŒŒì‹±
            try:
                # ì½”ë“œ ë¸”ë¡ ì œê±° (```json ... ```)
                if '```json' in content:
                    content = content.split('```json')[1].split('```')[0].strip()
                elif '```' in content:
                    content = content.split('```')[1].split('```')[0].strip()
                
                result = json.loads(content)
            
            except json.JSONDecodeError as e:
                print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                print(f"ì›ë³¸ ì‘ë‹µ:\n{content}")
                return {
                    'error': 'JSON íŒŒì‹± ì‹¤íŒ¨',
                    'raw_response': content,
                    'context': 'ë¶„ì„ ì˜¤ë¥˜',
                    'is_emergency': False,
                    'priority': 'LOW'
                }
            
            # ìŒì„± íŠ¹ì„± ë¶„ì„ìœ¼ë¡œ ì‹ ë¢°ë„ ê²€ì¦
            if audio_file_path and self.voice_analyzer:
                try:
                    print("   ğŸ¤ ìŒì„± íŠ¹ì„± ë¶„ì„ ì¤‘...")
                    audio_features = self.voice_analyzer.extract_features(audio_file_path)
                    
                    confidence_result = self.voice_analyzer.calculate_confidence_score(
                        audio_features=audio_features,
                        llm_priority=result.get('priority', 'LOW'),
                        llm_is_emergency=result.get('is_emergency', False)
                    )
                    
                    # ìµœì¢… ê²°ê³¼ì— ìŒì„± ë¶„ì„ ì •ë³´ ì¶”ê°€
                    result['voice_analysis'] = {
                        'voice_emergency_score': confidence_result['voice_emergency_score'],
                        'combined_score': confidence_result['combined_score'],
                        'final_priority': confidence_result['final_priority'],
                        'confidence': confidence_result['confidence'],
                        'indicators': confidence_result['breakdown']['voice_indicators']
                    }
                    
                    # ìµœì¢… ìš°ì„ ìˆœìœ„ ì—…ë°ì´íŠ¸ (LLM + ìŒì„± íŠ¹ì„±)
                    original_priority = result.get('priority', 'LOW')
                    final_priority = confidence_result['final_priority']
                    
                    if final_priority != original_priority:
                        result['priority_adjustment'] = {
                            'original': original_priority,
                            'adjusted': final_priority,
                            'reason': f'ìŒì„± íŠ¹ì„± ë¶„ì„ìœ¼ë¡œ ì¸í•œ ì¡°ì • (ì‹ ë¢°ë„: {confidence_result["confidence"]:.1%})'
                        }
                        result['priority'] = final_priority
                        result['is_emergency'] = final_priority == 'CRITICAL'
                    
                    print(f"      âœ… ìŒì„± ë¶„ì„ ì™„ë£Œ (ì‹ ë¢°ë„: {confidence_result['confidence']:.1%})")
                
                except Exception as e:
                    print(f"      âš ï¸  ìŒì„± íŠ¹ì„± ë¶„ì„ ì˜¤ë¥˜: {e}")
            
            return {
                    'error': 'JSON íŒŒì‹± ì‹¤íŒ¨',
                    'raw_response': content,
                    'context': 'ë¶„ì„ ì˜¤ë¥˜',
                    'is_emergency': False,
                    'priority': 'LOW'
                }
        
        except Exception as e:
            print(f"âŒ ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                'error': str(e),
                'context': 'ë¶„ì„ ì‹¤íŒ¨',
                'is_emergency': False,
                'priority': 'LOW'
            }
    
    def analyze_with_video_frame(
        self,
        audio_text: str,
        frame: np.ndarray,
        additional_context: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ì˜¤ë””ì˜¤ í…ìŠ¤íŠ¸ì™€ ë¹„ë””ì˜¤ í”„ë ˆì„ì„ í•¨ê»˜ ë¶„ì„
        
        Args:
            audio_text: ìŒì„±ì—ì„œ ë³€í™˜ëœ í…ìŠ¤íŠ¸
            frame: ë¹„ë””ì˜¤ í”„ë ˆì„ (numpy array, OpenCV format)
            additional_context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ (ì„ íƒ)
        
        Returns:
            ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # analyze_with_imageì™€ ë™ì¼í•˜ê²Œ ì²˜ë¦¬ (í”„ë ˆì„ì€ ì´ë¯¸ì§€ë¡œ ì·¨ê¸‰)
        return self.analyze_with_image(audio_text, frame, additional_context)
    
    def capture_screenshot(self, save_path: Optional[str] = None) -> str:
        """
        í™”ë©´ ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ (macOS)
        
        Args:
            save_path: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if save_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
            save_path = f"screenshots/screenshot_{timestamp}.png"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        
        try:
            import subprocess
            # macOS screencapture ëª…ë ¹ ì‚¬ìš©
            subprocess.run(['screencapture', '-x', save_path], check=True)
            print(f"âœ… ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {save_path}")
            return save_path
        except Exception as e:
            print(f"âŒ ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ì‹¤íŒ¨: {e}")
            return None
    
    def capture_webcam_frame(self, camera_id: int = 0, save_path: Optional[str] = None) -> Union[str, np.ndarray]:
        """
        ì›¹ìº ì—ì„œ í”„ë ˆì„ ìº¡ì²˜
        
        Args:
            camera_id: ì¹´ë©”ë¼ ID (ê¸°ë³¸ê°’: 0)
            save_path: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ì €ì¥í•˜ì§€ ì•Šê³  numpy array ë°˜í™˜)
        
        Returns:
            ì €ì¥ ê²½ë¡œ ë˜ëŠ” numpy array
        """
        if not OPENCV_AVAILABLE:
            raise ImportError("OpenCVê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install opencv-python")
        
        try:
            cap = cv2.VideoCapture(camera_id)
            
            if not cap.isOpened():
                raise RuntimeError(f"ì¹´ë©”ë¼ {camera_id}ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # í”„ë ˆì„ ì½ê¸°
            ret, frame = cap.read()
            cap.release()
            
            if not ret:
                raise RuntimeError("í”„ë ˆì„ ìº¡ì²˜ ì‹¤íŒ¨")
            
            if save_path:
                # ë””ë ‰í† ë¦¬ ìƒì„±
                Path(save_path).parent.mkdir(parents=True, exist_ok=True)
                cv2.imwrite(save_path, frame)
                print(f"âœ… ì›¹ìº  í”„ë ˆì„ ì €ì¥: {save_path}")
                return save_path
            else:
                return frame
        
        except Exception as e:
            print(f"âŒ ì›¹ìº  ìº¡ì²˜ ì‹¤íŒ¨: {e}")
            return None


# í¸ì˜ í•¨ìˆ˜
def analyze_audio_with_screenshot(audio_text: str, screenshot_path: Optional[str] = None) -> Dict[str, Any]:
    """
    ìŒì„± í…ìŠ¤íŠ¸ì™€ ìŠ¤í¬ë¦°ìƒ·ì„ í•¨ê»˜ ë¶„ì„í•˜ëŠ” í¸ì˜ í•¨ìˆ˜
    
    Args:
        audio_text: ìŒì„±ì—ì„œ ë³€í™˜ëœ í…ìŠ¤íŠ¸
        screenshot_path: ìŠ¤í¬ë¦°ìƒ· ê²½ë¡œ (Noneì´ë©´ ìë™ ìº¡ì²˜)
    
    Returns:
        ë¶„ì„ ê²°ê³¼
    """
    analyzer = MultimodalAnalyzer()
    
    # ìŠ¤í¬ë¦°ìƒ·ì´ ì—†ìœ¼ë©´ ìë™ ìº¡ì²˜
    if screenshot_path is None:
        screenshot_path = analyzer.capture_screenshot()
        if screenshot_path is None:
            return {'error': 'ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ì‹¤íŒ¨'}
    
    return analyzer.analyze_with_image(audio_text, screenshot_path)


def analyze_audio_with_webcam(audio_text: str, camera_id: int = 0) -> Dict[str, Any]:
    """
    ìŒì„± í…ìŠ¤íŠ¸ì™€ ì›¹ìº  í”„ë ˆì„ì„ í•¨ê»˜ ë¶„ì„í•˜ëŠ” í¸ì˜ í•¨ìˆ˜
    
    Args:
        audio_text: ìŒì„±ì—ì„œ ë³€í™˜ëœ í…ìŠ¤íŠ¸
        camera_id: ì¹´ë©”ë¼ ID
    
    Returns:
        ë¶„ì„ ê²°ê³¼
    """
    analyzer = MultimodalAnalyzer()
    
    # ì›¹ìº ì—ì„œ í”„ë ˆì„ ìº¡ì²˜
    frame = analyzer.capture_webcam_frame(camera_id)
    if frame is None:
        return {'error': 'ì›¹ìº  í”„ë ˆì„ ìº¡ì²˜ ì‹¤íŒ¨'}
    
    return analyzer.analyze_with_video_frame(audio_text, frame)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("=" * 70)
    print("ğŸ¥ ë©€í‹°ëª¨ë‹¬ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    analyzer = MultimodalAnalyzer()
    
    # 1. ìŠ¤í¬ë¦°ìƒ·ê³¼ í•¨ê»˜ ë¶„ì„
    print("\n1ï¸âƒ£ ìŠ¤í¬ë¦°ìƒ· ë¶„ì„ í…ŒìŠ¤íŠ¸")
    screenshot = analyzer.capture_screenshot()
    if screenshot:
        result = analyzer.analyze_with_image(
            audio_text="ì´ìƒí•œ ì†Œë¦¬ê°€ ë“¤ë ¤ìš”",
            image_source=screenshot
        )
        print(f"ë¶„ì„ ê²°ê³¼: {json.dumps(result, ensure_ascii=False, indent=2)}")
    
    # 2. ì›¹ìº ê³¼ í•¨ê»˜ ë¶„ì„
    print("\n2ï¸âƒ£ ì›¹ìº  ë¶„ì„ í…ŒìŠ¤íŠ¸")
    frame = analyzer.capture_webcam_frame()
    if frame is not None:
        result = analyzer.analyze_with_video_frame(
            audio_text="ì§€ê¸ˆ ìƒí™©ì´ ì–´ë–¤ê°€ìš”?",
            frame=frame
        )
        print(f"ë¶„ì„ ê²°ê³¼: {json.dumps(result, ensure_ascii=False, indent=2)}")
