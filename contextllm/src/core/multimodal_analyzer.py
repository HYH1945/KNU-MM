#!/usr/bin/env python3
"""
ë©€í‹°ëª¨ë‹¬ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ëª¨ë“ˆ
ì˜¤ë””ì˜¤ + ì´ë¯¸ì§€/ë¹„ë””ì˜¤ë¥¼ í•¨ê»˜ ë¶„ì„í•˜ì—¬ ë” ì •í™•í•œ ìƒí™© íŒë‹¨
ìŒì„± íŠ¹ì„± ë¶„ì„ìœ¼ë¡œ ì‘ê¸‰ ì‹ í˜¸ ì‹ ë¢°ë„ ê²€ì¦

ì‚¬ìš©ë²•:
    analyzer = MultimodalAnalyzer()
    
    # ì´ë¯¸ì§€ + ì˜¤ë””ì˜¤ ë¶„ì„ (ìŒì„± íŠ¹ì„± í¬í•¨)
    result = analyzer.analyze_with_image(
        audio_text="ë„ì™€ì£¼ì„¸ìš”!",
        image_path="screenshot.jpg",
        audio_file_path="audio.wav"  # ì„ íƒì‚¬í•­
    )
"""

import os
import sys
import json
import base64
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, Union
import numpy as np

try:
    from openai import OpenAI
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

try:
    import cv2
    OPENCV_AVAILABLE = True
except ImportError:
    OPENCV_AVAILABLE = False
    print("âš ï¸  OpenCVê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    print("âš ï¸  Pillowê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ì²˜ë¦¬ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

# ì„¤ì • ê´€ë¦¬ì ì„í¬íŠ¸
try:
    from core.config_manager import get_config, get_prompt, get_openai_config, get_api_key
    CONFIG_AVAILABLE = True
except ImportError:
    CONFIG_AVAILABLE = False
    # ê¸°ë³¸ê°’ í•¨ìˆ˜ë“¤
    def get_config(*keys, default=None):
        return default
    def get_prompt(prompt_type='system'):
        return ""
    def get_openai_config(key, default=None):
        return default
    def get_api_key(service='openai'):
        return os.getenv('OPENAI_API_KEY')

# ìŒì„± íŠ¹ì„± ë¶„ì„ê¸° ì„í¬íŠ¸
try:
    from core.voice_characteristics import VoiceCharacteristicsAnalyzer
    VOICE_ANALYSIS_AVAILABLE = True
except ImportError:
    VOICE_ANALYSIS_AVAILABLE = False


class MultimodalAnalyzer:
    """ë©€í‹°ëª¨ë‹¬ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ê¸° (ì˜¤ë””ì˜¤ + ë¹„ì „)"""
    
    # ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (configê°€ ì—†ì„ ë•Œ ì‚¬ìš©)
    DEFAULT_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ìŒì„±, ì´ë¯¸ì§€, ìŒì„± íŠ¹ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ëŠ” ìƒí™© ë¶„ì„ AIì…ë‹ˆë‹¤.

ë¶„ì„ í”„ë¡œì„¸ìŠ¤:
1. ìŒì„± íŠ¹ì„± í•´ì„: ì œê³µëœ ìŒì„± ë¶„ì„ ê²°ê³¼(í”¼ì¹˜, ì—ë„ˆì§€, ë§ ì†ë„, ë–¨ë¦¼)ë¥¼ ë§¥ë½ í•´ì„ì— í™œìš©
2. ìŒì„± ë§¥ë½ ë¶„ì„: ë‹¨ìˆœ í‚¤ì›Œë“œê°€ ì•„ë‹Œ ì „ì²´ ì˜ë„ì™€ ê°ì • íŒŒì•…
3. ì˜ìƒ ë¶„ì„: ì´ë¯¸ì§€ì—ì„œ ë³´ì´ëŠ” ì‹¤ì œ ìƒí™©, í™˜ê²½, ì‹ ì²´ ìƒíƒœ
4. ì¼ê´€ì„± í‰ê°€: ìŒì„± ë‚´ìš© + ìŒì„± íŠ¹ì„± + ì˜ìƒì´ ì¼ì¹˜í•˜ëŠ”ê°€?
5. ì¢…í•© íŒë‹¨: ëª¨ë“  ì‹ í˜¸ë¥¼ ì¢…í•©í•˜ì—¬ ê¸´ê¸‰ë„ ê²°ì •

íŒë‹¨ ì›ì¹™:
- CRITICAL: ìŒì„± íŠ¹ì„±ì´ ì ˆë°•í•¨ + ìŒì„± ë‚´ìš©ì´ ìœ„í—˜ ìƒí™© + ì˜ìƒ ì¼ì¹˜ = ì¦‰ì‹œ ì¡°ì¹˜
- HIGH: ë¶€ë¶„ì  ì ˆë°•í•¨ + ìœ„í—˜ ì‹ í˜¸ ìˆìŒ + ì˜ìƒê³¼ ë¶€ë¶„ ì¼ì¹˜ = ë¹ ë¥¸ ëŒ€ì‘
- MEDIUM: ìŒì„± íŠ¹ì„±ê³¼ ì˜ìƒì´ ë¶€ë¶„ ì¼ì¹˜ ë˜ëŠ” ë¶ˆëª…í™• = ëª¨ë‹ˆí„°ë§
- LOW: ìŒì„± íŠ¹ì„±, ë‚´ìš©, ì˜ìƒ ëª¨ë‘ ì¼ìƒì  = íŠ¹ë³„ ì¡°ì¹˜ ë¶ˆí•„ìš”

í•µì‹¬:
- ìŒì„± íŠ¹ì„±ì—ì„œ "ë–¨ë¦¼", "ë¹ ë¥¸ ì†ë„", "ë†’ì€ ì—ë„ˆì§€"ê°€ ìˆìœ¼ë©´ ì ˆë°•í•¨ì˜ ì‹ í˜¸
- ì˜ìƒì—ì„œ ìœ„í—˜ ì§•í›„(ë¶€ìƒ, í­ë ¥, ìœ„í—˜ í™˜ê²½)ê°€ ë³´ì´ë©´ ì‹ ë¢°ë„ ì¦ê°€
- ìŒì„±-ì˜ìƒ-íŠ¹ì„±ì´ ëª¨ë‘ ì¼ì¹˜í•˜ë©´ ê¸´ê¸‰ë„ ìƒí–¥

ë‹¤ìŒì„ JSONìœ¼ë¡œë§Œ ë°˜í™˜í•˜ì„¸ìš”:
{
  "context": "ìŒì„± ë§¥ë½ ë¶„ì„",
  "urgency": "ìœ„ê¸‰ë„ (LOW/MEDIUM/HIGH/CRITICAL)",
  "situation": "ìŒì„± ë‚´ìš© + ì˜ìƒ + ìŒì„± íŠ¹ì„± ì¢…í•© ë¶„ì„",
  "situation_type": "ìƒí™© ë¶„ë¥˜",
  "emotional_state": "ìŒì„±ì—ì„œ ê°ì§€ë˜ëŠ” ê°ì • ìƒíƒœ",
  "visual_content": "ì˜ìƒì—ì„œ ë³´ì´ëŠ” ì‹¤ì œ ìƒí™©",
  "audio_visual_consistency": "ìŒì„±, ìŒì„± íŠ¹ì„±, ì˜ìƒì˜ ì¼ê´€ì„± í‰ê°€",
  "is_emergency": true/false,
  "emergency_reason": "ê¸´ê¸‰ íŒë‹¨ ê·¼ê±° (ìŒì„±+íŠ¹ì„±+ì˜ìƒ ê¸°ë°˜)",
  "priority": "CRITICAL/HIGH/MEDIUM/LOW",
  "action": "ê¶Œì¥ ì¡°ì¹˜"
}"""
    
    def __init__(self, model: str = None):
        """
        ë©€í‹°ëª¨ë‹¬ ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            model: ì‚¬ìš©í•  OpenAI ëª¨ë¸ (Noneì´ë©´ configì—ì„œ ë¡œë“œ)
        """
        # ëª¨ë¸ ì„¤ì • (ì¸ì > config > ê¸°ë³¸ê°’)
        self.model = model or get_config('model', default='gpt-4o-mini')
        
        # API í‚¤ ë¡œë“œ (í™˜ê²½ë³€ìˆ˜ > .env > config.yaml)
        self.api_key = get_api_key('openai')
        
        if not self.api_key:
            raise ValueError(
                "âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
                "   ë‹¤ìŒ ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ ì„¤ì •í•˜ì„¸ìš”:\n"
                "   1. config/config.yamlì˜ api_keys.openaiì— ì…ë ¥\n"
                "   2. config/.env íŒŒì¼ì— OPENAI_API_KEY=sk-... í˜•ì‹ìœ¼ë¡œ ì…ë ¥\n"
                "   3. í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •: export OPENAI_API_KEY=sk-..."
            )
        
        self.client = OpenAI(api_key=self.api_key)
        
        # ìŒì„± íŠ¹ì„± ë¶„ì„ê¸° ì´ˆê¸°í™” (configì—ì„œ ì„¤ì • í™•ì¸)
        analysis_cfg = get_config('analysis', default={})
        self.use_voice_characteristics = analysis_cfg.get('voice_characteristics', True)
        self.use_streaming = analysis_cfg.get('streaming', False)
        
        if VOICE_ANALYSIS_AVAILABLE and self.use_voice_characteristics:
            self.voice_analyzer = VoiceCharacteristicsAnalyzer()
        else:
            self.voice_analyzer = None
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (configì—ì„œ ë¡œë“œ)
        self.system_prompt = get_prompt('system') or self.DEFAULT_SYSTEM_PROMPT
        
        # OpenAI API ì„¤ì •
        self.max_tokens = get_openai_config('max_tokens', default=800)
        self.temperature = get_openai_config('temperature', default=0.3)
        self.image_detail = get_openai_config('image_detail', default='low')
    
    def encode_image_to_base64(self, image_source: Union[str, np.ndarray], max_size: int = 1024) -> str:
        """
        ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”© (í¬ê¸° ìµœì í™” í¬í•¨)
        
        Args:
            image_source: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” numpy array (OpenCV ì´ë¯¸ì§€)
            max_size: ìµœëŒ€ ì´ë¯¸ì§€ í¬ê¸° (í”½ì…€). ì´ë³´ë‹¤ í¬ë©´ ë¦¬ì‚¬ì´ì§• (ê¸°ë³¸ê°’: 1024)
        
        Returns:
            base64 ì¸ì½”ë”©ëœ ë¬¸ìì—´
        """
        if isinstance(image_source, str):
            # íŒŒì¼ ê²½ë¡œì¸ ê²½ìš° - PILë¡œ ì—´ì–´ì„œ ë¦¬ì‚¬ì´ì§•
            if not PIL_AVAILABLE:
                # PILì´ ì—†ìœ¼ë©´ ì›ë³¸ ê·¸ëŒ€ë¡œ ì¸ì½”ë”©
                with open(image_source, "rb") as image_file:
                    return base64.b64encode(image_file.read()).decode('utf-8')
            
            # PILë¡œ ì´ë¯¸ì§€ ì—´ê¸°
            img = Image.open(image_source)
            
            # í¬ê¸° ì¡°ì • (ê¸´ ìª½ì´ max_sizeë¥¼ ë„˜ìœ¼ë©´ ë¦¬ì‚¬ì´ì§•)
            if max(img.size) > max_size:
                ratio = max_size / max(img.size)
                new_size = tuple(int(dim * ratio) for dim in img.size)
                img = img.resize(new_size, Image.Resampling.LANCZOS)
            
            # JPEGë¡œ ë³€í™˜í•˜ì—¬ base64 ì¸ì½”ë”©
            from io import BytesIO
            buffer = BytesIO()
            img.convert('RGB').save(buffer, format='JPEG', quality=85)
            return base64.b64encode(buffer.getvalue()).decode('utf-8')
        
        elif isinstance(image_source, np.ndarray):
            # numpy array (OpenCV ì´ë¯¸ì§€)ì¸ ê²½ìš°
            if not OPENCV_AVAILABLE:
                raise ImportError("OpenCVê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install opencv-python")
            
            # í¬ê¸° ì¡°ì •
            height, width = image_source.shape[:2]
            if max(height, width) > max_size:
                ratio = max_size / max(height, width)
                new_width = int(width * ratio)
                new_height = int(height * ratio)
                image_source = cv2.resize(image_source, (new_width, new_height), interpolation=cv2.INTER_LANCZOS4)
            
            # BGR to RGB ë³€í™˜ (OpenCVëŠ” BGR, PILì€ RGB)
            if len(image_source.shape) == 3 and image_source.shape[2] == 3:
                image_rgb = cv2.cvtColor(image_source, cv2.COLOR_BGR2RGB)
            else:
                image_rgb = image_source
            
            # numpy arrayë¥¼ JPEGë¡œ ì¸ì½”ë”© (í’ˆì§ˆ 85%)
            encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 85]
            _, buffer = cv2.imencode('.jpg', image_rgb, encode_param)
            return base64.b64encode(buffer).decode('utf-8')
        
        else:
            raise TypeError("image_sourceëŠ” íŒŒì¼ ê²½ë¡œ(str) ë˜ëŠ” numpy arrayì—¬ì•¼ í•©ë‹ˆë‹¤")
    
    def analyze_with_image(
        self, 
        audio_text: str, 
        image_source: Union[str, np.ndarray],
        additional_context: Optional[str] = None,
        audio_file_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ì˜¤ë””ì˜¤ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ë¶„ì„ (ìŒì„± íŠ¹ì„± ë¶„ì„ í¬í•¨)
        
        Args:
            audio_text: ìŒì„±ì—ì„œ ë³€í™˜ëœ í…ìŠ¤íŠ¸
            image_source: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” numpy array
            additional_context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ (ì„ íƒ)
            audio_file_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ìŒì„± íŠ¹ì„± ë¶„ì„ìš©, ì„ íƒ)
        
        Returns:
            ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            base64_image = self.encode_image_to_base64(image_source)
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ êµ¬ì„± (ìŒì„± + íŠ¹ì„± + ì˜ìƒ)
            user_message = f"""**1. ìŒì„± ì…ë ¥:**
"{audio_text}"
"""
            
            if additional_context:
                user_message += f"""
**2. ìŒì„± íŠ¹ì„± ë¶„ì„ ê²°ê³¼:**
{additional_context}
"""
            else:
                print("âš ï¸  ìŒì„± íŠ¹ì„± ë¶„ì„ ì •ë³´ ì—†ìŒ")
            
            user_message += f"""
**3. ì˜ìƒ:**
ì œê³µëœ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ìœ„ ìŒì„±ê³¼ ìŒì„± íŠ¹ì„±ê³¼ í•¨ê»˜ ì „ì²´ ìƒí™©ì„ íŒë‹¨í•´ì£¼ì„¸ìš”.
"""
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [
                {
                    "role": "system",
                    "content": self.system_prompt
                },
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": user_message
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}",
                                "detail": self.image_detail
                            }
                        }
                    ]
                }
            ]
            
            # OpenAI API í˜¸ì¶œ (ìŠ¤íŠ¸ë¦¬ë° ë˜ëŠ” ì¼ë°˜)
            if self.use_streaming:
                content = ""
                print("   ", end="", flush=True)
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    max_tokens=self.max_tokens,
                    temperature=self.temperature,
                    stream=True
                )
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        chunk_content = chunk.choices[0].delta.content
                        content += chunk_content
                        print("â–“", end="", flush=True)  # ì§„í–‰ í‘œì‹œ
                print(" âœ“")  # ì™„ë£Œ í‘œì‹œ
            else:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    max_tokens=self.max_tokens,
                    temperature=self.temperature
                )
                content = response.choices[0].message.content
            
            # ì•ˆì „ ì •ì±… ê±°ë¶€ ê°ì§€
            if content and ("I'm sorry" in content or "I can't assist" in content or "I cannot" in content):
                print(f"âš ï¸  OpenAI ì•ˆì „ ì •ì±…ìœ¼ë¡œ ì¸í•œ ë¶„ì„ ê±°ë¶€")
                print(f"   ì›ë³¸ ì‘ë‹µ: {content}")
                return {
                    'context': 'ì´ë¯¸ì§€ ë‚´ìš©ì„ ë¶„ì„í•  ìˆ˜ ì—†ìŒ (ì•ˆì „ ì •ì±…)',
                    'urgency': 'ë‚®ìŒ',
                    'urgency_reason': 'OpenAI ì•ˆì „ ì •ì±…ìœ¼ë¡œ ì¸í•œ ë¶„ì„ ì œí•œ',
                    'situation': 'ì´ë¯¸ì§€ê°€ ì•ˆì „ ì •ì±…ì— ìœ„ë°°ë˜ê±°ë‚˜ ë¶„ì„ì´ ì œí•œë˜ì—ˆìŠµë‹ˆë‹¤.',
                    'situation_type': 'ë¶„ì„ ì œí•œ',
                    'emotional_state': 'ì¤‘ë¦½',
                    'visual_content': 'ë¶„ì„ ë¶ˆê°€',
                    'audio_visual_consistency': 'N/A',
                    'action': 'ë‹¤ë¥¸ ì´ë¯¸ì§€ë¡œ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ì´ë¯¸ì§€ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”',
                    'is_emergency': False,
                    'emergency_reason': None,
                    'priority': 'LOW',
                    'error': 'ì•ˆì „ ì •ì±… ê±°ë¶€',
                    'raw_response': content
                }
            
            # JSON íŒŒì‹±
            try:
                # ì½”ë“œ ë¸”ë¡ ì œê±° (```json ... ```)
                if '```json' in content:
                    content = content.split('```json')[1].split('```')[0].strip()
                elif '```' in content:
                    content = content.split('```')[1].split('```')[0].strip()
                
                result = json.loads(content)
            
            except json.JSONDecodeError as e:
                print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                print(f"ì›ë³¸ ì‘ë‹µ:\n{content}")
                return {
                    'error': 'JSON íŒŒì‹± ì‹¤íŒ¨',
                    'raw_response': content,
                    'context': 'ë¶„ì„ ì˜¤ë¥˜',
                    'is_emergency': False,
                    'priority': 'LOW'
                }
            
            # LLMì˜ íŒë‹¨ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë³„ë„ ì¡°ì • ì—†ìŒ)
            # LLMì´ ì´ë¯¸ ìŒì„± íŠ¹ì„±ì„ ê³ ë ¤í•´ì„œ íŒë‹¨í–ˆìœ¼ë¯€ë¡œ ì‹ ë¢°
            if 'urgency' in result:
                del result['urgency']
            
            return result
        
        except Exception as e:
            print(f"âŒ ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                'error': str(e),
                'context': 'ë¶„ì„ ì‹¤íŒ¨',
                'is_emergency': False,
                'priority': 'LOW'
            }
    
    def analyze_with_video_frame(
        self,
        audio_text: str,
        frame: np.ndarray,
        additional_context: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ì˜¤ë””ì˜¤ í…ìŠ¤íŠ¸ì™€ ë¹„ë””ì˜¤ í”„ë ˆì„ì„ í•¨ê»˜ ë¶„ì„
        
        Args:
            audio_text: ìŒì„±ì—ì„œ ë³€í™˜ëœ í…ìŠ¤íŠ¸
            frame: ë¹„ë””ì˜¤ í”„ë ˆì„ (numpy array, OpenCV format)
            additional_context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ (ì„ íƒ)
        
        Returns:
            ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # analyze_with_imageì™€ ë™ì¼í•˜ê²Œ ì²˜ë¦¬ (í”„ë ˆì„ì€ ì´ë¯¸ì§€ë¡œ ì·¨ê¸‰)
        return self.analyze_with_image(audio_text, frame, additional_context)
    
    def capture_screenshot(self, save_path: Optional[str] = None) -> str:
        """
        í™”ë©´ ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ (macOS)
        
        Args:
            save_path: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if save_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
            save_path = f"screenshots/screenshot_{timestamp}.png"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        
        try:
            import subprocess
            # macOS screencapture ëª…ë ¹ ì‚¬ìš©
            subprocess.run(['screencapture', '-x', save_path], check=True)
            print(f"âœ… ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {save_path}")
            return save_path
        except Exception as e:
            print(f"âŒ ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ì‹¤íŒ¨: {e}")
            return None
    
    def capture_webcam_frame(self, camera_id: int = 0, save_path: Optional[str] = None) -> Union[str, np.ndarray]:
        """
        ì›¹ìº ì—ì„œ í”„ë ˆì„ ìº¡ì²˜
        
        Args:
            camera_id: ì¹´ë©”ë¼ ID (ê¸°ë³¸ê°’: 0)
            save_path: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ì €ì¥í•˜ì§€ ì•Šê³  numpy array ë°˜í™˜)
        
        Returns:
            ì €ì¥ ê²½ë¡œ ë˜ëŠ” numpy array
        """
        if not OPENCV_AVAILABLE:
            raise ImportError("OpenCVê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install opencv-python")
        
        try:
            cap = cv2.VideoCapture(camera_id)
            
            if not cap.isOpened():
                raise RuntimeError(f"ì¹´ë©”ë¼ {camera_id}ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # í”„ë ˆì„ ì½ê¸°
            ret, frame = cap.read()
            cap.release()
            
            if not ret:
                raise RuntimeError("í”„ë ˆì„ ìº¡ì²˜ ì‹¤íŒ¨")
            
            if save_path:
                # ë””ë ‰í† ë¦¬ ìƒì„±
                Path(save_path).parent.mkdir(parents=True, exist_ok=True)
                cv2.imwrite(save_path, frame)
                print(f"âœ… ì›¹ìº  í”„ë ˆì„ ì €ì¥: {save_path}")
                return save_path
            else:
                return frame
        
        except Exception as e:
            print(f"âŒ ì›¹ìº  ìº¡ì²˜ ì‹¤íŒ¨: {e}")
            return None


# í¸ì˜ í•¨ìˆ˜
def analyze_audio_with_screenshot(audio_text: str, screenshot_path: Optional[str] = None) -> Dict[str, Any]:
    """
    ìŒì„± í…ìŠ¤íŠ¸ì™€ ìŠ¤í¬ë¦°ìƒ·ì„ í•¨ê»˜ ë¶„ì„í•˜ëŠ” í¸ì˜ í•¨ìˆ˜
    
    Args:
        audio_text: ìŒì„±ì—ì„œ ë³€í™˜ëœ í…ìŠ¤íŠ¸
        screenshot_path: ìŠ¤í¬ë¦°ìƒ· ê²½ë¡œ (Noneì´ë©´ ìë™ ìº¡ì²˜)
    
    Returns:
        ë¶„ì„ ê²°ê³¼
    """
    analyzer = MultimodalAnalyzer()
    
    # ìŠ¤í¬ë¦°ìƒ·ì´ ì—†ìœ¼ë©´ ìë™ ìº¡ì²˜
    if screenshot_path is None:
        screenshot_path = analyzer.capture_screenshot()
        if screenshot_path is None:
            return {'error': 'ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ì‹¤íŒ¨'}
    
    return analyzer.analyze_with_image(audio_text, screenshot_path)


def analyze_audio_with_webcam(audio_text: str, camera_id: int = 0) -> Dict[str, Any]:
    """
    ìŒì„± í…ìŠ¤íŠ¸ì™€ ì›¹ìº  í”„ë ˆì„ì„ í•¨ê»˜ ë¶„ì„í•˜ëŠ” í¸ì˜ í•¨ìˆ˜
    
    Args:
        audio_text: ìŒì„±ì—ì„œ ë³€í™˜ëœ í…ìŠ¤íŠ¸
        camera_id: ì¹´ë©”ë¼ ID
    
    Returns:
        ë¶„ì„ ê²°ê³¼
    """
    analyzer = MultimodalAnalyzer()
    
    # ì›¹ìº ì—ì„œ í”„ë ˆì„ ìº¡ì²˜
    frame = analyzer.capture_webcam_frame(camera_id)
    if frame is None:
        return {'error': 'ì›¹ìº  í”„ë ˆì„ ìº¡ì²˜ ì‹¤íŒ¨'}
    
    return analyzer.analyze_with_video_frame(audio_text, frame)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("=" * 70)
    print("ğŸ¥ ë©€í‹°ëª¨ë‹¬ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    analyzer = MultimodalAnalyzer()
    
    # 1. ìŠ¤í¬ë¦°ìƒ·ê³¼ í•¨ê»˜ ë¶„ì„
    print("\n1ï¸âƒ£ ìŠ¤í¬ë¦°ìƒ· ë¶„ì„ í…ŒìŠ¤íŠ¸")
    screenshot = analyzer.capture_screenshot()
    if screenshot:
        result = analyzer.analyze_with_image(
            audio_text="ì´ìƒí•œ ì†Œë¦¬ê°€ ë“¤ë ¤ìš”",
            image_source=screenshot
        )
        print(f"ë¶„ì„ ê²°ê³¼: {json.dumps(result, ensure_ascii=False, indent=2)}")
    
    # 2. ì›¹ìº ê³¼ í•¨ê»˜ ë¶„ì„
    print("\n2ï¸âƒ£ ì›¹ìº  ë¶„ì„ í…ŒìŠ¤íŠ¸")
    frame = analyzer.capture_webcam_frame()
    if frame is not None:
        result = analyzer.analyze_with_video_frame(
            audio_text="ì§€ê¸ˆ ìƒí™©ì´ ì–´ë–¤ê°€ìš”?",
            frame=frame
        )
        print(f"ë¶„ì„ ê²°ê³¼: {json.dumps(result, ensure_ascii=False, indent=2)}")
