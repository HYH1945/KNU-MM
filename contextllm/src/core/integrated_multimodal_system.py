#!/usr/bin/env python3
"""
í†µí•© ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ
ìŒì„± ê°ì§€ ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í•˜ë©°, ìŒì„±ì´ ê°ì§€ë˜ë©´ ë™ì‹œì—:
1. ìŒì„± íŠ¹ì„± ë¶„ì„ (í”¼ì¹˜, ì—ë„ˆì§€, ì†ë„ ë“±)
2. ì˜ìƒ ìº¡ì²˜ ë° ë¶„ì„ (ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©)

ì‚¬ìš©ë²•:
    system = IntegratedMultimodalSystem()
    
    # ìŒì„± ê°ì§€ ì‹œ ìë™ìœ¼ë¡œ ì˜ìƒ ë¶„ì„
    system.start_monitoring()
    
    # ë˜ëŠ” ë‹¨ë°œì„± ë¶„ì„
    result = system.analyze_once()
"""

import os
import sys
import json
import cv2
import numpy as np
import threading
import queue
import time
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple, Callable
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed

try:
    import speech_recognition as sr
    SPEECH_RECOGNITION_AVAILABLE = True
except ImportError:
    SPEECH_RECOGNITION_AVAILABLE = False
    print("âš ï¸  speech_recognitionì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install SpeechRecognition")

try:
    from openai import OpenAI
    from dotenv import load_dotenv
    load_dotenv()
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("âš ï¸  OpenAIê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install openai python-dotenv")

# ë‚´ë¶€ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from core.voice_characteristics import VoiceCharacteristicsAnalyzer
    VOICE_CHARACTERISTICS_AVAILABLE = True
except ImportError:
    try:
        from voice_characteristics import VoiceCharacteristicsAnalyzer
        VOICE_CHARACTERISTICS_AVAILABLE = True
    except ImportError:
        VOICE_CHARACTERISTICS_AVAILABLE = False
        print("âš ï¸  ìŒì„± íŠ¹ì„± ë¶„ì„ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

try:
    from core.multimodal_analyzer import MultimodalAnalyzer
    MULTIMODAL_ANALYZER_AVAILABLE = True
except ImportError:
    try:
        from multimodal_analyzer import MultimodalAnalyzer
        MULTIMODAL_ANALYZER_AVAILABLE = True
    except ImportError:
        MULTIMODAL_ANALYZER_AVAILABLE = False
        print("âš ï¸  ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")


@dataclass
class DownsamplingConfig:
    """ë‹¤ìš´ìƒ˜í”Œë§ ì„¤ì •"""
    # ì´ë¯¸ì§€ ë‹¤ìš´ìƒ˜í”Œë§
    max_image_size: int = 640  # ìµœëŒ€ ì´ë¯¸ì§€ í¬ê¸° (í”½ì…€)
    jpeg_quality: int = 75  # JPEG í’ˆì§ˆ (1-100)
    
    # ë¹„ë””ì˜¤ ë‹¤ìš´ìƒ˜í”Œë§
    video_fps: float = 2.0  # ë¶„ì„ìš© FPS (ì›ë³¸ì—ì„œ ìƒ˜í”Œë§)
    max_video_frames: int = 10  # ìµœëŒ€ ë¶„ì„ í”„ë ˆì„ ìˆ˜ (5ì´ˆ * 2fps = 10)
    video_resolution_scale: float = 0.5  # ë¹„ë””ì˜¤ í•´ìƒë„ ìŠ¤ì¼€ì¼ (0.5 = 50%)
    
    # ë¹„ë””ì˜¤ ìº¡ì²˜ ì‹œê°„
    video_capture_duration: float = 5.0  # ìº¡ì²˜í•  ë¹„ë””ì˜¤ ê¸¸ì´ (ì´ˆ)


class VideoSourceType:
    """ë¹„ë””ì˜¤ ì†ŒìŠ¤ íƒ€ì…"""
    WEBCAM = "webcam"           # ì›¹ìº 
    FILE = "file"               # íŒŒì¼ (ì´ë¯¸ì§€/ë¹„ë””ì˜¤)
    NETWORK = "network"         # ë„¤íŠ¸ì›Œí¬ ì¹´ë©”ë¼ (RTSP/HTTP)
    TESTSET = "testset"         # í…ŒìŠ¤íŠ¸ì…‹ í´ë”


class VideoDownsampler:
    """ë¹„ë””ì˜¤/ì´ë¯¸ì§€ ë‹¤ìš´ìƒ˜í”Œë§ ìœ í‹¸ë¦¬í‹°"""
    
    def __init__(self, config: DownsamplingConfig = None):
        self.config = config or DownsamplingConfig()
    
    def downsample_image(self, image: np.ndarray) -> np.ndarray:
        """
        ì´ë¯¸ì§€ ë‹¤ìš´ìƒ˜í”Œë§
        
        Args:
            image: ì›ë³¸ ì´ë¯¸ì§€ (numpy array, BGR format)
        
        Returns:
            ë‹¤ìš´ìƒ˜í”Œë§ëœ ì´ë¯¸ì§€
        """
        if image is None:
            return None
        
        height, width = image.shape[:2]
        max_size = self.config.max_image_size
        
        # í¬ê¸°ê°€ max_sizeë³´ë‹¤ í¬ë©´ ë¦¬ì‚¬ì´ì§•
        if max(height, width) > max_size:
            scale = max_size / max(height, width)
            new_width = int(width * scale)
            new_height = int(height * scale)
            
            # INTER_AREA: ì¶•ì†Œì— ì í•©í•œ ë³´ê°„ë²•
            image = cv2.resize(image, (new_width, new_height), 
                             interpolation=cv2.INTER_AREA)
            
        return image
    
    def downsample_video_frames(
        self, 
        frames: List[np.ndarray], 
        timestamps: List[float] = None
    ) -> Tuple[List[np.ndarray], List[float]]:
        """
        ë¹„ë””ì˜¤ í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸ ë‹¤ìš´ìƒ˜í”Œë§
        
        Args:
            frames: í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸
            timestamps: íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            (ë‹¤ìš´ìƒ˜í”Œë§ëœ í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸, íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬ìŠ¤íŠ¸)
        """
        if not frames:
            return [], []
        
        # ìµœëŒ€ í”„ë ˆì„ ìˆ˜ë¡œ ì œí•œ
        max_frames = self.config.max_video_frames
        if len(frames) > max_frames:
            # ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§
            indices = np.linspace(0, len(frames) - 1, max_frames, dtype=int)
            frames = [frames[i] for i in indices]
            if timestamps:
                timestamps = [timestamps[i] for i in indices]
        
        # ê° í”„ë ˆì„ ë‹¤ìš´ìƒ˜í”Œë§
        downsampled_frames = []
        scale = self.config.video_resolution_scale
        
        for frame in frames:
            if scale < 1.0:
                new_width = int(frame.shape[1] * scale)
                new_height = int(frame.shape[0] * scale)
                frame = cv2.resize(frame, (new_width, new_height), 
                                 interpolation=cv2.INTER_AREA)
            
            # ì¶”ê°€ë¡œ max_image_size ì ìš©
            frame = self.downsample_image(frame)
            downsampled_frames.append(frame)
        
        return downsampled_frames, timestamps or []
    
    def encode_frame_to_jpeg(self, frame: np.ndarray) -> bytes:
        """í”„ë ˆì„ì„ JPEG ë°”ì´íŠ¸ë¡œ ì¸ì½”ë”©"""
        encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), self.config.jpeg_quality]
        _, buffer = cv2.imencode('.jpg', frame, encode_param)
        return buffer.tobytes()


class BaseVideoSource:
    """ë¹„ë””ì˜¤ ì†ŒìŠ¤ ê¸°ë³¸ í´ë˜ìŠ¤ (ì¶”ìƒ)"""
    
    def __init__(self):
        self.is_opened = False
        self.lock = threading.Lock()
        self.source_type = None
    
    def open(self) -> bool:
        """ì†ŒìŠ¤ ì—´ê¸°"""
        raise NotImplementedError
    
    def close(self):
        """ì†ŒìŠ¤ ë‹«ê¸°"""
        raise NotImplementedError
    
    def capture_frame(self) -> Optional[np.ndarray]:
        """ë‹¨ì¼ í”„ë ˆì„ ìº¡ì²˜"""
        raise NotImplementedError
    
    def capture_video_segment(
        self, 
        duration: float = 5.0, 
        target_fps: float = 2.0
    ) -> Tuple[List[np.ndarray], List[float]]:
        """ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ìº¡ì²˜"""
        raise NotImplementedError
    
    def get_info(self) -> Dict[str, Any]:
        """ì†ŒìŠ¤ ì •ë³´ ë°˜í™˜"""
        return {
            "source_type": self.source_type,
            "is_opened": self.is_opened
        }


class WebcamVideoSource(BaseVideoSource):
    """ì›¹ìº  ë¹„ë””ì˜¤ ì†ŒìŠ¤"""
    
    def __init__(self, camera_id: int = 0):
        super().__init__()
        self.camera_id = camera_id
        self.cap = None
        self.source_type = VideoSourceType.WEBCAM
    
    def open(self) -> bool:
        """ì›¹ìº  ì—´ê¸°"""
        with self.lock:
            if self.is_opened:
                return True
            
            self.cap = cv2.VideoCapture(self.camera_id)
            if self.cap.isOpened():
                self.is_opened = True
                print(f"âœ… ì›¹ìº  {self.camera_id} ì—´ë¦¼")
                return True
            else:
                print(f"âŒ ì›¹ìº  {self.camera_id}ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
    
    def close(self):
        """ì›¹ìº  ë‹«ê¸°"""
        with self.lock:
            if self.cap:
                self.cap.release()
                self.cap = None
                self.is_opened = False
                print("âœ… ì›¹ìº  ë‹«í˜")
    
    def capture_frame(self) -> Optional[np.ndarray]:
        """ë‹¨ì¼ í”„ë ˆì„ ìº¡ì²˜"""
        with self.lock:
            if not self.is_opened or not self.cap:
                return None
            
            ret, frame = self.cap.read()
            if ret:
                return frame
            return None
    
    def capture_video_segment(
        self, 
        duration: float = 5.0, 
        target_fps: float = 2.0
    ) -> Tuple[List[np.ndarray], List[float]]:
        """ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ìº¡ì²˜"""
        frames = []
        timestamps = []
        
        with self.lock:
            if not self.is_opened or not self.cap:
                print("âŒ ì›¹ìº ì´ ì—´ë ¤ìˆì§€ ì•ŠìŠµë‹ˆë‹¤")
                return frames, timestamps
            
            original_fps = self.cap.get(cv2.CAP_PROP_FPS) or 30.0
            frame_interval = int(original_fps / target_fps) if target_fps < original_fps else 1
            
            start_time = time.time()
            frame_count = 0
            
            print(f"ğŸ“¹ ì›¹ìº  ìº¡ì²˜ ì¤‘ ({duration}ì´ˆ, {target_fps}fps)...")
            
            while (time.time() - start_time) < duration:
                ret, frame = self.cap.read()
                if not ret:
                    break
                
                if frame_count % frame_interval == 0:
                    timestamp = time.time() - start_time
                    frames.append(frame.copy())
                    timestamps.append(timestamp)
                
                frame_count += 1
            
            print(f"   âœ… {len(frames)}ê°œ í”„ë ˆì„ ìº¡ì²˜ ì™„ë£Œ")
        
        return frames, timestamps
    
    def get_info(self) -> Dict[str, Any]:
        info = super().get_info()
        info["camera_id"] = self.camera_id
        if self.cap and self.is_opened:
            info["fps"] = self.cap.get(cv2.CAP_PROP_FPS)
            info["width"] = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            info["height"] = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        return info


class NetworkVideoSource(BaseVideoSource):
    """ë„¤íŠ¸ì›Œí¬ ì¹´ë©”ë¼ ë¹„ë””ì˜¤ ì†ŒìŠ¤ (RTSP/HTTP)"""
    
    def __init__(self, url: str):
        """
        Args:
            url: ì¹´ë©”ë¼ URL (ì˜ˆ: rtsp://192.168.1.100:554/stream, http://192.168.1.100:8080/video)
        """
        super().__init__()
        self.url = url
        self.cap = None
        self.source_type = VideoSourceType.NETWORK
    
    def open(self) -> bool:
        """ë„¤íŠ¸ì›Œí¬ ì¹´ë©”ë¼ ì—°ê²°"""
        with self.lock:
            if self.is_opened:
                return True
            
            print(f"ğŸŒ ë„¤íŠ¸ì›Œí¬ ì¹´ë©”ë¼ ì—°ê²° ì¤‘: {self.url}")
            self.cap = cv2.VideoCapture(self.url)
            
            # ë²„í¼ í¬ê¸° ì¤„ì´ê¸° (ì§€ì—° ê°ì†Œ)
            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
            
            if self.cap.isOpened():
                self.is_opened = True
                print(f"âœ… ë„¤íŠ¸ì›Œí¬ ì¹´ë©”ë¼ ì—°ê²°ë¨: {self.url}")
                return True
            else:
                print(f"âŒ ë„¤íŠ¸ì›Œí¬ ì¹´ë©”ë¼ ì—°ê²° ì‹¤íŒ¨: {self.url}")
                return False
    
    def close(self):
        """ë„¤íŠ¸ì›Œí¬ ì¹´ë©”ë¼ ì—°ê²° ì¢…ë£Œ"""
        with self.lock:
            if self.cap:
                self.cap.release()
                self.cap = None
                self.is_opened = False
                print("âœ… ë„¤íŠ¸ì›Œí¬ ì¹´ë©”ë¼ ì—°ê²° ì¢…ë£Œ")
    
    def capture_frame(self) -> Optional[np.ndarray]:
        """ë‹¨ì¼ í”„ë ˆì„ ìº¡ì²˜"""
        with self.lock:
            if not self.is_opened or not self.cap:
                return None
            
            # ë²„í¼ ë¹„ìš°ê¸° (ìµœì‹  í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°)
            for _ in range(3):
                self.cap.grab()
            
            ret, frame = self.cap.read()
            if ret:
                return frame
            return None
    
    def capture_video_segment(
        self, 
        duration: float = 5.0, 
        target_fps: float = 2.0
    ) -> Tuple[List[np.ndarray], List[float]]:
        """ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ìº¡ì²˜"""
        frames = []
        timestamps = []
        
        with self.lock:
            if not self.is_opened or not self.cap:
                print("âŒ ë„¤íŠ¸ì›Œí¬ ì¹´ë©”ë¼ê°€ ì—°ê²°ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤")
                return frames, timestamps
            
            frame_interval = 1.0 / target_fps
            start_time = time.time()
            last_capture_time = 0
            
            print(f"ğŸ“¹ ë„¤íŠ¸ì›Œí¬ ì¹´ë©”ë¼ ìº¡ì²˜ ì¤‘ ({duration}ì´ˆ, {target_fps}fps)...")
            
            while (time.time() - start_time) < duration:
                current_time = time.time() - start_time
                
                if current_time - last_capture_time >= frame_interval:
                    ret, frame = self.cap.read()
                    if ret:
                        frames.append(frame.copy())
                        timestamps.append(current_time)
                        last_capture_time = current_time
                
                time.sleep(0.01)  # CPU ì‚¬ìš©ëŸ‰ ì¤„ì´ê¸°
            
            print(f"   âœ… {len(frames)}ê°œ í”„ë ˆì„ ìº¡ì²˜ ì™„ë£Œ")
        
        return frames, timestamps
    
    def get_info(self) -> Dict[str, Any]:
        info = super().get_info()
        info["url"] = self.url
        if self.cap and self.is_opened:
            info["fps"] = self.cap.get(cv2.CAP_PROP_FPS)
            info["width"] = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            info["height"] = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        return info


class FileVideoSource(BaseVideoSource):
    """íŒŒì¼ ê¸°ë°˜ ë¹„ë””ì˜¤ ì†ŒìŠ¤ (ì´ë¯¸ì§€ ë˜ëŠ” ë¹„ë””ì˜¤ íŒŒì¼)"""
    
    def __init__(self, file_path: str):
        """
        Args:
            file_path: ì´ë¯¸ì§€ ë˜ëŠ” ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        """
        super().__init__()
        self.file_path = Path(file_path)
        self.cap = None
        self.is_video = False
        self.is_image = False
        self.image = None
        self.source_type = VideoSourceType.FILE
        
        # íŒŒì¼ íƒ€ì… í™•ì¸
        self._detect_file_type()
    
    def _detect_file_type(self):
        """íŒŒì¼ íƒ€ì… ê°ì§€"""
        suffix = self.file_path.suffix.lower()
        video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.webm', '.flv', '.wmv'}
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp'}
        
        if suffix in video_extensions:
            self.is_video = True
        elif suffix in image_extensions:
            self.is_image = True
    
    def open(self) -> bool:
        """íŒŒì¼ ì—´ê¸°"""
        with self.lock:
            if self.is_opened:
                return True
            
            if not self.file_path.exists():
                print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.file_path}")
                return False
            
            if self.is_video:
                self.cap = cv2.VideoCapture(str(self.file_path))
                if self.cap.isOpened():
                    self.is_opened = True
                    print(f"âœ… ë¹„ë””ì˜¤ íŒŒì¼ ì—´ë¦¼: {self.file_path.name}")
                    return True
                else:
                    print(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.file_path}")
                    return False
            
            elif self.is_image:
                self.image = cv2.imread(str(self.file_path))
                if self.image is not None:
                    self.is_opened = True
                    print(f"âœ… ì´ë¯¸ì§€ íŒŒì¼ ì—´ë¦¼: {self.file_path.name}")
                    return True
                else:
                    print(f"âŒ ì´ë¯¸ì§€ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.file_path}")
                    return False
            
            else:
                print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {self.file_path.suffix}")
                return False
    
    def close(self):
        """íŒŒì¼ ë‹«ê¸°"""
        with self.lock:
            if self.cap:
                self.cap.release()
                self.cap = None
            self.image = None
            self.is_opened = False
            print(f"âœ… íŒŒì¼ ë‹«í˜: {self.file_path.name}")
    
    def capture_frame(self) -> Optional[np.ndarray]:
        """í”„ë ˆì„/ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°"""
        with self.lock:
            if not self.is_opened:
                return None
            
            if self.is_image:
                return self.image.copy() if self.image is not None else None
            
            elif self.is_video and self.cap:
                ret, frame = self.cap.read()
                if ret:
                    return frame
                else:
                    # ë¹„ë””ì˜¤ ëì— ë„ë‹¬í•˜ë©´ ì²˜ìŒìœ¼ë¡œ ë˜ê°ê¸°
                    self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                    ret, frame = self.cap.read()
                    return frame if ret else None
            
            return None
    
    def capture_video_segment(
        self, 
        duration: float = 5.0, 
        target_fps: float = 2.0
    ) -> Tuple[List[np.ndarray], List[float]]:
        """ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ìº¡ì²˜"""
        frames = []
        timestamps = []
        
        with self.lock:
            if not self.is_opened:
                print("âŒ íŒŒì¼ì´ ì—´ë ¤ìˆì§€ ì•ŠìŠµë‹ˆë‹¤")
                return frames, timestamps
            
            if self.is_image:
                # ì´ë¯¸ì§€ì¸ ê²½ìš° ê°™ì€ ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ ë²ˆ ë°˜í™˜
                num_frames = int(duration * target_fps)
                print(f"ğŸ“· ì´ë¯¸ì§€ì—ì„œ {num_frames}ê°œ í”„ë ˆì„ ìƒì„±...")
                
                for i in range(num_frames):
                    frames.append(self.image.copy())
                    timestamps.append(i / target_fps)
                
                print(f"   âœ… {len(frames)}ê°œ í”„ë ˆì„ ìƒì„± ì™„ë£Œ")
                return frames, timestamps
            
            elif self.is_video and self.cap:
                original_fps = self.cap.get(cv2.CAP_PROP_FPS) or 30.0
                total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
                video_duration = total_frames / original_fps
                
                # ìš”ì²­ëœ ì‹œê°„ì´ ë¹„ë””ì˜¤ ê¸¸ì´ë³´ë‹¤ ê¸¸ë©´ ë¹„ë””ì˜¤ ê¸¸ì´ë¡œ ì œí•œ
                actual_duration = min(duration, video_duration)
                
                frame_interval = int(original_fps / target_fps) if target_fps < original_fps else 1
                frame_count = 0
                
                print(f"ğŸ“¹ ë¹„ë””ì˜¤ íŒŒì¼ì—ì„œ ìº¡ì²˜ ì¤‘ ({actual_duration:.1f}ì´ˆ, {target_fps}fps)...")
                
                # ë¹„ë””ì˜¤ ì²˜ìŒìœ¼ë¡œ ë˜ê°ê¸°
                self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                
                while True:
                    ret, frame = self.cap.read()
                    if not ret:
                        break
                    
                    current_time = frame_count / original_fps
                    if current_time > actual_duration:
                        break
                    
                    if frame_count % frame_interval == 0:
                        frames.append(frame.copy())
                        timestamps.append(current_time)
                    
                    frame_count += 1
                
                print(f"   âœ… {len(frames)}ê°œ í”„ë ˆì„ ìº¡ì²˜ ì™„ë£Œ")
        
        return frames, timestamps
    
    def get_info(self) -> Dict[str, Any]:
        info = super().get_info()
        info["file_path"] = str(self.file_path)
        info["is_video"] = self.is_video
        info["is_image"] = self.is_image
        
        if self.is_video and self.cap and self.is_opened:
            info["fps"] = self.cap.get(cv2.CAP_PROP_FPS)
            info["width"] = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            info["height"] = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            info["total_frames"] = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
            info["duration"] = info["total_frames"] / info["fps"] if info["fps"] > 0 else 0
        
        elif self.is_image and self.image is not None:
            info["height"], info["width"] = self.image.shape[:2]
        
        return info
    
    def seek(self, position: float):
        """ë¹„ë””ì˜¤ì—ì„œ íŠ¹ì • ìœ„ì¹˜ë¡œ ì´ë™ (ì´ˆ ë‹¨ìœ„)"""
        if self.is_video and self.cap and self.is_opened:
            fps = self.cap.get(cv2.CAP_PROP_FPS) or 30.0
            frame_num = int(position * fps)
            self.cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)


class TestsetVideoSource(BaseVideoSource):
    """í…ŒìŠ¤íŠ¸ì…‹ í´ë” ë¹„ë””ì˜¤ ì†ŒìŠ¤ (í´ë” ë‚´ ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ìˆœì°¨ ì¬ìƒ)"""
    
    def __init__(self, folder_path: str, loop: bool = True):
        """
        Args:
            folder_path: í…ŒìŠ¤íŠ¸ì…‹ í´ë” ê²½ë¡œ
            loop: íŒŒì¼ ëì— ë„ë‹¬í•˜ë©´ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í• ì§€ ì—¬ë¶€
        """
        super().__init__()
        self.folder_path = Path(folder_path)
        self.loop = loop
        self.source_type = VideoSourceType.TESTSET
        
        self.files: List[Path] = []
        self.current_index = 0
        self.current_source: Optional[FileVideoSource] = None
    
    def _scan_files(self):
        """í´ë” ë‚´ ë¯¸ë””ì–´ íŒŒì¼ ìŠ¤ìº”"""
        video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.webm', '.flv', '.wmv'}
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp'}
        all_extensions = video_extensions | image_extensions
        
        self.files = sorted([
            f for f in self.folder_path.iterdir()
            if f.is_file() and f.suffix.lower() in all_extensions
        ])
        
        print(f"ğŸ“ í…ŒìŠ¤íŠ¸ì…‹ í´ë” ìŠ¤ìº” ì™„ë£Œ: {len(self.files)}ê°œ íŒŒì¼")
        for f in self.files:
            print(f"   - {f.name}")
    
    def open(self) -> bool:
        """í…ŒìŠ¤íŠ¸ì…‹ í´ë” ì—´ê¸°"""
        with self.lock:
            if self.is_opened:
                return True
            
            if not self.folder_path.exists():
                print(f"âŒ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.folder_path}")
                return False
            
            if not self.folder_path.is_dir():
                print(f"âŒ ë””ë ‰í† ë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤: {self.folder_path}")
                return False
            
            self._scan_files()
            
            if not self.files:
                print(f"âŒ í´ë”ì— ë¯¸ë””ì–´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.folder_path}")
                return False
            
            # ì²« ë²ˆì§¸ íŒŒì¼ ì—´ê¸°
            self.current_index = 0
            self.current_source = FileVideoSource(str(self.files[0]))
            
            if self.current_source.open():
                self.is_opened = True
                print(f"âœ… í…ŒìŠ¤íŠ¸ì…‹ ì¤€ë¹„ ì™„ë£Œ: {self.folder_path.name}")
                return True
            else:
                return False
    
    def close(self):
        """í…ŒìŠ¤íŠ¸ì…‹ ë‹«ê¸°"""
        with self.lock:
            if self.current_source:
                self.current_source.close()
                self.current_source = None
            self.is_opened = False
            self.current_index = 0
            print("âœ… í…ŒìŠ¤íŠ¸ì…‹ ë‹«í˜")
    
    def _next_file(self) -> bool:
        """ë‹¤ìŒ íŒŒì¼ë¡œ ì´ë™"""
        if self.current_source:
            self.current_source.close()
        
        self.current_index += 1
        
        if self.current_index >= len(self.files):
            if self.loop:
                self.current_index = 0
                print("ğŸ”„ í…ŒìŠ¤íŠ¸ì…‹ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘")
            else:
                print("ğŸ“ í…ŒìŠ¤íŠ¸ì…‹ ëì— ë„ë‹¬")
                return False
        
        self.current_source = FileVideoSource(str(self.files[self.current_index]))
        return self.current_source.open()
    
    def select_file(self, index: int) -> bool:
        """íŠ¹ì • ì¸ë±ìŠ¤ì˜ íŒŒì¼ ì„ íƒ"""
        with self.lock:
            if index < 0 or index >= len(self.files):
                print(f"âŒ ì˜ëª»ëœ ì¸ë±ìŠ¤: {index} (0-{len(self.files)-1})")
                return False
            
            if self.current_source:
                self.current_source.close()
            
            self.current_index = index
            self.current_source = FileVideoSource(str(self.files[index]))
            success = self.current_source.open()
            
            if success:
                print(f"ğŸ“‚ íŒŒì¼ ì„ íƒ: {self.files[index].name}")
            
            return success
    
    def select_file_by_name(self, filename: str) -> bool:
        """íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ì„ íƒ"""
        for i, f in enumerate(self.files):
            if f.name == filename or f.stem == filename:
                return self.select_file(i)
        
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
        return False
    
    def capture_frame(self) -> Optional[np.ndarray]:
        """í˜„ì¬ íŒŒì¼ì—ì„œ í”„ë ˆì„ ìº¡ì²˜"""
        with self.lock:
            if not self.is_opened or not self.current_source:
                return None
            
            return self.current_source.capture_frame()
    
    def capture_video_segment(
        self, 
        duration: float = 5.0, 
        target_fps: float = 2.0
    ) -> Tuple[List[np.ndarray], List[float]]:
        """í˜„ì¬ íŒŒì¼ì—ì„œ ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ìº¡ì²˜"""
        with self.lock:
            if not self.is_opened or not self.current_source:
                print("âŒ í…ŒìŠ¤íŠ¸ì…‹ì´ ì—´ë ¤ìˆì§€ ì•ŠìŠµë‹ˆë‹¤")
                return [], []
            
            return self.current_source.capture_video_segment(duration, target_fps)
    
    def get_info(self) -> Dict[str, Any]:
        info = super().get_info()
        info["folder_path"] = str(self.folder_path)
        info["total_files"] = len(self.files)
        info["current_index"] = self.current_index
        info["current_file"] = self.files[self.current_index].name if self.files else None
        info["loop"] = self.loop
        
        if self.current_source:
            info["current_source_info"] = self.current_source.get_info()
        
        return info
    
    def list_files(self) -> List[str]:
        """íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        return [f.name for f in self.files]


def create_video_source(
    source_type: str,
    **kwargs
) -> BaseVideoSource:
    """
    ë¹„ë””ì˜¤ ì†ŒìŠ¤ íŒ©í† ë¦¬ í•¨ìˆ˜
    
    Args:
        source_type: ì†ŒìŠ¤ íƒ€ì… (webcam, file, network, testset)
        **kwargs: ì†ŒìŠ¤ë³„ ì¶”ê°€ ì¸ì
            - webcam: camera_id (int, ê¸°ë³¸ê°’ 0)
            - file: file_path (str)
            - network: url (str)
            - testset: folder_path (str), loop (bool, ê¸°ë³¸ê°’ True)
    
    Returns:
        BaseVideoSource ì¸ìŠ¤í„´ìŠ¤
    
    Examples:
        # ì›¹ìº 
        source = create_video_source("webcam", camera_id=0)
        
        # íŒŒì¼
        source = create_video_source("file", file_path="testsets/violence.mp4")
        
        # ë„¤íŠ¸ì›Œí¬ ì¹´ë©”ë¼
        source = create_video_source("network", url="rtsp://192.168.1.100:554/stream")
        
        # í…ŒìŠ¤íŠ¸ì…‹ í´ë”
        source = create_video_source("testset", folder_path="testsets/")
    """
    if source_type == VideoSourceType.WEBCAM or source_type == "webcam":
        camera_id = kwargs.get("camera_id", 0)
        return WebcamVideoSource(camera_id)
    
    elif source_type == VideoSourceType.FILE or source_type == "file":
        file_path = kwargs.get("file_path")
        if not file_path:
            raise ValueError("file_pathê°€ í•„ìš”í•©ë‹ˆë‹¤")
        return FileVideoSource(file_path)
    
    elif source_type == VideoSourceType.NETWORK or source_type == "network":
        url = kwargs.get("url")
        if not url:
            raise ValueError("urlì´ í•„ìš”í•©ë‹ˆë‹¤")
        return NetworkVideoSource(url)
    
    elif source_type == VideoSourceType.TESTSET or source_type == "testset":
        folder_path = kwargs.get("folder_path")
        if not folder_path:
            raise ValueError("folder_pathê°€ í•„ìš”í•©ë‹ˆë‹¤")
        loop = kwargs.get("loop", True)
        return TestsetVideoSource(folder_path, loop)
    
    else:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ì†ŒìŠ¤ íƒ€ì…: {source_type}")


class VideoCaptureManager:
    """ë¹„ë””ì˜¤ ìº¡ì²˜ ê´€ë¦¬ì (ë ˆê±°ì‹œ í˜¸í™˜ + ë©€í‹° ì†ŒìŠ¤ ì§€ì›)"""
    
    def __init__(self, camera_id: int = 0):
        self.camera_id = camera_id
        self.cap = None
        self.is_opened = False
        self.lock = threading.Lock()
        
        # ìƒˆë¡œìš´ ë¹„ë””ì˜¤ ì†ŒìŠ¤ ì‹œìŠ¤í…œ
        self._video_source: Optional[BaseVideoSource] = None
    
    def set_source(self, source: BaseVideoSource):
        """ë¹„ë””ì˜¤ ì†ŒìŠ¤ ì„¤ì •"""
        if self._video_source and self._video_source.is_opened:
            self._video_source.close()
        self._video_source = source
        print(f"âœ… ë¹„ë””ì˜¤ ì†ŒìŠ¤ ì„¤ì •ë¨: {source.source_type}")
    
    def get_source(self) -> Optional[BaseVideoSource]:
        """í˜„ì¬ ë¹„ë””ì˜¤ ì†ŒìŠ¤ ë°˜í™˜"""
        return self._video_source
    
    def open(self) -> bool:
        """ì¹´ë©”ë¼/ì†ŒìŠ¤ ì—´ê¸°"""
        # ìƒˆë¡œìš´ ì†ŒìŠ¤ê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
        if self._video_source:
            result = self._video_source.open()
            self.is_opened = result
            return result
        
        # ë ˆê±°ì‹œ ëª¨ë“œ: ì›¹ìº 
        with self.lock:
            if self.is_opened:
                return True
            
            self.cap = cv2.VideoCapture(self.camera_id)
            if self.cap.isOpened():
                self.is_opened = True
                print(f"âœ… ì¹´ë©”ë¼ {self.camera_id} ì—´ë¦¼")
                return True
            else:
                print(f"âŒ ì¹´ë©”ë¼ {self.camera_id}ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
    
    def close(self):
        """ì¹´ë©”ë¼/ì†ŒìŠ¤ ë‹«ê¸°"""
        # ìƒˆë¡œìš´ ì†ŒìŠ¤ ì‹œìŠ¤í…œ
        if self._video_source:
            self._video_source.close()
            self.is_opened = False
            return
        
        # ë ˆê±°ì‹œ ëª¨ë“œ
        with self.lock:
            if self.cap:
                self.cap.release()
                self.cap = None
                self.is_opened = False
                print("âœ… ì¹´ë©”ë¼ ë‹«í˜")
    
    def capture_frame(self) -> Optional[np.ndarray]:
        """ë‹¨ì¼ í”„ë ˆì„ ìº¡ì²˜"""
        # ìƒˆë¡œìš´ ì†ŒìŠ¤ ì‹œìŠ¤í…œ
        if self._video_source:
            return self._video_source.capture_frame()
        
        # ë ˆê±°ì‹œ ëª¨ë“œ
        with self.lock:
            if not self.is_opened or not self.cap:
                return None
            
            ret, frame = self.cap.read()
            if ret:
                return frame
            return None
    
    def capture_video_segment(
        self, 
        duration: float = 5.0, 
        target_fps: float = 2.0
    ) -> Tuple[List[np.ndarray], List[float]]:
        """
        ì§€ì •ëœ ì‹œê°„ ë™ì•ˆ ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ìº¡ì²˜
        
        Args:
            duration: ìº¡ì²˜ ì‹œê°„ (ì´ˆ)
            target_fps: íƒ€ê²Ÿ FPS (ë‹¤ìš´ìƒ˜í”Œë§ìš©)
        
        Returns:
            (í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸, íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬ìŠ¤íŠ¸)
        """
        # ìƒˆë¡œìš´ ì†ŒìŠ¤ ì‹œìŠ¤í…œ
        if self._video_source:
            return self._video_source.capture_video_segment(duration, target_fps)
        
        # ë ˆê±°ì‹œ ëª¨ë“œ
        frames = []
        timestamps = []
        
        with self.lock:
            if not self.is_opened or not self.cap:
                print("âŒ ì¹´ë©”ë¼ê°€ ì—´ë ¤ìˆì§€ ì•ŠìŠµë‹ˆë‹¤")
                return frames, timestamps
            
            # ì›ë³¸ FPS ê°€ì ¸ì˜¤ê¸°
            original_fps = self.cap.get(cv2.CAP_PROP_FPS) or 30.0
            frame_interval = int(original_fps / target_fps) if target_fps < original_fps else 1
            
            start_time = time.time()
            frame_count = 0
            
            print(f"ğŸ“¹ ë¹„ë””ì˜¤ ìº¡ì²˜ ì¤‘ ({duration}ì´ˆ, {target_fps}fps)...")
            
            while (time.time() - start_time) < duration:
                ret, frame = self.cap.read()
                if not ret:
                    break
                
                # í”„ë ˆì„ ê°„ê²©ì— ë”°ë¼ ìƒ˜í”Œë§
                if frame_count % frame_interval == 0:
                    timestamp = time.time() - start_time
                    frames.append(frame.copy())
                    timestamps.append(timestamp)
                
                frame_count += 1
            
            print(f"   âœ… {len(frames)}ê°œ í”„ë ˆì„ ìº¡ì²˜ ì™„ë£Œ")
        
        return frames, timestamps


class SpeechDetector:
    """ìŒì„± ê°ì§€ ë° ì¸ì‹"""
    
    def __init__(self, energy_threshold: int = 300, pause_threshold: float = 0.8):
        """
        Args:
            energy_threshold: ìŒì„± ê°ì§€ ì—ë„ˆì§€ ì„ê³„ê°’
            pause_threshold: ë¬¸ì¥ ë íŒë‹¨ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        """
        if not SPEECH_RECOGNITION_AVAILABLE:
            raise ImportError("speech_recognitionì´ í•„ìš”í•©ë‹ˆë‹¤: pip install SpeechRecognition")
        
        self.recognizer = sr.Recognizer()
        self.recognizer.energy_threshold = energy_threshold
        self.recognizer.pause_threshold = pause_threshold
        self.recognizer.dynamic_energy_threshold = True
        
        # ë§ˆì´í¬ ì´ˆê¸°í™”
        self.microphone = sr.Microphone()
        
        # ì£¼ë³€ ì†ŒìŒ ì¡°ì •
        with self.microphone as source:
            print("ğŸ¤ ì£¼ë³€ ì†ŒìŒ ì¡°ì • ì¤‘...")
            self.recognizer.adjust_for_ambient_noise(source, duration=1)
            print(f"   âœ… ì—ë„ˆì§€ ì„ê³„ê°’: {self.recognizer.energy_threshold}")
    
    def listen_for_speech(self, timeout: float = None, phrase_time_limit: float = None) -> Tuple[Optional[sr.AudioData], bool]:
        """
        ìŒì„±ì´ ê°ì§€ë  ë•Œê¹Œì§€ ëŒ€ê¸°
        
        Args:
            timeout: ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            phrase_time_limit: ìµœëŒ€ ë°œí™” ì‹œê°„ (ì´ˆ)
        
        Returns:
            (AudioData ë˜ëŠ” None, ìŒì„± ê°ì§€ ì—¬ë¶€)
        """
        try:
            with self.microphone as source:
                print("ğŸ‘‚ ìŒì„± ëŒ€ê¸° ì¤‘...")
                audio = self.recognizer.listen(
                    source, 
                    timeout=timeout, 
                    phrase_time_limit=phrase_time_limit
                )
                print("âœ… ìŒì„± ê°ì§€ë¨!")
                return audio, True
        
        except sr.WaitTimeoutError:
            return None, False
        except Exception as e:
            print(f"âŒ ìŒì„± ê°ì§€ ì˜¤ë¥˜: {e}")
            return None, False
    
    def recognize_speech(self, audio: sr.AudioData, language: str = "ko-KR") -> Optional[str]:
        """
        ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (Google Speech Recognition)
        
        Args:
            audio: ìŒì„± ë°ì´í„°
            language: ì¸ì‹ ì–¸ì–´
        
        Returns:
            ì¸ì‹ëœ í…ìŠ¤íŠ¸ ë˜ëŠ” None
        """
        try:
            text = self.recognizer.recognize_google(audio, language=language)
            print(f"ğŸ“ ì¸ì‹ëœ í…ìŠ¤íŠ¸: {text}")
            return text
        except sr.UnknownValueError:
            print("âš ï¸  ìŒì„±ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None
        except sr.RequestError as e:
            print(f"âŒ ìŒì„± ì¸ì‹ API ì˜¤ë¥˜: {e}")
            return None
    
    def save_audio_to_wav(self, audio: sr.AudioData, output_path: str) -> str:
        """
        AudioDataë¥¼ WAV íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            audio: ìŒì„± ë°ì´í„°
            output_path: ì €ì¥ ê²½ë¡œ
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, "wb") as f:
            f.write(audio.get_wav_data())
        
        return output_path


class IntegratedMultimodalSystem:
    """í†µí•© ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ"""
    
    def __init__(
        self, 
        camera_id: int = 0,
        model: str = "gpt-4o-mini",
        downsampling_config: DownsamplingConfig = None,
        log_dir: str = None
    ):
        """
        í†µí•© ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            camera_id: ì›¹ìº  ì¹´ë©”ë¼ ID
            model: OpenAI ëª¨ë¸ëª…
            downsampling_config: ë‹¤ìš´ìƒ˜í”Œë§ ì„¤ì •
            log_dir: ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.camera_id = camera_id
        self.model = model
        self.downsampling_config = downsampling_config or DownsamplingConfig()
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.video_manager = VideoCaptureManager(camera_id)
        self.downsampler = VideoDownsampler(self.downsampling_config)
        self.speech_detector = SpeechDetector()
        
        # ë©€í‹°ëª¨ë‹¬ ë¶„ì„ê¸°
        if MULTIMODAL_ANALYZER_AVAILABLE:
            self.multimodal_analyzer = MultimodalAnalyzer(model=model)
        else:
            self.multimodal_analyzer = None
            print("âš ï¸  ë©€í‹°ëª¨ë‹¬ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ìŒì„± íŠ¹ì„± ë¶„ì„ê¸°
        if VOICE_CHARACTERISTICS_AVAILABLE:
            self.voice_characteristics_analyzer = VoiceCharacteristicsAnalyzer()
        else:
            self.voice_characteristics_analyzer = None
            print("âš ï¸  ìŒì„± íŠ¹ì„± ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ëª¨ë‹ˆí„°ë§ ìƒíƒœ
        self.is_monitoring = False
        self.monitoring_thread = None
        
        # ë¡œê·¸ ì„¤ì •
        self.log_dir = Path(log_dir) if log_dir else Path("data/logs")
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # ë…¹ìŒ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
        self.recordings_dir = Path("recordings")
        self.recordings_dir.mkdir(parents=True, exist_ok=True)
        
        # ì½œë°± í•¨ìˆ˜
        self.on_result_callback: Optional[Callable[[Dict], None]] = None
        
        print("âœ… í†µí•© ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # ==================== ë¹„ë””ì˜¤ ì†ŒìŠ¤ ì„¤ì • ë©”ì„œë“œ ====================
    
    def set_video_source(self, source: BaseVideoSource):
        """
        ë¹„ë””ì˜¤ ì†ŒìŠ¤ ì§ì ‘ ì„¤ì •
        
        Args:
            source: BaseVideoSource ì¸ìŠ¤í„´ìŠ¤
        """
        self.video_manager.set_source(source)
    
    def use_webcam(self, camera_id: int = 0):
        """
        ì›¹ìº ì„ ë¹„ë””ì˜¤ ì†ŒìŠ¤ë¡œ ì‚¬ìš©
        
        Args:
            camera_id: ì¹´ë©”ë¼ ID (ê¸°ë³¸ê°’: 0)
        """
        source = WebcamVideoSource(camera_id)
        self.video_manager.set_source(source)
        print(f"ğŸ“¹ ë¹„ë””ì˜¤ ì†ŒìŠ¤: ì›¹ìº  (ID: {camera_id})")
    
    def use_file(self, file_path: str):
        """
        íŒŒì¼(ì´ë¯¸ì§€/ë¹„ë””ì˜¤)ì„ ë¹„ë””ì˜¤ ì†ŒìŠ¤ë¡œ ì‚¬ìš©
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
        """
        source = FileVideoSource(file_path)
        self.video_manager.set_source(source)
        print(f"ğŸ“¹ ë¹„ë””ì˜¤ ì†ŒìŠ¤: íŒŒì¼ ({file_path})")
    
    def use_network_camera(self, url: str):
        """
        ë„¤íŠ¸ì›Œí¬ ì¹´ë©”ë¼(RTSP/HTTP)ë¥¼ ë¹„ë””ì˜¤ ì†ŒìŠ¤ë¡œ ì‚¬ìš©
        
        Args:
            url: ì¹´ë©”ë¼ URL
                - RTSP: rtsp://username:password@192.168.1.100:554/stream
                - HTTP: http://192.168.1.100:8080/video
        """
        source = NetworkVideoSource(url)
        self.video_manager.set_source(source)
        print(f"ğŸ“¹ ë¹„ë””ì˜¤ ì†ŒìŠ¤: ë„¤íŠ¸ì›Œí¬ ì¹´ë©”ë¼ ({url})")
    
    def use_testset(self, folder_path: str, loop: bool = True):
        """
        í…ŒìŠ¤íŠ¸ì…‹ í´ë”ë¥¼ ë¹„ë””ì˜¤ ì†ŒìŠ¤ë¡œ ì‚¬ìš©
        
        Args:
            folder_path: í…ŒìŠ¤íŠ¸ì…‹ í´ë” ê²½ë¡œ
            loop: íŒŒì¼ ëì— ë„ë‹¬í•˜ë©´ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í• ì§€ ì—¬ë¶€
        """
        source = TestsetVideoSource(folder_path, loop)
        self.video_manager.set_source(source)
        print(f"ğŸ“¹ ë¹„ë””ì˜¤ ì†ŒìŠ¤: í…ŒìŠ¤íŠ¸ì…‹ ({folder_path})")
    
    def get_testset_files(self) -> List[str]:
        """í…ŒìŠ¤íŠ¸ì…‹ì˜ íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        source = self.video_manager.get_source()
        if isinstance(source, TestsetVideoSource):
            return source.list_files()
        return []
    
    def select_testset_file(self, index_or_name) -> bool:
        """
        í…ŒìŠ¤íŠ¸ì…‹ì—ì„œ íŠ¹ì • íŒŒì¼ ì„ íƒ
        
        Args:
            index_or_name: íŒŒì¼ ì¸ë±ìŠ¤(int) ë˜ëŠ” íŒŒì¼ëª…(str)
        """
        source = self.video_manager.get_source()
        if not isinstance(source, TestsetVideoSource):
            print("âŒ í˜„ì¬ í…ŒìŠ¤íŠ¸ì…‹ ëª¨ë“œê°€ ì•„ë‹™ë‹ˆë‹¤")
            return False
        
        if isinstance(index_or_name, int):
            return source.select_file(index_or_name)
        else:
            return source.select_file_by_name(index_or_name)
    
    def get_video_source_info(self) -> Dict[str, Any]:
        """í˜„ì¬ ë¹„ë””ì˜¤ ì†ŒìŠ¤ ì •ë³´ ë°˜í™˜"""
        source = self.video_manager.get_source()
        if source:
            return source.get_info()
        return {"source_type": "legacy_webcam", "camera_id": self.camera_id}
    
    # ==================== í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ìŒì„± ì…ë ¥ ì—†ì´ ì˜ìƒë§Œ ë¶„ì„) ====================
    
    def analyze_video_only(self, text_input: str = None) -> Dict[str, Any]:
        """
        ìŒì„± ì…ë ¥ ì—†ì´ ì˜ìƒë§Œ ë¶„ì„ (í…ŒìŠ¤íŠ¸ìš©)
        
        Args:
            text_input: ìŒì„± ëŒ€ì‹  ì‚¬ìš©í•  í…ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©)
        
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        result = {
            "timestamp": datetime.now().isoformat(),
            "success": False,
            "mode": "video_only",
            "text_input": text_input or "(í…ŒìŠ¤íŠ¸ ëª¨ë“œ - ìŒì„± ì…ë ¥ ì—†ìŒ)",
            "video_analysis": None,
            "multimodal_analysis": None,
            "error": None
        }
        
        try:
            # 1. ë¹„ë””ì˜¤ ì†ŒìŠ¤ ì—´ê¸°
            if not self.video_manager.open():
                result["error"] = "ë¹„ë””ì˜¤ ì†ŒìŠ¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                return result
            
            print("\n" + "=" * 60)
            print("ğŸ¬ ì˜ìƒë§Œ ë¶„ì„ ëª¨ë“œ (í…ŒìŠ¤íŠ¸ìš©)")
            print("=" * 60)
            
            # 2. ì˜ìƒ ìº¡ì²˜
            print("\nğŸ“¹ ì˜ìƒ ìº¡ì²˜ ì¤‘...")
            frames, timestamps = self._capture_and_process_video()
            
            if not frames:
                result["error"] = "í”„ë ˆì„ì„ ìº¡ì²˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                return result
            
            result["video_analysis"] = {
                "frame_count": len(frames),
                "timestamps": timestamps
            }
            
            # 3. ë©€í‹°ëª¨ë‹¬ ë¶„ì„ (ì˜ìƒ + í…ìŠ¤íŠ¸ ì…ë ¥)
            if self.multimodal_analyzer and frames:
                print("\nğŸ” ì˜ìƒ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
                
                # ëŒ€í‘œ í”„ë ˆì„ ì„ íƒ (ì¤‘ê°„ í”„ë ˆì„)
                representative_frame = frames[len(frames) // 2]
                
                # ë¶„ì„í•  í…ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: ì˜ìƒ ë¶„ì„ ìš”ì²­)
                # ê¸°ì¡´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ëŠ” "ìŒì„± ì…ë ¥"ì„ ê¸°ëŒ€í•˜ë¯€ë¡œ, 
                # í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œëŠ” ìƒí™© ì„¤ëª… ìš”ì²­ìœ¼ë¡œ ëŒ€ì²´
                default_text = "í˜„ì¬ ìƒí™©ì„ ë¶„ì„í•´ ì£¼ì„¸ìš”. ìœ„í—˜í•˜ê±°ë‚˜ ê¸´ê¸‰í•œ ìƒí™©ì¸ì§€ íŒë‹¨í•´ ì£¼ì„¸ìš”."
                analysis_text = text_input or default_text
                
                multimodal_result = self.multimodal_analyzer.analyze_with_image(
                    audio_text=analysis_text,
                    image_source=representative_frame,
                    additional_context="[í…ŒìŠ¤íŠ¸ ëª¨ë“œ] ì‹¤ì œ ìŒì„± ì…ë ¥ ì—†ì´ ì˜ìƒë§Œ ë¶„ì„. ì˜ìƒì—ì„œ ë³´ì´ëŠ” ìƒí™©ì„ ê°ê´€ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”."
                )
                
                result["multimodal_analysis"] = multimodal_result
            
            result["success"] = True
            
            # ë¡œê·¸ ì €ì¥
            self._save_result_log(result)
            
            return result
        
        except Exception as e:
            result["error"] = str(e)
            print(f"âŒ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return result
    
    def analyze_testset_all(self, text_input: str = None) -> List[Dict[str, Any]]:
        """
        í…ŒìŠ¤íŠ¸ì…‹ì˜ ëª¨ë“  íŒŒì¼ì„ ìˆœì°¨ì ìœ¼ë¡œ ë¶„ì„
        
        Args:
            text_input: ê° íŒŒì¼ ë¶„ì„ ì‹œ ì‚¬ìš©í•  í…ìŠ¤íŠ¸
        
        Returns:
            ê° íŒŒì¼ì˜ ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        source = self.video_manager.get_source()
        if not isinstance(source, TestsetVideoSource):
            print("âŒ í˜„ì¬ í…ŒìŠ¤íŠ¸ì…‹ ëª¨ë“œê°€ ì•„ë‹™ë‹ˆë‹¤. use_testset()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
            return []
        
        results = []
        files = source.list_files()
        
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ì…‹ ì „ì²´ ë¶„ì„ ì‹œì‘ ({len(files)}ê°œ íŒŒì¼)")
        print("=" * 60)
        
        for i, filename in enumerate(files):
            print(f"\n[{i+1}/{len(files)}] ğŸ“‚ {filename}")
            print("-" * 40)
            
            # íŒŒì¼ ì„ íƒ
            if not source.select_file(i):
                print(f"   âŒ íŒŒì¼ ì—´ê¸° ì‹¤íŒ¨")
                continue
            
            # ë¶„ì„
            result = self.analyze_video_only(text_input)
            result["file_index"] = i
            result["filename"] = filename
            
            results.append(result)
            
            # ê²°ê³¼ ìš”ì•½
            if result.get("success"):
                analysis = result.get("multimodal_analysis", {})
                print(f"   âœ… ì„±ê³µ")
                print(f"      ìƒí™©: {analysis.get('situation_type', 'N/A')}")
                print(f"      ìœ„ê¸‰ë„: {analysis.get('urgency', 'N/A')}")
                print(f"      ìš°ì„ ìˆœìœ„: {analysis.get('priority', 'N/A')}")
            else:
                print(f"   âŒ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
        
        print("\n" + "=" * 60)
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ì…‹ ë¶„ì„ ì™„ë£Œ")
        print(f"   ì´ {len(files)}ê°œ íŒŒì¼, ì„±ê³µ {sum(1 for r in results if r.get('success'))}ê°œ")
        print("=" * 60)
        
        return results
    
    # ==================== ê¸°ì¡´ ë©”ì„œë“œ ====================
    
    def analyze_once(self, phrase_time_limit: float = 30.0) -> Dict[str, Any]:
        """
        ë‹¨ë°œì„± ë¶„ì„ ìˆ˜í–‰
        ìŒì„±ì´ ê°ì§€ë˜ë©´ ìŒì„± + ì˜ìƒì„ í•¨ê»˜ ë¶„ì„
        
        Args:
            phrase_time_limit: ìµœëŒ€ ë°œí™” ì‹œê°„ (ì´ˆ)
        
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        result = {
            "timestamp": datetime.now().isoformat(),
            "success": False,
            "speech_detected": False,
            "transcribed_text": None,
            "voice_characteristics": None,
            "video_analysis": None,
            "multimodal_analysis": None,
            "error": None
        }
        
        try:
            # 1. ì¹´ë©”ë¼ ì—´ê¸° (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëŒ€ê¸°)
            if not self.video_manager.open():
                result["error"] = "ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                return result
            
            # 2. ìŒì„± ê°ì§€ ëŒ€ê¸° (ìŒì„± ê°ì§€ ì „ê¹Œì§€ëŠ” ìŒì„±ë§Œ ëŒ€ê¸°)
            print("\n" + "=" * 60)
            print("ğŸ™ï¸  ìŒì„± ê°ì§€ ëŒ€ê¸° ì¤‘... (ë§ì”€í•´ ì£¼ì„¸ìš”)")
            print("=" * 60)
            
            audio, detected = self.speech_detector.listen_for_speech(
                phrase_time_limit=phrase_time_limit
            )
            
            if not detected or audio is None:
                result["error"] = "ìŒì„±ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
                return result
            
            result["speech_detected"] = True
            
            # 3. ìŒì„± ê°ì§€ë¨! ë™ì‹œì— ì˜ìƒ ìº¡ì²˜ ì‹œì‘
            print("\nğŸš€ ìŒì„± ê°ì§€! ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì‹œì‘...")
            
            # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
            with ThreadPoolExecutor(max_workers=3) as executor:
                futures = {}
                
                # 3-1. ìŒì„± ì¸ì‹ (í…ìŠ¤íŠ¸ ë³€í™˜)
                futures["speech_recognition"] = executor.submit(
                    self.speech_detector.recognize_speech, 
                    audio
                )
                
                # 3-2. ìŒì„± íŠ¹ì„± ë¶„ì„ (ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥ í›„ ë¶„ì„)
                audio_path = self.recordings_dir / f"audio_{datetime.now().strftime('%Y%m%d_%H%M%S')}.wav"
                self.speech_detector.save_audio_to_wav(audio, str(audio_path))
                
                if self.voice_characteristics_analyzer:
                    futures["voice_characteristics"] = executor.submit(
                        self._analyze_voice_characteristics,
                        str(audio_path)
                    )
                
                # 3-3. ì˜ìƒ ìº¡ì²˜ ë° ë‹¤ìš´ìƒ˜í”Œë§
                futures["video_capture"] = executor.submit(
                    self._capture_and_process_video
                )
                
                # ê²°ê³¼ ìˆ˜ì§‘
                transcribed_text = None
                voice_features = None
                video_frames = []
                
                for name, future in futures.items():
                    try:
                        if name == "speech_recognition":
                            transcribed_text = future.result(timeout=30)
                            result["transcribed_text"] = transcribed_text
                        
                        elif name == "voice_characteristics":
                            voice_features = future.result(timeout=30)
                            result["voice_characteristics"] = voice_features
                        
                        elif name == "video_capture":
                            video_frames, timestamps = future.result(timeout=30)
                            result["video_analysis"] = {
                                "frame_count": len(video_frames),
                                "timestamps": timestamps
                            }
                    
                    except Exception as e:
                        print(f"   âš ï¸  {name} ì˜¤ë¥˜: {e}")
            
            # 4. ë©€í‹°ëª¨ë‹¬ ë¶„ì„ (ìŒì„± í…ìŠ¤íŠ¸ + ì˜ìƒ)
            if transcribed_text and video_frames and self.multimodal_analyzer:
                print("\nğŸ” ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
                
                # ëŒ€í‘œ í”„ë ˆì„ ì„ íƒ (ì¤‘ê°„ í”„ë ˆì„)
                representative_frame = video_frames[len(video_frames) // 2] if video_frames else None
                
                if representative_frame is not None:
                    # ìŒì„± íŠ¹ì„± ì •ë³´ë¥¼ ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ë¡œ ì „ë‹¬
                    additional_context = None
                    if voice_features:
                        additional_context = self._format_voice_features_context(voice_features)
                    
                    multimodal_result = self.multimodal_analyzer.analyze_with_image(
                        audio_text=transcribed_text,
                        image_source=representative_frame,
                        additional_context=additional_context,
                        audio_file_path=str(audio_path)
                    )
                    
                    result["multimodal_analysis"] = multimodal_result
            
            # 5. ì„±ê³µ í‘œì‹œ
            result["success"] = True
            
            # 6. ê²°ê³¼ ë¡œê·¸ ì €ì¥
            self._save_result_log(result)
            
            # 7. ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ ì‚­ì œ (ì„ íƒì )
            # os.remove(audio_path)
            
            return result
        
        except Exception as e:
            result["error"] = str(e)
            print(f"âŒ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return result
        
        finally:
            # ì¹´ë©”ë¼ ë‹«ì§€ ì•ŠìŒ (ì—°ì† ëª¨ë‹ˆí„°ë§ì„ ìœ„í•´)
            pass
    
    def _analyze_voice_characteristics(self, audio_path: str) -> Dict[str, Any]:
        """ìŒì„± íŠ¹ì„± ë¶„ì„"""
        print("   ğŸ¤ ìŒì„± íŠ¹ì„± ë¶„ì„ ì¤‘...")
        
        try:
            features = self.voice_characteristics_analyzer.extract_features(audio_path)
            
            # ê¸´ê¸‰ë„ ì ìˆ˜ ê³„ì‚°
            emergency_indicators = self._calculate_voice_emergency_indicators(features)
            
            print("   âœ… ìŒì„± íŠ¹ì„± ë¶„ì„ ì™„ë£Œ")
            
            return {
                "features": features,
                "emergency_indicators": emergency_indicators
            }
        
        except Exception as e:
            print(f"   âš ï¸  ìŒì„± íŠ¹ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None
    
    def _calculate_voice_emergency_indicators(self, features: Dict) -> Dict[str, Any]:
        """ìŒì„± íŠ¹ì„±ì—ì„œ ê¸´ê¸‰ ì§€í‘œ ê³„ì‚°"""
        indicators = {
            "high_pitch": False,
            "high_energy": False,
            "fast_speech": False,
            "voice_trembling": False,
            "overall_score": 0.0
        }
        
        if not features:
            return indicators
        
        score = 0.0
        
        # í”¼ì¹˜ ë¶„ì„ (ë†’ì€ í”¼ì¹˜ = ê¸´ì¥/ê³µí¬)
        pitch = features.get("pitch", {})
        if pitch.get("mean", 0) > 250:  # í‰ê·  í”¼ì¹˜ê°€ ë†’ìœ¼ë©´
            indicators["high_pitch"] = True
            score += 0.25
        if pitch.get("std", 0) > 50:  # í”¼ì¹˜ ë³€ë™ì´ í¬ë©´ (ë–¨ë¦¼)
            indicators["voice_trembling"] = True
            score += 0.25
        
        # ì—ë„ˆì§€ ë¶„ì„ (ë†’ì€ ì—ë„ˆì§€ = ì†Œë¦¬ ì§€ë¦„)
        energy = features.get("energy", {})
        if energy.get("max", 0) > 0.3:  # ìµœëŒ€ ì—ë„ˆì§€ê°€ ë†’ìœ¼ë©´
            indicators["high_energy"] = True
            score += 0.25
        
        # ë§ ì†ë„ ë¶„ì„ (ë¹ ë¥¸ ë§ = ê¸‰ë°•í•¨)
        speech_rate = features.get("speech_rate", {})
        if speech_rate.get("estimated_syllables_per_second", 0) > 5:  # ì´ˆë‹¹ 5ìŒì ˆ ì´ìƒ
            indicators["fast_speech"] = True
            score += 0.25
        
        indicators["overall_score"] = min(score, 1.0)
        
        return indicators
    
    def _capture_and_process_video(self) -> Tuple[List[np.ndarray], List[float]]:
        """ë¹„ë””ì˜¤ ìº¡ì²˜ ë° ë‹¤ìš´ìƒ˜í”Œë§"""
        print("   ğŸ“¹ ë¹„ë””ì˜¤ ìº¡ì²˜ ì¤‘...")
        
        # ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ìº¡ì²˜
        frames, timestamps = self.video_manager.capture_video_segment(
            duration=self.downsampling_config.video_capture_duration,
            target_fps=self.downsampling_config.video_fps
        )
        
        # ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©
        frames, timestamps = self.downsampler.downsample_video_frames(frames, timestamps)
        
        print(f"   âœ… ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ ({len(frames)} í”„ë ˆì„, ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©)")
        
        return frames, timestamps
    
    def _format_voice_features_context(self, voice_features: Dict) -> str:
        """ìŒì„± íŠ¹ì„±ì„ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ í¬ë§·"""
        if not voice_features:
            return ""
        
        indicators = voice_features.get("emergency_indicators", {})
        
        context_parts = ["**ìŒì„± íŠ¹ì„± ë¶„ì„ ê²°ê³¼:**"]
        
        if indicators.get("high_pitch"):
            context_parts.append("- ë†’ì€ í”¼ì¹˜ ê°ì§€ (ê¸´ì¥/ê³µí¬ ê°€ëŠ¥ì„±)")
        
        if indicators.get("high_energy"):
            context_parts.append("- ë†’ì€ ì—ë„ˆì§€ ê°ì§€ (ì†Œë¦¬ ì§€ë¦„/í¥ë¶„)")
        
        if indicators.get("fast_speech"):
            context_parts.append("- ë¹ ë¥¸ ë§ ì†ë„ (ê¸‰ë°•í•¨)")
        
        if indicators.get("voice_trembling"):
            context_parts.append("- ìŒì„± ë–¨ë¦¼ ê°ì§€ (ë¶ˆì•ˆ/ê³µí¬)")
        
        score = indicators.get("overall_score", 0)
        context_parts.append(f"- ìŒì„± ê¸´ê¸‰ë„ ì ìˆ˜: {score:.0%}")
        
        return "\n".join(context_parts)
    
    def _save_result_log(self, result: Dict):
        """ê²°ê³¼ ë¡œê·¸ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = self.log_dir / f"integrated_analysis_{timestamp}.json"
        
        # numpy array ë“± ì§ë ¬í™” ë¶ˆê°€ëŠ¥í•œ ê°ì²´ ì²˜ë¦¬
        serializable_result = self._make_serializable(result)
        
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_result, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ë¡œê·¸ ì €ì¥: {log_file}")
    
    def _make_serializable(self, obj):
        """ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜"""
        if isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, np.ndarray):
            return f"<ndarray shape={obj.shape}>"
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        else:
            return obj
    
    def start_monitoring(
        self, 
        on_result: Callable[[Dict], None] = None,
        max_iterations: int = None
    ):
        """
        ì—°ì† ëª¨ë‹ˆí„°ë§ ì‹œì‘
        
        Args:
            on_result: ê²°ê³¼ ì½œë°± í•¨ìˆ˜
            max_iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (Noneì´ë©´ ë¬´í•œ)
        """
        self.on_result_callback = on_result
        self.is_monitoring = True
        
        # ì¹´ë©”ë¼ ë¯¸ë¦¬ ì—´ê¸°
        self.video_manager.open()
        
        print("\n" + "=" * 60)
        print("ğŸ”„ ì—°ì† ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        print("   - ìŒì„±ì´ ê°ì§€ë˜ë©´ ìë™ìœ¼ë¡œ ì˜ìƒ ë¶„ì„")
        print("   - Ctrl+Cë¡œ ì¢…ë£Œ")
        print("=" * 60)
        
        iteration = 0
        
        try:
            while self.is_monitoring:
                iteration += 1
                
                if max_iterations and iteration > max_iterations:
                    print(f"\nâœ… {max_iterations}íšŒ ì™„ë£Œ!")
                    break
                
                print(f"\n[{iteration}íšŒì°¨] {datetime.now().strftime('%H:%M:%S')}")
                
                # ë¶„ì„ ìˆ˜í–‰
                result = self.analyze_once()
                
                # ì½œë°± í˜¸ì¶œ
                if self.on_result_callback and result.get("success"):
                    self.on_result_callback(result)
                
                # ê²°ê³¼ ì¶œë ¥
                self._print_result_summary(result)
        
        except KeyboardInterrupt:
            print("\n\nâ¹ï¸  ëª¨ë‹ˆí„°ë§ ì¤‘ì§€ë¨ (Ctrl+C)")
        
        finally:
            self.stop_monitoring()
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.is_monitoring = False
        self.video_manager.close()
        print("âœ… ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ")
    
    def _print_result_summary(self, result: Dict):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "-" * 40)
        
        if not result.get("success"):
            print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            return
        
        # ìŒì„± í…ìŠ¤íŠ¸
        text = result.get("transcribed_text", "")
        if text:
            print(f"ğŸ“ ìŒì„±: {text[:80]}{'...' if len(text) > 80 else ''}")
        
        # ìŒì„± íŠ¹ì„±
        voice = result.get("voice_characteristics")
        if voice:
            indicators = voice.get("emergency_indicators", {})
            score = indicators.get("overall_score", 0)
            print(f"ğŸ¤ ìŒì„± ê¸´ê¸‰ë„: {score:.0%}")
        
        # ë©€í‹°ëª¨ë‹¬ ë¶„ì„
        analysis = result.get("multimodal_analysis")
        if analysis:
            print(f"ğŸ” ìƒí™©: {analysis.get('situation_type', 'N/A')}")
            print(f"âš¡ ìœ„ê¸‰ë„: {analysis.get('urgency', 'N/A')}")
            print(f"ğŸš¨ ìš°ì„ ìˆœìœ„: {analysis.get('priority', 'N/A')}")
            
            if analysis.get("is_emergency"):
                print(f"âš ï¸  ê¸´ê¸‰ ìƒí™©: {analysis.get('emergency_reason', '')}")
        
        print("-" * 40)


# í…ŒìŠ¤íŠ¸ ë° ì‹¤í–‰
if __name__ == "__main__":
    print("=" * 70)
    print("ğŸš€ í†µí•© ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    # ë‹¤ìš´ìƒ˜í”Œë§ ì„¤ì • (ì„±ëŠ¥ ìµœì í™”)
    config = DownsamplingConfig(
        max_image_size=640,
        jpeg_quality=75,
        video_fps=2.0,
        max_video_frames=10,
        video_resolution_scale=0.5,
        video_capture_duration=5.0
    )
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    system = IntegratedMultimodalSystem(
        camera_id=0,
        model="gpt-4o-mini",
        downsampling_config=config
    )
    
    print("\ní…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„ íƒ:")
    print("1. ë‹¨ë°œì„± ë¶„ì„ (í•œ ë²ˆë§Œ)")
    print("2. ì—°ì† ëª¨ë‹ˆí„°ë§ (ë¬´í•œ)")
    print("3. ì—°ì† ëª¨ë‹ˆí„°ë§ (3íšŒ)")
    
    choice = input("\nì„ íƒ (1/2/3): ").strip()
    
    if choice == "1":
        print("\në‹¨ë°œì„± ë¶„ì„ ì‹œì‘...")
        result = system.analyze_once()
        print(f"\nìµœì¢… ê²°ê³¼: {json.dumps(system._make_serializable(result), ensure_ascii=False, indent=2)}")
    
    elif choice == "2":
        system.start_monitoring()
    
    elif choice == "3":
        system.start_monitoring(max_iterations=3)
    
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
