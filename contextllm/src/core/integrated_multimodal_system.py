#!/usr/bin/env python3
"""
í†µí•© ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ
ìŒì„± ê°ì§€ ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í•˜ë©°, ìŒì„±ì´ ê°ì§€ë˜ë©´ ë™ì‹œì—:
1. ìŒì„± íŠ¹ì„± ë¶„ì„ (í”¼ì¹˜, ì—ë„ˆì§€, ì†ë„ ë“±)
2. ì˜ìƒ ìº¡ì²˜ ë° ë¶„ì„ (ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©)

ì‚¬ìš©ë²•:
    system = IntegratedMultimodalSystem()
    
    # ìŒì„± ê°ì§€ ì‹œ ìë™ìœ¼ë¡œ ì˜ìƒ ë¶„ì„
    system.start_monitoring()
    
    # ë˜ëŠ” ë‹¨ë°œì„± ë¶„ì„
    result = system.analyze_once()
"""

import os
import sys
import json
import cv2
import numpy as np
import threading
import queue
import time
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple, Callable
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed

# config ë¡œë“œ
try:
    from core.config_manager import get_config
except ImportError:
    try:
        from config_manager import get_config
    except ImportError:
        def get_config(key, default=None):
            return default

try:
    import speech_recognition as sr
    SPEECH_RECOGNITION_AVAILABLE = True
except ImportError:
    sr = None
    SPEECH_RECOGNITION_AVAILABLE = False

try:
    from openai import OpenAI
    from dotenv import load_dotenv
    load_dotenv()
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# ë‚´ë¶€ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from core.voice_characteristics import VoiceCharacteristicsAnalyzer
    VOICE_CHARACTERISTICS_AVAILABLE = True
except ImportError:
    try:
        from voice_characteristics import VoiceCharacteristicsAnalyzer
        VOICE_CHARACTERISTICS_AVAILABLE = True
    except ImportError:
        VOICE_CHARACTERISTICS_AVAILABLE = False

try:
    from core.multimodal_analyzer import MultimodalAnalyzer
    MULTIMODAL_ANALYZER_AVAILABLE = True
except ImportError:
    try:
        from multimodal_analyzer import MultimodalAnalyzer
        MULTIMODAL_ANALYZER_AVAILABLE = True
    except ImportError:
        MULTIMODAL_ANALYZER_AVAILABLE = False


@dataclass
class DownsamplingConfig:
    """ë‹¤ìš´ìƒ˜í”Œë§ ì„¤ì •"""
    # ì´ë¯¸ì§€ ë‹¤ìš´ìƒ˜í”Œë§
    max_image_size: int = 640  # ìµœëŒ€ ì´ë¯¸ì§€ í¬ê¸° (í”½ì…€)
    jpeg_quality: int = 75  # JPEG í’ˆì§ˆ (1-100)
    
    # ë¹„ë””ì˜¤ ë‹¤ìš´ìƒ˜í”Œë§
    video_fps: float = 2.0  # ë¶„ì„ìš© FPS (ì›ë³¸ì—ì„œ ìƒ˜í”Œë§)
    max_video_frames: int = 10  # ìµœëŒ€ ë¶„ì„ í”„ë ˆì„ ìˆ˜ (5ì´ˆ * 2fps = 10)
    video_resolution_scale: float = 0.5  # ë¹„ë””ì˜¤ í•´ìƒë„ ìŠ¤ì¼€ì¼ (0.5 = 50%)
    
    # ë¹„ë””ì˜¤ ìº¡ì²˜ ì‹œê°„
    video_capture_duration: float = 5.0  # ìº¡ì²˜í•  ë¹„ë””ì˜¤ ê¸¸ì´ (ì´ˆ)


class VideoSourceType:
    """ë¹„ë””ì˜¤ ì†ŒìŠ¤ íƒ€ì…"""
    WEBCAM = "webcam"           # ì›¹ìº 
    FILE = "file"               # íŒŒì¼ (ì´ë¯¸ì§€/ë¹„ë””ì˜¤)
    NETWORK = "network"         # ë„¤íŠ¸ì›Œí¬ ì¹´ë©”ë¼ (RTSP/HTTP)
    TESTSET = "testset"         # í…ŒìŠ¤íŠ¸ì…‹ í´ë”


class VideoDownsampler:
    """ë¹„ë””ì˜¤/ì´ë¯¸ì§€ ë‹¤ìš´ìƒ˜í”Œë§ ìœ í‹¸ë¦¬í‹°"""
    
    def __init__(self, config: DownsamplingConfig = None):
        self.config = config or DownsamplingConfig()
    
    def downsample_image(self, image: np.ndarray) -> np.ndarray:
        """
        ì´ë¯¸ì§€ ë‹¤ìš´ìƒ˜í”Œë§
        
        Args:
            image: ì›ë³¸ ì´ë¯¸ì§€ (numpy array, BGR format)
        
        Returns:
            ë‹¤ìš´ìƒ˜í”Œë§ëœ ì´ë¯¸ì§€
        """
        if image is None:
            return None
        
        height, width = image.shape[:2]
        max_size = self.config.max_image_size
        
        # í¬ê¸°ê°€ max_sizeë³´ë‹¤ í¬ë©´ ë¦¬ì‚¬ì´ì§•
        if max(height, width) > max_size:
            scale = max_size / max(height, width)
            new_width = int(width * scale)
            new_height = int(height * scale)
            
            # INTER_AREA: ì¶•ì†Œì— ì í•©í•œ ë³´ê°„ë²•
            image = cv2.resize(image, (new_width, new_height), 
                             interpolation=cv2.INTER_AREA)
            
        return image
    
    def downsample_video_frames(
        self, 
        frames: List[np.ndarray], 
        timestamps: List[float] = None
    ) -> Tuple[List[np.ndarray], List[float]]:
        """
        ë¹„ë””ì˜¤ í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸ ë‹¤ìš´ìƒ˜í”Œë§
        
        Args:
            frames: í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸
            timestamps: íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            (ë‹¤ìš´ìƒ˜í”Œë§ëœ í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸, íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬ìŠ¤íŠ¸)
        """
        if not frames:
            return [], []
        
        # ìµœëŒ€ í”„ë ˆì„ ìˆ˜ë¡œ ì œí•œ
        max_frames = self.config.max_video_frames
        if len(frames) > max_frames:
            # ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§
            indices = np.linspace(0, len(frames) - 1, max_frames, dtype=int)
            frames = [frames[i] for i in indices]
            if timestamps:
                timestamps = [timestamps[i] for i in indices]
        
        # ê° í”„ë ˆì„ ë‹¤ìš´ìƒ˜í”Œë§
        downsampled_frames = []
        scale = self.config.video_resolution_scale
        
        for frame in frames:
            if scale < 1.0:
                new_width = int(frame.shape[1] * scale)
                new_height = int(frame.shape[0] * scale)
                frame = cv2.resize(frame, (new_width, new_height), 
                                 interpolation=cv2.INTER_AREA)
            
            # ì¶”ê°€ë¡œ max_image_size ì ìš©
            frame = self.downsample_image(frame)
            downsampled_frames.append(frame)
        
        return downsampled_frames, timestamps or []
    
    def encode_frame_to_jpeg(self, frame: np.ndarray) -> bytes:
        """í”„ë ˆì„ì„ JPEG ë°”ì´íŠ¸ë¡œ ì¸ì½”ë”©"""
        encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), self.config.jpeg_quality]
        _, buffer = cv2.imencode('.jpg', frame, encode_param)
        return buffer.tobytes()


class BaseVideoSource:
    """ë¹„ë””ì˜¤ ì†ŒìŠ¤ ê¸°ë³¸ í´ë˜ìŠ¤ (ì¶”ìƒ)"""
    
    def __init__(self):
        self.is_opened = False
        self.lock = threading.Lock()
        self.source_type = None
    
    def open(self) -> bool:
        """ì†ŒìŠ¤ ì—´ê¸°"""
        raise NotImplementedError
    
    def close(self):
        """ì†ŒìŠ¤ ë‹«ê¸°"""
        raise NotImplementedError
    
    def capture_frame(self) -> Optional[np.ndarray]:
        """ë‹¨ì¼ í”„ë ˆì„ ìº¡ì²˜"""
        raise NotImplementedError
    
    def capture_video_segment(
        self, 
        duration: float = 5.0, 
        target_fps: float = 2.0
    ) -> Tuple[List[np.ndarray], List[float]]:
        """ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ìº¡ì²˜"""
        raise NotImplementedError
    
    def get_info(self) -> Dict[str, Any]:
        """ì†ŒìŠ¤ ì •ë³´ ë°˜í™˜"""
        return {
            "source_type": self.source_type,
            "is_opened": self.is_opened
        }


class WebcamVideoSource(BaseVideoSource):
    """ì›¹ìº  ë¹„ë””ì˜¤ ì†ŒìŠ¤"""
    
    def __init__(self, camera_id: int = 0):
        super().__init__()
        self.camera_id = camera_id
        self.cap = None
        self.source_type = VideoSourceType.WEBCAM
    
    def open(self) -> bool:
        """ì›¹ìº  ì—´ê¸°"""
        with self.lock:
            if self.is_opened:
                return True
            
            self.cap = cv2.VideoCapture(self.camera_id)
            if self.cap.isOpened():
                self.is_opened = True
                return True
            else:
                return False
    
    def close(self):
        """ì›¹ìº  ë‹«ê¸°"""
        with self.lock:
            if self.cap:
                self.cap.release()
                self.cap = None
                self.is_opened = False
    
    def capture_frame(self) -> Optional[np.ndarray]:
        """ë‹¨ì¼ í”„ë ˆì„ ìº¡ì²˜"""
        with self.lock:
            if not self.is_opened or not self.cap:
                return None
            
            ret, frame = self.cap.read()
            if ret:
                return frame
            return None
    
    def capture_video_segment(
        self, 
        duration: float = 5.0, 
        target_fps: float = 2.0
    ) -> Tuple[List[np.ndarray], List[float]]:
        """ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ìº¡ì²˜"""
        frames = []
        timestamps = []
        
        with self.lock:
            if not self.is_opened or not self.cap:
                return frames, timestamps
            
            original_fps = self.cap.get(cv2.CAP_PROP_FPS) or 30.0
            frame_interval = int(original_fps / target_fps) if target_fps < original_fps else 1
            
            start_time = time.time()
            frame_count = 0
            
            while (time.time() - start_time) < duration:
                ret, frame = self.cap.read()
                if not ret:
                    break
                
                if frame_count % frame_interval == 0:
                    timestamp = time.time() - start_time
                    frames.append(frame.copy())
                    timestamps.append(timestamp)
                
                frame_count += 1
        
        return frames, timestamps
    
    def get_info(self) -> Dict[str, Any]:
        info = super().get_info()
        info["camera_id"] = self.camera_id
        if self.cap and self.is_opened:
            info["fps"] = self.cap.get(cv2.CAP_PROP_FPS)
            info["width"] = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            info["height"] = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        return info


class NetworkVideoSource(BaseVideoSource):
    """ë„¤íŠ¸ì›Œí¬ ì¹´ë©”ë¼ ë¹„ë””ì˜¤ ì†ŒìŠ¤ (RTSP/HTTP)"""
    
    def __init__(self, url: str):
        """
        Args:
            url: ì¹´ë©”ë¼ URL (ì˜ˆ: rtsp://192.168.1.100:554/stream, http://192.168.1.100:8080/video)
        """
        super().__init__()
        # URL ê²€ì¦ (í—ˆìš©ëœ í”„ë¡œí† ì½œë§Œ)
        allowed_protocols = ('rtsp://', 'http://', 'https://')
        if not any(url.lower().startswith(p) for p in allowed_protocols):
            raise ValueError(f"í—ˆìš©ë˜ì§€ ì•Šì€ í”„ë¡œí† ì½œì…ë‹ˆë‹¤. í—ˆìš©: {allowed_protocols}")
        
        # ë¡œì»¬í˜¸ìŠ¤íŠ¸/ë‚´ë¶€ IP ì°¨ë‹¨ (ì„ íƒì  - SSRF ë°©ì§€)
        # from urllib.parse import urlparse
        # parsed = urlparse(url)
        # if parsed.hostname in ('localhost', '127.0.0.1', '0.0.0.0'):
        #     raise ValueError("ë¡œì»¬ ì£¼ì†ŒëŠ” í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        
        self.url = url
        self.cap = None
        self.source_type = VideoSourceType.NETWORK
    
    def open(self) -> bool:
        """ë„¤íŠ¸ì›Œí¬ ì¹´ë©”ë¼ ì—°ê²°"""
        with self.lock:
            if self.is_opened:
                return True
            
            self.cap = cv2.VideoCapture(self.url)
            
            # ë²„í¼ í¬ê¸° ì¤„ì´ê¸° (ì§€ì—° ê°ì†Œ)
            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
            
            if self.cap.isOpened():
                self.is_opened = True
                return True
            else:
                return False
    
    def close(self):
        """ë„¤íŠ¸ì›Œí¬ ì¹´ë©”ë¼ ì—°ê²° ì¢…ë£Œ"""
        with self.lock:
            if self.cap:
                self.cap.release()
                self.cap = None
                self.is_opened = False
    
    def capture_frame(self) -> Optional[np.ndarray]:
        """ë‹¨ì¼ í”„ë ˆì„ ìº¡ì²˜"""
        with self.lock:
            if not self.is_opened or not self.cap:
                return None
            
            # ë²„í¼ ë¹„ìš°ê¸° (ìµœì‹  í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°)
            for _ in range(3):
                self.cap.grab()
            
            ret, frame = self.cap.read()
            if ret:
                return frame
            return None
    
    def capture_video_segment(
        self, 
        duration: float = 5.0, 
        target_fps: float = 2.0
    ) -> Tuple[List[np.ndarray], List[float]]:
        """ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ìº¡ì²˜"""
        frames = []
        timestamps = []
        
        with self.lock:
            if not self.is_opened or not self.cap:
                print("âŒ ë„¤íŠ¸ì›Œí¬ ì¹´ë©”ë¼ê°€ ì—°ê²°ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤")
                return frames, timestamps
            
            frame_interval = 1.0 / target_fps
            start_time = time.time()
            last_capture_time = 0
            
            print(f"ğŸ“¹ ë„¤íŠ¸ì›Œí¬ ì¹´ë©”ë¼ ìº¡ì²˜ ì¤‘ ({duration}ì´ˆ, {target_fps}fps)...")
            
            while (time.time() - start_time) < duration:
                current_time = time.time() - start_time
                
                if current_time - last_capture_time >= frame_interval:
                    ret, frame = self.cap.read()
                    if ret:
                        frames.append(frame.copy())
                        timestamps.append(current_time)
                        last_capture_time = current_time
                
                time.sleep(0.01)  # CPU ì‚¬ìš©ëŸ‰ ì¤„ì´ê¸°
            
            print(f"   âœ… {len(frames)}ê°œ í”„ë ˆì„ ìº¡ì²˜ ì™„ë£Œ")
        
        return frames, timestamps
    
    def get_info(self) -> Dict[str, Any]:
        info = super().get_info()
        info["url"] = self.url
        if self.cap and self.is_opened:
            info["fps"] = self.cap.get(cv2.CAP_PROP_FPS)
            info["width"] = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            info["height"] = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        return info


class FileVideoSource(BaseVideoSource):
    """íŒŒì¼ ê¸°ë°˜ ë¹„ë””ì˜¤ ì†ŒìŠ¤ (ì´ë¯¸ì§€ ë˜ëŠ” ë¹„ë””ì˜¤ íŒŒì¼)"""
    
    def __init__(self, file_path: str):
        """
        Args:
            file_path: ì´ë¯¸ì§€ ë˜ëŠ” ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        """
        super().__init__()
        self.file_path = Path(file_path)
        self.cap = None
        self.is_video = False
        self.is_image = False
        self.image = None
        self.source_type = VideoSourceType.FILE
        
        # íŒŒì¼ íƒ€ì… í™•ì¸
        self._detect_file_type()
    
    def _detect_file_type(self):
        """íŒŒì¼ íƒ€ì… ê°ì§€"""
        suffix = self.file_path.suffix.lower()
        video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.webm', '.flv', '.wmv'}
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp'}
        
        if suffix in video_extensions:
            self.is_video = True
        elif suffix in image_extensions:
            self.is_image = True
    
    def open(self) -> bool:
        """íŒŒì¼ ì—´ê¸°"""
        with self.lock:
            if self.is_opened:
                return True
            
            if not self.file_path.exists():
                return False
            
            if self.is_video:
                self.cap = cv2.VideoCapture(str(self.file_path))
                if self.cap.isOpened():
                    self.is_opened = True
                    return True
                else:
                    return False
            
            elif self.is_image:
                self.image = cv2.imread(str(self.file_path))
                if self.image is not None:
                    self.is_opened = True
                    return True
                else:
                    return False
            
            else:
                return False
    
    def close(self):
        """íŒŒì¼ ë‹«ê¸°"""
        with self.lock:
            if self.cap:
                self.cap.release()
                self.cap = None
            self.image = None
            self.is_opened = False
    
    def capture_frame(self) -> Optional[np.ndarray]:
        """í”„ë ˆì„/ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°"""
        with self.lock:
            if not self.is_opened:
                return None
            
            if self.is_image:
                return self.image.copy() if self.image is not None else None
            
            elif self.is_video and self.cap:
                ret, frame = self.cap.read()
                if ret:
                    return frame
                else:
                    # ë¹„ë””ì˜¤ ëì— ë„ë‹¬í•˜ë©´ ì²˜ìŒìœ¼ë¡œ ë˜ê°ê¸°
                    self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                    ret, frame = self.cap.read()
                    return frame if ret else None
            
            return None
    
    def capture_video_segment(
        self, 
        duration: float = 5.0, 
        target_fps: float = 2.0
    ) -> Tuple[List[np.ndarray], List[float]]:
        """ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ìº¡ì²˜"""
        frames = []
        timestamps = []
        
        with self.lock:
            if not self.is_opened:
                return frames, timestamps
            
            if self.is_image:
                # ì´ë¯¸ì§€ì¸ ê²½ìš° ê°™ì€ ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ ë²ˆ ë°˜í™˜
                num_frames = int(duration * target_fps)
                
                for i in range(num_frames):
                    frames.append(self.image.copy())
                    timestamps.append(i / target_fps)
                
                return frames, timestamps
            
            elif self.is_video and self.cap:
                original_fps = self.cap.get(cv2.CAP_PROP_FPS) or 30.0
                total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
                video_duration = total_frames / original_fps
                
                # ìš”ì²­ëœ ì‹œê°„ì´ ë¹„ë””ì˜¤ ê¸¸ì´ë³´ë‹¤ ê¸¸ë©´ ë¹„ë””ì˜¤ ê¸¸ì´ë¡œ ì œí•œ
                actual_duration = min(duration, video_duration)
                
                frame_interval = int(original_fps / target_fps) if target_fps < original_fps else 1
                frame_count = 0
                
                # ë¹„ë””ì˜¤ ì²˜ìŒìœ¼ë¡œ ë˜ê°ê¸°
                self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                
                while True:
                    ret, frame = self.cap.read()
                    if not ret:
                        break
                    
                    current_time = frame_count / original_fps
                    if current_time > actual_duration:
                        break
                    
                    if frame_count % frame_interval == 0:
                        frames.append(frame.copy())
                        timestamps.append(current_time)
                    
                    frame_count += 1
        
        return frames, timestamps
    
    def get_info(self) -> Dict[str, Any]:
        info = super().get_info()
        info["file_path"] = str(self.file_path)
        info["is_video"] = self.is_video
        info["is_image"] = self.is_image
        
        if self.is_video and self.cap and self.is_opened:
            info["fps"] = self.cap.get(cv2.CAP_PROP_FPS)
            info["width"] = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            info["height"] = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            info["total_frames"] = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
            info["duration"] = info["total_frames"] / info["fps"] if info["fps"] > 0 else 0
        
        elif self.is_image and self.image is not None:
            info["height"], info["width"] = self.image.shape[:2]
        
        return info
    
    def seek(self, position: float):
        """ë¹„ë””ì˜¤ì—ì„œ íŠ¹ì • ìœ„ì¹˜ë¡œ ì´ë™ (ì´ˆ ë‹¨ìœ„)"""
        if self.is_video and self.cap and self.is_opened:
            fps = self.cap.get(cv2.CAP_PROP_FPS) or 30.0
            frame_num = int(position * fps)
            self.cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)


class TestsetVideoSource(BaseVideoSource):
    """í…ŒìŠ¤íŠ¸ì…‹ í´ë” ë¹„ë””ì˜¤ ì†ŒìŠ¤ (í´ë” ë‚´ ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ìˆœì°¨ ì¬ìƒ)"""
    
    def __init__(self, folder_path: str, loop: bool = True):
        """
        Args:
            folder_path: í…ŒìŠ¤íŠ¸ì…‹ í´ë” ê²½ë¡œ
            loop: íŒŒì¼ ëì— ë„ë‹¬í•˜ë©´ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í• ì§€ ì—¬ë¶€
        """
        super().__init__()
        self.folder_path = Path(folder_path)
        self.loop = loop
        self.source_type = VideoSourceType.TESTSET
        
        self.files: List[Path] = []
        self.current_index = 0
        self.current_source: Optional[FileVideoSource] = None
        
        # ì´ˆê¸°í™” ì‹œ íŒŒì¼ ìŠ¤ìº” (open ì „ì—ë„ íŒŒì¼ ëª©ë¡ í™•ì¸ ê°€ëŠ¥)
        self._scan_files()
    
    def _scan_files(self):
        """í´ë” ë‚´ ë¯¸ë””ì–´ íŒŒì¼ ìŠ¤ìº”"""
        if not self.folder_path.exists() or not self.folder_path.is_dir():
            print(f"âš ï¸  í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {self.folder_path}")
            self.files = []
            return
        
        video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.webm', '.flv', '.wmv'}
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp'}
        all_extensions = video_extensions | image_extensions
        
        self.files = sorted([
            f for f in self.folder_path.iterdir()
            if f.is_file() and f.suffix.lower() in all_extensions
        ])
    
    def open(self) -> bool:
        """í…ŒìŠ¤íŠ¸ì…‹ í´ë” ì—´ê¸°"""
        with self.lock:
            if self.is_opened:
                return True
            
            # íŒŒì¼ì´ ì•„ì§ ìŠ¤ìº”ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ë‹¤ì‹œ ìŠ¤ìº”
            if not self.files:
                self._scan_files()
            
            if not self.folder_path.exists():
                return False
            
            if not self.folder_path.is_dir():
                return False
            
            self._scan_files()
            
            if not self.files:
                return False
            
            # ì²« ë²ˆì§¸ íŒŒì¼ ì—´ê¸°
            self.current_index = 0
            self.current_source = FileVideoSource(str(self.files[0]))
            
            if self.current_source.open():
                self.is_opened = True
                return True
            else:
                return False
    
    def close(self):
        """í…ŒìŠ¤íŠ¸ì…‹ ë‹«ê¸°"""
        with self.lock:
            if self.current_source:
                self.current_source.close()
                self.current_source = None
            self.is_opened = False
            self.current_index = 0
    
    def _next_file(self) -> bool:
        """ë‹¤ìŒ íŒŒì¼ë¡œ ì´ë™"""
        if self.current_source:
            self.current_source.close()
        
        self.current_index += 1
        
        if self.current_index >= len(self.files):
            if self.loop:
                self.current_index = 0
            else:
                return False
        
        self.current_source = FileVideoSource(str(self.files[self.current_index]))
        return self.current_source.open()
    
    def select_file(self, index: int) -> bool:
        """íŠ¹ì • ì¸ë±ìŠ¤ì˜ íŒŒì¼ ì„ íƒ"""
        with self.lock:
            if index < 0 or index >= len(self.files):
                return False
            
            if self.current_source:
                self.current_source.close()
            
            self.current_index = index
            self.current_source = FileVideoSource(str(self.files[index]))
            success = self.current_source.open()
            
            return success
    
    def select_file_by_name(self, filename: str) -> bool:
        """íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ì„ íƒ"""
        for i, f in enumerate(self.files):
            if f.name == filename or f.stem == filename:
                return self.select_file(i)
        
        return False
    
    def capture_frame(self) -> Optional[np.ndarray]:
        """í˜„ì¬ íŒŒì¼ì—ì„œ í”„ë ˆì„ ìº¡ì²˜"""
        with self.lock:
            if not self.is_opened or not self.current_source:
                return None
            
            return self.current_source.capture_frame()
    
    def capture_video_segment(
        self, 
        duration: float = 5.0, 
        target_fps: float = 2.0
    ) -> Tuple[List[np.ndarray], List[float]]:
        """í˜„ì¬ íŒŒì¼ì—ì„œ ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ìº¡ì²˜"""
        with self.lock:
            if not self.is_opened or not self.current_source:
                return [], []
            
            return self.current_source.capture_video_segment(duration, target_fps)
    
    def get_info(self) -> Dict[str, Any]:
        info = super().get_info()
        info["folder_path"] = str(self.folder_path)
        info["total_files"] = len(self.files)
        info["current_index"] = self.current_index
        info["current_file"] = self.files[self.current_index].name if self.files else None
        info["loop"] = self.loop
        
        if self.current_source:
            info["current_source_info"] = self.current_source.get_info()
        
        return info
    
    def list_files(self) -> List[str]:
        """íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        return [f.name for f in self.files]


def create_video_source(
    source_type: str,
    **kwargs
) -> BaseVideoSource:
    """
    ë¹„ë””ì˜¤ ì†ŒìŠ¤ íŒ©í† ë¦¬ í•¨ìˆ˜
    
    Args:
        source_type: ì†ŒìŠ¤ íƒ€ì… (webcam, file, network, testset)
        **kwargs: ì†ŒìŠ¤ë³„ ì¶”ê°€ ì¸ì
            - webcam: camera_id (int, ê¸°ë³¸ê°’ 0)
            - file: file_path (str)
            - network: url (str)
            - testset: folder_path (str), loop (bool, ê¸°ë³¸ê°’ True)
    
    Returns:
        BaseVideoSource ì¸ìŠ¤í„´ìŠ¤
    
    Examples:
        # ì›¹ìº 
        source = create_video_source("webcam", camera_id=0)
        
        # íŒŒì¼
        source = create_video_source("file", file_path="testsets/violence.mp4")
        
        # ë„¤íŠ¸ì›Œí¬ ì¹´ë©”ë¼
        source = create_video_source("network", url="rtsp://192.168.1.100:554/stream")
        
        # í…ŒìŠ¤íŠ¸ì…‹ í´ë”
        source = create_video_source("testset", folder_path="testsets/")
    """
    if source_type == VideoSourceType.WEBCAM or source_type == "webcam":
        camera_id = kwargs.get("camera_id", 0)
        return WebcamVideoSource(camera_id)
    
    elif source_type == VideoSourceType.FILE or source_type == "file":
        file_path = kwargs.get("file_path")
        if not file_path:
            raise ValueError("file_pathê°€ í•„ìš”í•©ë‹ˆë‹¤")
        return FileVideoSource(file_path)
    
    elif source_type == VideoSourceType.NETWORK or source_type == "network":
        url = kwargs.get("url")
        if not url:
            raise ValueError("urlì´ í•„ìš”í•©ë‹ˆë‹¤")
        return NetworkVideoSource(url)
    
    elif source_type == VideoSourceType.TESTSET or source_type == "testset":
        folder_path = kwargs.get("folder_path")
        if not folder_path:
            raise ValueError("folder_pathê°€ í•„ìš”í•©ë‹ˆë‹¤")
        loop = kwargs.get("loop", True)
        return TestsetVideoSource(folder_path, loop)
    
    else:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ì†ŒìŠ¤ íƒ€ì…: {source_type}")


class VideoCaptureManager:
    """ë¹„ë””ì˜¤ ìº¡ì²˜ ê´€ë¦¬ì (ë ˆê±°ì‹œ í˜¸í™˜ + ë©€í‹° ì†ŒìŠ¤ ì§€ì›)"""
    
    def __init__(self, camera_id: int = 0):
        self.camera_id = camera_id
        self.cap = None
        self.is_opened = False
        self.lock = threading.Lock()
        
        # ìƒˆë¡œìš´ ë¹„ë””ì˜¤ ì†ŒìŠ¤ ì‹œìŠ¤í…œ
        self._video_source: Optional[BaseVideoSource] = None
    
    def set_source(self, source: BaseVideoSource):
        """ë¹„ë””ì˜¤ ì†ŒìŠ¤ ì„¤ì •"""
        if self._video_source and self._video_source.is_opened:
            self._video_source.close()
        self._video_source = source
    
    def get_source(self) -> Optional[BaseVideoSource]:
        """í˜„ì¬ ë¹„ë””ì˜¤ ì†ŒìŠ¤ ë°˜í™˜"""
        return self._video_source
    
    def open(self) -> bool:
        """ì¹´ë©”ë¼/ì†ŒìŠ¤ ì—´ê¸°"""
        # ìƒˆë¡œìš´ ì†ŒìŠ¤ê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
        if self._video_source:
            result = self._video_source.open()
            self.is_opened = result
            return result
        
        # ë ˆê±°ì‹œ ëª¨ë“œ: ì›¹ìº 
        with self.lock:
            if self.is_opened:
                return True
            
            self.cap = cv2.VideoCapture(self.camera_id)
            if self.cap.isOpened():
                self.is_opened = True
                return True
            else:
                return False
    
    def close(self):
        """ì¹´ë©”ë¼/ì†ŒìŠ¤ ë‹«ê¸°"""
        # ìƒˆë¡œìš´ ì†ŒìŠ¤ ì‹œìŠ¤í…œ
        if self._video_source:
            self._video_source.close()
            self.is_opened = False
            return
        
        # ë ˆê±°ì‹œ ëª¨ë“œ
        with self.lock:
            if self.cap:
                self.cap.release()
                self.cap = None
                self.is_opened = False
    
    def capture_frame(self) -> Optional[np.ndarray]:
        """ë‹¨ì¼ í”„ë ˆì„ ìº¡ì²˜"""
        # ìƒˆë¡œìš´ ì†ŒìŠ¤ ì‹œìŠ¤í…œ
        if self._video_source:
            return self._video_source.capture_frame()
        
        # ë ˆê±°ì‹œ ëª¨ë“œ
        with self.lock:
            if not self.is_opened or not self.cap:
                return None
            
            ret, frame = self.cap.read()
            if ret:
                return frame
            return None
    
    def capture_video_segment(
        self, 
        duration: float = 5.0, 
        target_fps: float = 2.0
    ) -> Tuple[List[np.ndarray], List[float]]:
        """
        ì§€ì •ëœ ì‹œê°„ ë™ì•ˆ ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ìº¡ì²˜
        
        Args:
            duration: ìº¡ì²˜ ì‹œê°„ (ì´ˆ)
            target_fps: íƒ€ê²Ÿ FPS (ë‹¤ìš´ìƒ˜í”Œë§ìš©)
        
        Returns:
            (í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸, íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬ìŠ¤íŠ¸)
        """
        # ìƒˆë¡œìš´ ì†ŒìŠ¤ ì‹œìŠ¤í…œ
        if self._video_source:
            return self._video_source.capture_video_segment(duration, target_fps)
        
        # ë ˆê±°ì‹œ ëª¨ë“œ
        frames = []
        timestamps = []
        
        with self.lock:
            if not self.is_opened or not self.cap:
                return frames, timestamps
            
            # ì›ë³¸ FPS ê°€ì ¸ì˜¤ê¸°
            original_fps = self.cap.get(cv2.CAP_PROP_FPS) or 30.0
            frame_interval = int(original_fps / target_fps) if target_fps < original_fps else 1
            
            start_time = time.time()
            frame_count = 0
            
            while (time.time() - start_time) < duration:
                ret, frame = self.cap.read()
                if not ret:
                    break
                
                # í”„ë ˆì„ ê°„ê²©ì— ë”°ë¼ ìƒ˜í”Œë§
                if frame_count % frame_interval == 0:
                    timestamp = time.time() - start_time
                    frames.append(frame.copy())
                    timestamps.append(timestamp)
                
                frame_count += 1
        
        return frames, timestamps


class SpeechDetector:
    """ìŒì„± ê°ì§€ ë° ì¸ì‹"""
    
    def __init__(self, energy_threshold: int = 400, pause_threshold: float = 3.0, dynamic_threshold: bool = False):
        """
        Args:
            energy_threshold: ìŒì„± ê°ì§€ ì—ë„ˆì§€ ì„ê³„ê°’ (ë‚®ì„ìˆ˜ë¡ ë¯¼ê°í•¨) - ê¸°ë³¸ê°’ 400
            pause_threshold: ë¬¸ì¥ ë íŒë‹¨ ëŒ€ê¸° ì‹œê°„ (ì´ˆ) - ê¸°ë³¸ê°’ 3.0 (ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” íë¦„)
                           3ì´ˆ ì¹¨ë¬µ í›„ ë¬¸ì¥ ëìœ¼ë¡œ íŒë‹¨ â†’ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” í¬í•¨
            dynamic_threshold: ë™ì  ì—ë„ˆì§€ ì„ê³„ê°’ ì¡°ì • ì—¬ë¶€ - False=ê³ ì •(ìŠ¤í”¼ì»¤ ì†Œë¦¬ìš©), True=ìë™(ì‹¤ì‹œê°„ ì¡°ì •)
        """
        if not SPEECH_RECOGNITION_AVAILABLE:
            raise ImportError("speech_recognitionì´ í•„ìš”í•©ë‹ˆë‹¤: pip install SpeechRecognition")
        
        self.recognizer = sr.Recognizer()
        self.recognizer.pause_threshold = pause_threshold
        # dynamic_energy_threshold ì„¤ì •
        # False: ê³ ì • ì„ê³„ê°’ (ìŠ¤í”¼ì»¤/ìœ íŠœë¸Œ ì†Œë¦¬ ì¸ì‹ì— ë” ì¢‹ìŒ)
        # True: ë™ì  ì¡°ì • (ì‹¤ì‹œê°„ ë§ˆì´í¬ ì…ë ¥ì— ì¢‹ìŒ)
        self.recognizer.dynamic_energy_threshold = dynamic_threshold
        
        # ë§ˆì´í¬ ì´ˆê¸°í™”
        self.microphone = sr.Microphone()
        
        # ì£¼ë³€ ì†ŒìŒ ì¡°ì • í›„ ì—ë„ˆì§€ ì„ê³„ê°’ ì„¤ì •
        with self.microphone as source:
            self.recognizer.adjust_for_ambient_noise(source, duration=1)
            # ì¡°ì •ëœ ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì¶”ê°€ ë°°ìˆ˜ ì—†ìŒ)
            self.recognizer.energy_threshold = max(energy_threshold, self.recognizer.energy_threshold)
        
        # ë°±ê·¸ë¼ìš´ë“œ ìŒì„± ì¸ì‹ìš© ì €ì¥ì†Œ
        self._bg_audio_queue = None
        self._is_listening = False
    
    def listen_and_recognize(self, timeout: float = None, phrase_time_limit: float = None, language: str = "ko-KR") -> Tuple[Optional[str], Optional[Any]]:
        """
        ìŒì„±ì„ ë“£ê³  ë°”ë¡œ ì¸ì‹ (ê°ì§€ + ì¸ì‹ í†µí•©)
        pause_threshold ë‚´ì—ì„œ ìˆ˜ì§‘í•œ ëª¨ë“  ìŒì„±ì„ ì¸ì‹
        
        Args:
            timeout: ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ) - Noneì´ë©´ ë¬´í•œ ëŒ€ê¸°
            phrase_time_limit: ìµœëŒ€ ë°œí™” ì‹œê°„ (ì´ˆ) - Noneì´ë©´ pause_threshold ì‚¬ìš©
            language: ì¸ì‹ ì–¸ì–´
        
        Returns:
            (ì¸ì‹ëœ í…ìŠ¤íŠ¸ ë˜ëŠ” None, AudioData ë˜ëŠ” None)
        """
        try:
            with self.microphone as source:
                # pause_threshold ì‹œê°„ ë™ì•ˆ ê³„ì† ìˆ˜ì§‘í•˜ë„ë¡ ì„¤ì •
                # phrase_time_limit=Noneì´ë©´, pause_threshold ì‹œê°„ë§Œí¼ ëŒ€ê¸° í›„ ì¸ì‹
                # ì˜ˆ: pause_threshold=10ì´ˆ â†’ 10ì´ˆ ë™ì•ˆ ê³„ì† ìŒì„± ìˆ˜ì§‘ í›„ ì¸ì‹
                audio = self.recognizer.listen(
                    source, 
                    timeout=timeout, 
                    phrase_time_limit=phrase_time_limit
                    # phrase_time_limit=Noneì´ í•µì‹¬: pause_thresholdê¹Œì§€ ê¸°ë‹¤ë¦° í›„ ì¸ì‹
                )
                
                # ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸ (ë„ˆë¬´ ì§§ìœ¼ë©´ ë…¸ì´ì¦ˆ)
                audio_data = audio.get_raw_data()
                duration = len(audio_data) / (audio.sample_rate * audio.sample_width)
                
                if duration < 0.3:  # 0.5ì´ˆ â†’ 0.3ì´ˆë¡œ ì™„í™”
                    return None, None
                
                # ë°”ë¡œ í…ìŠ¤íŠ¸ ì¸ì‹
                try:
                    text = self.recognizer.recognize_google(audio, language=language)
                    return text, audio
                except sr.UnknownValueError:
                    # ìŒì„±ì€ ê°ì§€ëì§€ë§Œ ì¸ì‹ ë¶ˆê°€
                    return None, None
                except sr.RequestError as e:
                    print(f"âŒ ìŒì„± ì¸ì‹ API ì˜¤ë¥˜: {e}")
                    return None, None
        
        except sr.WaitTimeoutError:
            return None, None
        except Exception as e:
            return None, None
    
    def start_background_listening(self, language: str = "ko-KR"):
        """
        ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê³„ì† ìŒì„±ì„ ê°ì§€í•˜ê³  ì¸ì‹
        ë£¨í”„ê°€ ë©ˆì¶”ì§€ ì•Šê³  ìŒì„±ì´ ê°ì§€ë˜ë©´ íì— ì¶”ê°€
        
        Args:
            language: ì¸ì‹ ì–¸ì–´
        """
        import queue
        import threading
        
        if self._is_listening:
            return  # ì´ë¯¸ ì‹¤í–‰ ì¤‘
        
        self._is_listening = True
        self._bg_audio_queue = queue.Queue()
        
        def background_worker():
            """ë°±ê·¸ë¼ìš´ë“œ ìŒì„± ì¸ì‹ ì›Œì»¤"""
            
            while self._is_listening:
                try:
                    with self.microphone as source:
                        # ìŒì„± ê°ì§€
                        audio = self.recognizer.listen(source, timeout=None)
                        
                        # ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸
                        audio_data = audio.get_raw_data()
                        duration = len(audio_data) / (audio.sample_rate * audio.sample_width)
                        
                        if duration < 0.5:  # 0.5ì´ˆ ì´ìƒë§Œ ì¸ì‹ ì‹œë„
                            continue
                        
                        # í…ìŠ¤íŠ¸ ì¸ì‹
                        try:
                            text = self.recognizer.recognize_google(audio, language=language)
                            print(f"\nì¸ì‹ë¨: {text}")
                            # íì— ì¶”ê°€ (ë©”ì¸ ë£¨í”„ì—ì„œ êº¼ë‚¼ ìˆ˜ ìˆìŒ)
                            self._bg_audio_queue.put((text, audio))
                        except sr.UnknownValueError:
                            pass
                        except sr.RequestError:
                            pass
                
                except Exception:
                    pass
        
        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì‹œì‘
        bg_thread = threading.Thread(target=background_worker, daemon=True)
        bg_thread.start()
    
    def get_recognized_speech(self):
        """
        ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì¸ì‹ëœ ìŒì„± ê°€ì ¸ì˜¤ê¸° (ë…¼ë¸”ë¡œí‚¹)
        
        Returns:
            (í…ìŠ¤íŠ¸, ì˜¤ë””ì˜¤) ë˜ëŠ” (None, None) - íê°€ ë¹„ì–´ìˆìœ¼ë©´ None ë°˜í™˜
        """
        if not self._bg_audio_queue:
            return None, None
        
        try:
            return self._bg_audio_queue.get_nowait()
        except:
            return None, None
    
    def stop_background_listening(self):
        """ë°±ê·¸ë¼ìš´ë“œ ë¦¬ìŠ¤ë‹ ì¤‘ì§€"""
        self._is_listening = False
    
    def listen_continuous(self, duration: float = 5.0, language: str = "ko-KR") -> Tuple[Optional[str], Optional[Any]]:
        """
        ì§€ì •ëœ ì‹œê°„(ì´ˆ) ë™ì•ˆ ì—°ì†ìœ¼ë¡œ ìŒì„±ì„ ìˆ˜ì§‘í•˜ê³  ì¸ì‹
        ì—¬ëŸ¬ ë¬¸ì¥ì„ í•œë²ˆì— ëª¨ì•„ì„œ ëŒ€í™” ë§¥ë½ì„ íŒŒì•…í•  ìˆ˜ ìˆìŒ
        
        Args:
            duration: ìŒì„± ìˆ˜ì§‘ ì‹œê°„ (ì´ˆ) - ê¸°ë³¸ê°’ 5ì´ˆ
            language: ì¸ì‹ ì–¸ì–´
        
        Returns:
            (ì¸ì‹ëœ ì „ì²´ í…ìŠ¤íŠ¸ ë˜ëŠ” None, AudioData ë˜ëŠ” None)
        """
        import time
        try:
            with self.microphone as source:
                # ìŒì„± ê°ì§€ ë° ìˆ˜ì§‘ (ìµœëŒ€ duration ì´ˆ)
                audio = self.recognizer.listen(
                    source,
                    timeout=None,  # ìŒì„±ì´ ë“¤ì–´ì˜¬ ë•Œê¹Œì§€ ë¬´í•œ ëŒ€ê¸°
                    phrase_time_limit=duration  # ìµœëŒ€ durationì´ˆê¹Œì§€ ìˆ˜ì§‘
                )
                
                # ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸
                audio_data = audio.get_raw_data()
                duration_actual = len(audio_data) / (audio.sample_rate * audio.sample_width)
                
                if duration_actual < 0.5:  # 0.5ì´ˆ ë¯¸ë§Œì´ë©´ ë¬´ì‹œ
                    return None, None
                
                # í…ìŠ¤íŠ¸ ì¸ì‹
                try:
                    text = self.recognizer.recognize_google(audio, language=language)
                    return text, audio
                except sr.UnknownValueError:
                    return None, None
                except sr.RequestError as e:
                    print(f"âŒ ìŒì„± ì¸ì‹ API ì˜¤ë¥˜: {e}")
                    return None, None
        
        except sr.WaitTimeoutError:
            return None, None
        except Exception as e:
            return None, None
        """
        ìŒì„±ì´ ê°ì§€ë  ë•Œê¹Œì§€ ëŒ€ê¸°
        
        Args:
            timeout: ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            phrase_time_limit: ìµœëŒ€ ë°œí™” ì‹œê°„ (ì´ˆ)
        
        Returns:
            (AudioData ë˜ëŠ” None, ìŒì„± ê°ì§€ ì—¬ë¶€)
        """
        try:
            with self.microphone as source:
                audio = self.recognizer.listen(
                    source, 
                    timeout=timeout, 
                    phrase_time_limit=phrase_time_limit
                )
                
                # ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸ (ë„ˆë¬´ ì§§ìœ¼ë©´ ë…¸ì´ì¦ˆë¡œ íŒë‹¨)
                audio_data = audio.get_raw_data()
                duration = len(audio_data) / (audio.sample_rate * audio.sample_width)
                
                if duration < 0.5:  # 0.5ì´ˆ ë¯¸ë§Œì´ë©´ ë…¸ì´ì¦ˆë¡œ íŒë‹¨
                    return None, False
                
                return audio, True
        
        except sr.WaitTimeoutError:
            return None, False
        except Exception as e:
            return None, False
    
    def recognize_speech(self, audio: Any, language: str = "ko-KR") -> Optional[str]:
        """
        ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (Google Speech Recognition)
        
        Args:
            audio: ìŒì„± ë°ì´í„°
            language: ì¸ì‹ ì–¸ì–´
        
        Returns:
            ì¸ì‹ëœ í…ìŠ¤íŠ¸ ë˜ëŠ” None
        """
        try:
            text = self.recognizer.recognize_google(audio, language=language)
            return text
        except sr.UnknownValueError:
            return None
        except sr.RequestError as e:
            return None
    
    def save_audio_to_wav(self, audio: Any, output_path: str) -> str:
        """
        AudioDataë¥¼ WAV íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            audio: ìŒì„± ë°ì´í„°
            output_path: ì €ì¥ ê²½ë¡œ
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, "wb") as f:
            f.write(audio.get_wav_data())
        
        return output_path


class IntegratedMultimodalSystem:
    """í†µí•© ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ"""
    
    def __init__(
        self, 
        camera_id: int = 0,
        model: str = "gpt-4o-mini",
        downsampling_config: DownsamplingConfig = None,
        log_dir: str = None,
        energy_threshold: int = 400,
        dynamic_threshold: bool = False
    ):
        """
        í†µí•© ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            camera_id: ì›¹ìº  ì¹´ë©”ë¼ ID
            model: OpenAI ëª¨ë¸ëª…
            downsampling_config: ë‹¤ìš´ìƒ˜í”Œë§ ì„¤ì •
            log_dir: ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬
            energy_threshold: ìŒì„± ê°ì§€ ì—ë„ˆì§€ ì„ê³„ê°’ (ë‚®ì„ìˆ˜ë¡ ë¯¼ê°í•¨)
            dynamic_threshold: ë™ì  ì—ë„ˆì§€ ì„ê³„ê°’ ì—¬ë¶€ (False=ê³ ì •/ìŠ¤í”¼ì»¤ì†Œë¦¬ìš©, True=ìë™/ë§ˆì´í¬ìš©)
        """
        self.camera_id = camera_id
        self.model = model
        self.downsampling_config = downsampling_config or DownsamplingConfig()
        
        # ë¶„ì„ ì„¤ì • ë¡œë“œ
        self.analysis_config = get_config('analysis', default={}) or {}
        self.use_voice_characteristics = self.analysis_config.get('voice_characteristics', True)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.video_manager = VideoCaptureManager(camera_id)
        self.downsampler = VideoDownsampler(self.downsampling_config)
        self.speech_detector = SpeechDetector(energy_threshold=energy_threshold, dynamic_threshold=dynamic_threshold)
        
        # ë©€í‹°ëª¨ë‹¬ ë¶„ì„ê¸°
        if MULTIMODAL_ANALYZER_AVAILABLE:
            self.multimodal_analyzer = MultimodalAnalyzer(model=model)
        else:
            self.multimodal_analyzer = None
        
        # ìŒì„± íŠ¹ì„± ë¶„ì„ê¸° (configì—ì„œ ë¹„í™œì„±í™” ê°€ëŠ¥)
        if VOICE_CHARACTERISTICS_AVAILABLE and self.use_voice_characteristics:
            self.voice_characteristics_analyzer = VoiceCharacteristicsAnalyzer()
        else:
            self.voice_characteristics_analyzer = None
        
        # ëª¨ë‹ˆí„°ë§ ìƒíƒœ
        self.is_monitoring = False
        self.monitoring_thread = None
        
        # ë¡œê·¸ ì„¤ì •
        self.log_dir = Path(log_dir) if log_dir else Path("data/logs")
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # ë…¹ìŒ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
        self.recordings_dir = Path("recordings")
        self.recordings_dir.mkdir(parents=True, exist_ok=True)
        
        # ì½œë°± í•¨ìˆ˜
        self.on_result_callback: Optional[Callable[[Dict], None]] = None
        
        # ë””ìŠ¤í”Œë ˆì´ ì„¤ì •
        self.use_opencv_display = False
        self.opencv_display = None
        self.use_web_dashboard = False
    
    # ==================== ë””ìŠ¤í”Œë ˆì´ ì„¤ì • ë©”ì„œë“œ ====================
    
    def enable_opencv_display(self, enable: bool = True):
        """OpenCV ì‹¤ì‹œê°„ ë””ìŠ¤í”Œë ˆì´ í™œì„±í™”/ë¹„í™œì„±í™”"""
        self.use_opencv_display = enable
        if enable:
            try:
                from core.display_manager import OpenCVDisplay
                self.opencv_display = OpenCVDisplay()
            except ImportError:
                print("âš ï¸ OpenCV ë””ìŠ¤í”Œë ˆì´ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                self.use_opencv_display = False
    
    def enable_web_dashboard(self, enable: bool = True):
        """ì›¹ ëŒ€ì‹œë³´ë“œ ì—°ë™ í™œì„±í™”/ë¹„í™œì„±í™”"""
        self.use_web_dashboard = enable
    
    # ==================== ë¹„ë””ì˜¤ ì†ŒìŠ¤ ì„¤ì • ë©”ì„œë“œ ====================
    
    def set_video_source(self, source: BaseVideoSource):
        """
        ë¹„ë””ì˜¤ ì†ŒìŠ¤ ì§ì ‘ ì„¤ì •
        
        Args:
            source: BaseVideoSource ì¸ìŠ¤í„´ìŠ¤
        """
        self.video_manager.set_source(source)
    
    def use_webcam(self, camera_id: int = 0):
        """
        ì›¹ìº ì„ ë¹„ë””ì˜¤ ì†ŒìŠ¤ë¡œ ì‚¬ìš©
        
        Args:
            camera_id: ì¹´ë©”ë¼ ID (ê¸°ë³¸ê°’: 0)
        """
        source = WebcamVideoSource(camera_id)
        self.video_manager.set_source(source)
    
    def use_file(self, file_path: str):
        """
        íŒŒì¼(ì´ë¯¸ì§€/ë¹„ë””ì˜¤)ì„ ë¹„ë””ì˜¤ ì†ŒìŠ¤ë¡œ ì‚¬ìš©
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
        """
        source = FileVideoSource(file_path)
        self.video_manager.set_source(source)
    
    def use_network_camera(self, url: str):
        """
        ë„¤íŠ¸ì›Œí¬ ì¹´ë©”ë¼(RTSP/HTTP)ë¥¼ ë¹„ë””ì˜¤ ì†ŒìŠ¤ë¡œ ì‚¬ìš©
        
        Args:
            url: ì¹´ë©”ë¼ URL
                - RTSP: rtsp://username:password@192.168.1.100:554/stream
                - HTTP: http://192.168.1.100:8080/video
        """
        source = NetworkVideoSource(url)
        self.video_manager.set_source(source)
    
    def use_testset(self, folder_path: str, loop: bool = True):
        """
        í…ŒìŠ¤íŠ¸ì…‹ í´ë”ë¥¼ ë¹„ë””ì˜¤ ì†ŒìŠ¤ë¡œ ì‚¬ìš©
        
        Args:
            folder_path: í…ŒìŠ¤íŠ¸ì…‹ í´ë” ê²½ë¡œ
            loop: íŒŒì¼ ëì— ë„ë‹¬í•˜ë©´ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í• ì§€ ì—¬ë¶€
        """
        source = TestsetVideoSource(folder_path, loop)
        self.video_manager.set_source(source)
    
    def get_testset_files(self) -> List[str]:
        """í…ŒìŠ¤íŠ¸ì…‹ì˜ íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        source = self.video_manager.get_source()
        if isinstance(source, TestsetVideoSource):
            return source.list_files()
        return []
    
    def select_testset_file(self, index_or_name) -> bool:
        """
        í…ŒìŠ¤íŠ¸ì…‹ì—ì„œ íŠ¹ì • íŒŒì¼ ì„ íƒ
        
        Args:
            index_or_name: íŒŒì¼ ì¸ë±ìŠ¤(int) ë˜ëŠ” íŒŒì¼ëª…(str)
        """
        source = self.video_manager.get_source()
        if not isinstance(source, TestsetVideoSource):
            return False
        
        if isinstance(index_or_name, int):
            return source.select_file(index_or_name)
        else:
            return source.select_file_by_name(index_or_name)
    
    def get_video_source_info(self) -> Dict[str, Any]:
        """í˜„ì¬ ë¹„ë””ì˜¤ ì†ŒìŠ¤ ì •ë³´ ë°˜í™˜"""
        source = self.video_manager.get_source()
        if source:
            return source.get_info()
        return {"source_type": "legacy_webcam", "camera_id": self.camera_id}
    
    # ==================== í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ìŒì„± ì…ë ¥ ì—†ì´ ì˜ìƒë§Œ ë¶„ì„) ====================
    
    def analyze_video_only(self, text_input: str = None) -> Dict[str, Any]:
        """
        ìŒì„± ì…ë ¥ ì—†ì´ ì˜ìƒë§Œ ë¶„ì„ (í…ŒìŠ¤íŠ¸ìš©)
        
        Args:
            text_input: ìŒì„± ëŒ€ì‹  ì‚¬ìš©í•  í…ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©)
        
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        result = {
            "timestamp": datetime.now().isoformat(),
            "success": False,
            "mode": "video_only",
            "text_input": text_input or "(í…ŒìŠ¤íŠ¸ ëª¨ë“œ - ìŒì„± ì…ë ¥ ì—†ìŒ)",
            "video_analysis": None,
            "multimodal_analysis": None,
            "error": None
        }
        
        try:
            # 1. ë¹„ë””ì˜¤ ì†ŒìŠ¤ ì—´ê¸°
            if not self.video_manager.open():
                result["error"] = "ë¹„ë””ì˜¤ ì†ŒìŠ¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                return result
            
            # 2. ì†ŒìŠ¤ íƒ€ì…ì— ë”°ë¼ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
            source = self.video_manager.get_source()
            
            # ì´ë¯¸ì§€ íŒŒì¼ì¸ ê²½ìš° ë‹¨ì¼ í”„ë ˆì„ë§Œ ê°€ì ¸ì˜´
            if isinstance(source, FileVideoSource) and source.is_image:
                frame = source.capture_frame()
                if frame is not None:
                    frames = [frame]
                    timestamps = [0.0]
                else:
                    result["error"] = "ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                    return result
            # í…ŒìŠ¤íŠ¸ì…‹ì˜ í˜„ì¬ íŒŒì¼ì´ ì´ë¯¸ì§€ì¸ ê²½ìš°
            elif isinstance(source, TestsetVideoSource):
                current_src = source.current_source
                if current_src and current_src.is_image:
                    frame = current_src.capture_frame()
                    if frame is not None:
                        frames = [frame]
                        timestamps = [0.0]
                    else:
                        result["error"] = "ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                        return result
                else:
                    # ë¹„ë””ì˜¤ì¸ ê²½ìš° ìº¡ì²˜
                    frames, timestamps = self._capture_and_process_video()
            else:
                # ì›¹ìº /ë„¤íŠ¸ì›Œí¬ ë“± ë¹„ë””ì˜¤ ì†ŒìŠ¤
                frames, timestamps = self._capture_and_process_video()
            
            if not frames:
                result["error"] = "í”„ë ˆì„ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                return result
            
            # ì´ë¯¸ì§€ ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©
            frames = [self.downsampler.downsample_image(f) for f in frames]
            
            result["video_analysis"] = {
                "frame_count": len(frames),
                "timestamps": timestamps
            }
            
            # 3. ë©€í‹°ëª¨ë‹¬ ë¶„ì„ (ì˜ìƒ + í…ìŠ¤íŠ¸ ì…ë ¥)
            if self.multimodal_analyzer and frames:
                # ëŒ€í‘œ í”„ë ˆì„ ì„ íƒ (ì¤‘ê°„ í”„ë ˆì„)
                representative_frame = frames[len(frames) // 2]
                
                # ë¶„ì„í•  í…ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: ì˜ìƒ ë¶„ì„ ìš”ì²­)
                default_text = "í˜„ì¬ ìƒí™©ì„ ë¶„ì„í•´ ì£¼ì„¸ìš”. ìœ„í—˜í•˜ê±°ë‚˜ ê¸´ê¸‰í•œ ìƒí™©ì¸ì§€ íŒë‹¨í•´ ì£¼ì„¸ìš”."
                analysis_text = text_input or default_text
                
                multimodal_result = self.multimodal_analyzer.analyze_with_image(
                    audio_text=analysis_text,
                    image_source=representative_frame,
                    additional_context="[í…ŒìŠ¤íŠ¸ ëª¨ë“œ] ì‹¤ì œ ìŒì„± ì…ë ¥ ì—†ì´ ì˜ìƒë§Œ ë¶„ì„. ì˜ìƒì—ì„œ ë³´ì´ëŠ” ìƒí™©ì„ ê°ê´€ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”."
                )
                
                result["multimodal_analysis"] = multimodal_result
            
            result["success"] = True
            
            # ë¡œê·¸ ì €ì¥
            self._save_result_log(result)
            
            return result
        
        except Exception as e:
            result["error"] = str(e)
            return result
    
    def analyze_testset_all(self, text_input: str = None) -> List[Dict[str, Any]]:
        """
        í…ŒìŠ¤íŠ¸ì…‹ì˜ ëª¨ë“  íŒŒì¼ì„ ìˆœì°¨ì ìœ¼ë¡œ ë¶„ì„
        
        Args:
            text_input: ê° íŒŒì¼ ë¶„ì„ ì‹œ ì‚¬ìš©í•  í…ìŠ¤íŠ¸
        
        Returns:
            ê° íŒŒì¼ì˜ ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        source = self.video_manager.get_source()
        if not isinstance(source, TestsetVideoSource):
            return []
        
        results = []
        files = source.list_files()
        
        for i, filename in enumerate(files):
            # íŒŒì¼ ì„ íƒ
            if not source.select_file(i):
                continue
            
            # ë¶„ì„
            result = self.analyze_video_only(text_input)
            result["file_index"] = i
            result["filename"] = filename
            
            results.append(result)
        
        return results
    
    # ==================== ê¸°ì¡´ ë©”ì„œë“œ ====================
    
    def analyze_once(self, phrase_time_limit: float = 30.0) -> Dict[str, Any]:
        """
        ë‹¨ë°œì„± ë¶„ì„ ìˆ˜í–‰
        ìŒì„±ì´ ê°ì§€ë˜ë©´ ìŒì„± + ì˜ìƒì„ í•¨ê»˜ ë¶„ì„
        
        Args:
            phrase_time_limit: ìµœëŒ€ ë°œí™” ì‹œê°„ (ì´ˆ)
        
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        result = {
            "timestamp": datetime.now().isoformat(),
            "success": False,
            "speech_detected": False,
            "transcribed_text": None,
            "voice_characteristics": None,
            "video_analysis": None,
            "multimodal_analysis": None,
            "error": None
        }
        
        try:
            # 1. ì¹´ë©”ë¼ ë¯¸ë¦¬ ì—´ì–´ë‘ê¸°
            if not self.video_manager.open():
                result["error"] = "ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                return result
            
            # 2. ìŒì„± ë“£ê³  ì¸ì‹ (ë¬¸ì¥ì´ ì™„ì„±ë  ë•Œê¹Œì§€ ëŒ€ê¸°)
            #    í…ìŠ¤íŠ¸ê°€ ì¸ì‹ë˜ë©´ ê·¸ ìˆœê°„ ë°˜í™˜
            transcribed_text, audio = self.speech_detector.listen_and_recognize(
                phrase_time_limit=phrase_time_limit
            )
            
            # í…ìŠ¤íŠ¸ê°€ ì¸ì‹ë˜ì§€ ì•Šìœ¼ë©´ ë‹¤ìŒ ë£¨í”„ë¡œ
            if not transcribed_text:
                return result
            
            result["speech_detected"] = True
            result["transcribed_text"] = transcribed_text
            
            # 3. ë¬¸ì¥ì´ ì¸ì‹ë¨! ì´ ìˆœê°„ ì˜ìƒ ìº¡ì²˜
            print(f"ğŸ¤ \"{transcribed_text}\"")
            print("ğŸ“¸ ì˜ìƒ ìº¡ì²˜ ì¤‘...")
            
            # í˜„ì¬ í”„ë ˆì„ ìº¡ì²˜ (ì´ë¯¸ì§€ 1ì¥)
            frame = self.video_manager.capture_frame()
            if frame is not None:
                frame = self.downsampler.downsample_image(frame)
                video_frames = [frame]
                result["video_analysis"] = {"frame_count": 1}
            else:
                video_frames = []
            
            # 4. ìŒì„± íŠ¹ì„± ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥)
            audio_path = None
            voice_features = None
            
            if audio and self.voice_characteristics_analyzer:
                import tempfile
                temp_audio_file = tempfile.NamedTemporaryFile(
                    suffix='.wav', 
                    dir=self.recordings_dir, 
                    delete=False,
                    prefix='temp_audio_'
                )
                audio_path = Path(temp_audio_file.name)
                temp_audio_file.close()
                self.speech_detector.save_audio_to_wav(audio, str(audio_path))
                
                voice_features = self._analyze_voice_characteristics(str(audio_path))
                result["voice_characteristics"] = voice_features
            
            # 5. ë©€í‹°ëª¨ë‹¬ ë¶„ì„ (ìŒì„± í…ìŠ¤íŠ¸ + ì˜ìƒ)
            if transcribed_text and video_frames and self.multimodal_analyzer:
                print("ğŸ” ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì¤‘...")
                
                representative_frame = video_frames[0]
                
                # ìŒì„± íŠ¹ì„± ì •ë³´ë¥¼ ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ë¡œ ì „ë‹¬
                additional_context = None
                if voice_features:
                    additional_context = self._format_voice_features_context(voice_features)
                
                multimodal_result = self.multimodal_analyzer.analyze_with_image(
                    audio_text=transcribed_text,
                    image_source=representative_frame,
                    additional_context=additional_context,
                    audio_file_path=str(audio_path) if audio_path else None
                )
                
                result["multimodal_analysis"] = multimodal_result
            
            # 6. ì„±ê³µ í‘œì‹œ
            result["success"] = True
            
            # 7. ê²°ê³¼ ë¡œê·¸ ì €ì¥
            self._save_result_log(result)
            
            # 8. ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ ì‚­ì œ
            if audio_path and audio_path.exists():
                try:
                    audio_path.unlink()
                except:
                    pass
            
            return result
        
        except Exception as e:
            result["error"] = str(e)
            return result
    
    def _analyze_voice_characteristics(self, audio_path: str) -> Dict[str, Any]:
        """ìŒì„± íŠ¹ì„± ë¶„ì„"""
        try:
            features = self.voice_characteristics_analyzer.extract_features(audio_path)
            
            # ê¸´ê¸‰ë„ ì ìˆ˜ ê³„ì‚°
            emergency_indicators = self._calculate_voice_emergency_indicators(features)
            
            return {
                "features": features,
                "emergency_indicators": emergency_indicators
            }
        except Exception as e:
            print(f"âŒ ìŒì„± íŠ¹ì„± ë¶„ì„ ì˜¤ë¥˜: {e}")
            return None
        except Exception as e:
            return None
    
    def _calculate_voice_emergency_indicators(self, features: Dict) -> Dict[str, Any]:
        """ìŒì„± íŠ¹ì„±ì—ì„œ ê¸´ê¸‰ ì§€í‘œ ê³„ì‚°"""
        indicators = {
            "high_pitch": False,
            "high_energy": False,
            "fast_speech": False,
            "voice_trembling": False,
            "overall_score": 0.0
        }
        
        if not features:
            return indicators
        
        # voice_analysis ì„¤ì •ì—ì„œ ì„ê³„ê°’ ì½ê¸°
        # self.configëŠ” ì´ˆê¸°í™” ì‹œ get_configë¡œ ë°›ì€ ê°’
        analysis_cfg = self.analysis_config  # ì´ë¯¸ ì €ì¥ëœ config
        voice_cfg = analysis_cfg.get('voice_analysis', {}) or {}
        
        pitch_cfg = voice_cfg.get('pitch', {})
        energy_cfg = voice_cfg.get('energy', {})
        speech_rate_cfg = voice_cfg.get('speech_rate', {})
        
        # configì—ì„œ ì„ê³„ê°’ ë¡œë“œ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        high_pitch_threshold = pitch_cfg.get('high_threshold', 180)
        pitch_variability_threshold = pitch_cfg.get('variability_threshold', 40)
        high_energy_threshold = energy_cfg.get('high_threshold', 0.05)
        fast_speech_threshold = speech_rate_cfg.get('fast_threshold', 7)
        
        score = 0.0
        
        # ë””ë²„ê·¸: ë¶„ì„ëœ íŠ¹ì„± ì¶œë ¥
        print("\nğŸ” ìŒì„± íŠ¹ì„± ë¶„ì„ ìƒì„¸:")
        print(f"  - Pitch: {features.get('pitch', {})}")
        print(f"  - Energy: {features.get('energy', {})}")
        print(f"  - Speech Rate: {features.get('speech_rate', {})}")
        
        # í”¼ì¹˜ ë¶„ì„ (ë†’ì€ í”¼ì¹˜ = ê¸´ì¥/ê³µí¬)
        pitch = features.get("pitch", {})
        if isinstance(pitch, dict):
            pitch_mean = pitch.get("mean", 0)
            pitch_std = pitch.get("std", 0)
            print(f"  â†’ Pitch Mean: {pitch_mean:.1f} (threshold: {high_pitch_threshold})")
            print(f"  â†’ Pitch Std: {pitch_std:.1f} (threshold: {pitch_variability_threshold})")
            
            if pitch_mean > high_pitch_threshold:
                indicators["high_pitch"] = True
                score += 0.25
            if pitch_std > pitch_variability_threshold:  # í”¼ì¹˜ ë³€ë™ì´ í¬ë©´ (ë–¨ë¦¼)
                indicators["voice_trembling"] = True
                score += 0.25
        
        # ì—ë„ˆì§€ ë¶„ì„ (ë†’ì€ ì—ë„ˆì§€ = ì†Œë¦¬ ì§€ë¦„)
        energy = features.get("energy", {})
        if isinstance(energy, dict):
            energy_max = energy.get("max", 0)
            print(f"  â†’ Energy Max: {energy_max:.3f} (threshold: {high_energy_threshold})")
            if energy_max > high_energy_threshold:
                indicators["high_energy"] = True
                score += 0.25
        
        # ë§ ì†ë„ ë¶„ì„ (ë¹ ë¥¸ ë§ = ê¸‰ë°•í•¨)
        speech_rate = features.get("speech_rate", {})
        if isinstance(speech_rate, dict):
            syllables_per_sec = speech_rate.get("estimated_syllables_per_second", 0)
            print(f"  â†’ Speech Rate: {syllables_per_sec:.2f} syllables/sec (threshold: {fast_speech_threshold})")
            if syllables_per_sec > fast_speech_threshold:
                indicators["fast_speech"] = True
                score += 0.25
        
        print(f"  â†’ ê²°ê³¼: {indicators}")
        
        indicators["overall_score"] = min(score, 1.0)
        
        return indicators
    
    def _capture_and_process_video(self) -> Tuple[List[np.ndarray], List[float]]:
        """ë¹„ë””ì˜¤ ìº¡ì²˜ ë° ë‹¤ìš´ìƒ˜í”Œë§"""
        # ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ìº¡ì²˜
        frames, timestamps = self.video_manager.capture_video_segment(
            duration=self.downsampling_config.video_capture_duration,
            target_fps=self.downsampling_config.video_fps
        )
        
        # ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©
        frames, timestamps = self.downsampler.downsample_video_frames(frames, timestamps)
        
        return frames, timestamps
    
    def _format_voice_features_context(self, voice_features: Dict) -> str:
        """ìŒì„± íŠ¹ì„±ì„ LLM ë¶„ì„ìš© ì»¨í…ìŠ¤íŠ¸ë¡œ í¬ë§·"""
        if not voice_features:
            return ""
        
        indicators = voice_features.get("emergency_indicators", {})
        features = voice_features.get("features", {})
        
        context_parts = ["**ìŒì„± íŠ¹ì„± ë¶„ì„ ê²°ê³¼:**"]
        
        # Raw íŠ¹ì„±ê°’ ì¶œë ¥ (LLMì´ ë” ì •í™•í•˜ê²Œ íŒë‹¨í•˜ë„ë¡)
        pitch = features.get("pitch", {})
        if isinstance(pitch, dict):
            pitch_mean = pitch.get("mean", 0)
            pitch_std = pitch.get("std", 0)
            context_parts.append(f"**í”¼ì¹˜(ìŒë†’ì´):** í‰ê·  {pitch_mean:.1f}Hz, ë³€ë™ {pitch_std:.1f}Hz")
        
        energy = features.get("energy", {})
        if isinstance(energy, dict):
            energy_max = energy.get("max", 0)
            energy_mean = energy.get("mean", 0)
            context_parts.append(f"**ì—ë„ˆì§€(ìŒëŸ‰):** ìµœëŒ€ {energy_max:.3f}, í‰ê·  {energy_mean:.3f}")
        
        speech_rate = features.get("speech_rate", {})
        if isinstance(speech_rate, dict):
            syllables_per_sec = speech_rate.get("estimated_syllables_per_second", 0)
            context_parts.append(f"**ë§ ì†ë„:** {syllables_per_sec:.2f} ìŒì ˆ/ì´ˆ")
        
        # êµ¬ì²´ì ì¸ íŠ¹ì„± ì„¤ëª…
        context_parts.append("\n**íŠ¹ì„± ë¶„ì„:**")
        
        if indicators.get("high_pitch"):
            context_parts.append("- ë†’ì€ í”¼ì¹˜ ê°ì§€ â†’ ê¸´ì¥/ê³µí¬ ê°€ëŠ¥ì„±")
        
        if indicators.get("high_energy"):
            context_parts.append("- ë†’ì€ ìŒì„± ì—ë„ˆì§€ â†’ ì†Œë¦¬ ì§€ë¦„/ê°•í•œ ê°ì • í‘œì¶œ")
        
        if indicators.get("fast_speech"):
            context_parts.append("- ë¹ ë¥¸ ë§ ì†ë„ â†’ ê¸‰ë°•í•œ ìƒí™©/ë¶ˆì•ˆì • ì‹¬ë¦¬")
        
        if indicators.get("voice_trembling"):
            context_parts.append("- ìŒì„± ë–¨ë¦¼ ê°ì§€ â†’ ë‘ë ¤ì›€/ê·¹ì‹¬í•œ ìŠ¤íŠ¸ë ˆìŠ¤")
        
        # íŠ¹ì„±ì´ ì—†ìœ¼ë©´ ì•ˆì •ì  ìƒíƒœë¡œ ê¸°ìˆ 
        if not any([indicators.get("high_pitch"), indicators.get("high_energy"), 
                    indicators.get("fast_speech"), indicators.get("voice_trembling")]):
            context_parts.append("- ìŒì„±ì´ ì•ˆì •ì ì´ê³  ì§„ì •ëœ ìƒíƒœ")
        
        # ì „ë°˜ì  í‰ê°€ (ì ìˆ˜ ëŒ€ì‹  ì„¤ëª…)
        score = indicators.get("overall_score", 0)
        if score > 0.7:
            context_parts.append("\nâ†’ ì¢…í•© í‰ê°€: ë§¤ìš° ì ˆë°•í•˜ê³  ê¸´ì¥ëœ ìƒíƒœ")
        elif score > 0.4:
            context_parts.append("\nâ†’ ì¢…í•© í‰ê°€: ë¶€ë¶„ì ì¸ ê¸´ì¥ ë˜ëŠ” ìŠ¤íŠ¸ë ˆìŠ¤ ì‹ í˜¸")
        else:
            context_parts.append("\nâ†’ ì¢…í•© í‰ê°€: ìŒì„± íŠ¹ì„±ìƒ íŠ¹ë³„í•œ ê¸´ê¸‰ ì‹ í˜¸ ì—†ìŒ")
        
        return "\n".join(context_parts)
    
    def _save_result_log(self, result: Dict):
        """ê²°ê³¼ ë¡œê·¸ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = self.log_dir / f"integrated_analysis_{timestamp}.json"
        
        # numpy array ë“± ì§ë ¬í™” ë¶ˆê°€ëŠ¥í•œ ê°ì²´ ì²˜ë¦¬
        serializable_result = self._make_serializable(result)
        
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_result, f, ensure_ascii=False, indent=2)
    
    def _make_serializable(self, obj):
        """ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜"""
        if isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, np.ndarray):
            return f"<ndarray shape={obj.shape}>"
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        else:
            return obj
    
    def start_monitoring(
        self, 
        on_result: Callable[[Dict], None] = None,
        max_iterations: int = None,
        verbose: bool = False
    ):
        """
        ì—°ì† ëª¨ë‹ˆí„°ë§ ì‹œì‘
        
        Args:
            on_result: ê²°ê³¼ ì½œë°± í•¨ìˆ˜
            max_iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (Noneì´ë©´ ë¬´í•œ)
            verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        """
        self.on_result_callback = on_result
        self.is_monitoring = True
        self.verbose = verbose
        
        # ì¹´ë©”ë¼ ë¯¸ë¦¬ ì—´ê¸°
        self.video_manager.open()
        
        # OpenCV ë””ìŠ¤í”Œë ˆì´ ì‹œì‘ (ì„¤ì •ëœ ê²½ìš°)
        if self.use_opencv_display and self.opencv_display:
            self.opencv_display.start()
        
        # ì›¹ ëŒ€ì‹œë³´ë“œ ì—°ë™ í™•ì¸
        try:
            from web.app import dashboard, enable_video_stream
            if dashboard.running:
                self.use_web_dashboard = True
                print("   ğŸ“¡ ì›¹ ëŒ€ì‹œë³´ë“œ ì—°ë™ í™œì„±í™”")
                
                # ì›¹ ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™” (localhost ì ‘ê·¼ ê±°ë¶€ ì´ìŠˆ)
                enable_video_stream(False)
                self.web_video_streaming = False
        except:
            self.web_video_streaming = False
        
        self._start_monitoring_sequential(max_iterations)
    
    
    def _start_monitoring_sequential(self, max_iterations: int = None):
        """ìˆœì°¨ ëª¨ë‹ˆí„°ë§: ë°±ê·¸ë¼ìš´ë“œ ìŒì„± ê°ì§€ ë°©ì‹"""
        print("\nğŸ”„ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (Ctrl+Cë¡œ ì¢…ë£Œ)")
        print("   ğŸ’¡ ë°±ê·¸ë¼ìš´ë“œ ìŒì„± ê°ì§€ ì¤‘... ì•„ë¬´ê±°ë‚˜ ë§ì”€í•˜ì„¸ìš”!")
        
        iteration = 0
        
        # ë°±ê·¸ë¼ìš´ë“œ ìŒì„± ê°ì§€ ì‹œì‘
        self.speech_detector.start_background_listening()
        
        try:
            while self.is_monitoring:
                if max_iterations and iteration >= max_iterations:
                    print(f"\nâœ… {max_iterations}íšŒ ë¶„ì„ ì™„ë£Œ!")
                    break
                
                # ë¹„ë¸”ë¡œí‚¹ - ê°ì§€ëœ ìŒì„±ì´ ìˆëŠ”ì§€ í™•ì¸
                transcribed_text, audio = self.speech_detector.get_recognized_speech()
                
                if transcribed_text:
                    print("ğŸ“¸ ì˜ìƒ ìº¡ì²˜ ì¤‘...")
                    
                    # í˜„ì¬ í”„ë ˆì„ ìº¡ì²˜
                    frame = self.video_manager.capture_frame()
                    if frame is not None:
                        frame = self.downsampler.downsample_image(frame)
                    
                    # ë¶„ì„ ìˆ˜í–‰
                    result = self._analyze_with_data(transcribed_text, audio, frame)
                    
                    # ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬
                    if result.get("success"):
                        iteration += 1
                        
                        # ì½œë°± í˜¸ì¶œ
                        if self.on_result_callback:
                            self.on_result_callback(result)
                        
                        # ê²°ê³¼ ì¶œë ¥
                        self._print_result_summary(result, verbose=self.verbose)
                
                # ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ OpenCV ë Œë”ë§ ì²˜ë¦¬ (í•„ìˆ˜: ë©”ì¸ ìŠ¤ë ˆë“œë§Œ ê°€ëŠ¥)
                if self.opencv_display and self.opencv_display.is_running():
                    if not self.opencv_display.render():
                        self.is_monitoring = False
                        break
                
                time.sleep(0.01)  # ì´ë²¤íŠ¸ ë£¨í”„ ì†ë„ ì œì–´
        
        except KeyboardInterrupt:
            print("\nâ¹ï¸  ëª¨ë‹ˆí„°ë§ ì¤‘ì§€ë¨")
        
        finally:
            # ë°±ê·¸ë¼ìš´ë“œ ë¦¬ìŠ¤ë‹ ì¤‘ì§€
            self.speech_detector.stop_background_listening()
            self.stop_monitoring()
    
    def _analyze_with_data(self, transcribed_text: str, audio: Any, frame: np.ndarray) -> Dict[str, Any]:
        """ì´ë¯¸ ìº¡ì²˜ëœ ë°ì´í„°ë¡œ ë¶„ì„ ìˆ˜í–‰"""
        result = {
            "timestamp": datetime.now().isoformat(),
            "success": False,
            "speech_detected": True,
            "transcribed_text": transcribed_text,
            "voice_characteristics": None,
            "video_analysis": None,
            "multimodal_analysis": None,
            "error": None
        }
        
        try:
            video_frames = [frame] if frame is not None else []
            result["video_analysis"] = {"frame_count": len(video_frames)}
            
            # ìŒì„± íŠ¹ì„± ë¶„ì„
            audio_path = None
            voice_features = None
            
            if audio:
                if self.voice_characteristics_analyzer:
                    import tempfile
                    temp_audio_file = tempfile.NamedTemporaryFile(
                        suffix='.wav', 
                        dir=self.recordings_dir, 
                        delete=False,
                        prefix='temp_audio_'
                    )
                    audio_path = Path(temp_audio_file.name)
                    temp_audio_file.close()
                    self.speech_detector.save_audio_to_wav(audio, str(audio_path))
                    
                    voice_features = self._analyze_voice_characteristics(str(audio_path))
                    result["voice_characteristics"] = voice_features
                    if voice_features:
                        print("âœ… ìŒì„± íŠ¹ì„± ë¶„ì„ ì™„ë£Œ")
                else:
                    print("âš ï¸  ìŒì„± íŠ¹ì„± ë¶„ì„ê¸° ë¹„í™œì„±í™”")
            else:
                print("âš ï¸  ì˜¤ë””ì˜¤ ë°ì´í„° ì—†ìŒ")
            
            # ë©€í‹°ëª¨ë‹¬ ë¶„ì„
            if transcribed_text and video_frames and self.multimodal_analyzer:
                print("ğŸ” ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì¤‘...")
                
                representative_frame = video_frames[0]
                
                additional_context = None
                if voice_features:
                    additional_context = self._format_voice_features_context(voice_features)
                
                multimodal_result = self.multimodal_analyzer.analyze_with_image(
                    audio_text=transcribed_text,
                    image_source=representative_frame,
                    additional_context=additional_context,
                    audio_file_path=str(audio_path) if audio_path else None
                )
                
                result["multimodal_analysis"] = multimodal_result
            
            result["success"] = True
            
            # ë¡œê·¸ ì €ì¥
            self._save_result_log(result)
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if audio_path and audio_path.exists():
                try:
                    audio_path.unlink()
                except:
                    pass
            
            return result
        
        except Exception as e:
            result["error"] = str(e)
            return result
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.is_monitoring = False
        self.video_manager.close()
        
        # OpenCV ë””ìŠ¤í”Œë ˆì´ ì •ë¦¬
        if self.opencv_display:
            self.opencv_display.stop()
        
        # ì›¹ ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° ì •ë¦¬
        if getattr(self, 'web_video_streaming', False):
            try:
                from web.app import enable_video_stream
                enable_video_stream(False)
            except:
                pass
    
    def _push_to_displays(self, result: Dict, frame=None):
        """ê²°ê³¼ë¥¼ ë””ìŠ¤í”Œë ˆì´ë“¤(ì›¹, OpenCV)ì— ì „ì†¡"""
        # ì›¹ ëŒ€ì‹œë³´ë“œë¡œ ì „ì†¡
        if self.use_web_dashboard:
            try:
                from web.app import push_result
                push_result(result)
            except Exception as e:
                pass  # ì›¹ ëŒ€ì‹œë³´ë“œ ì˜¤ë¥˜ëŠ” ë¬´ì‹œ
        
        # OpenCV ë””ìŠ¤í”Œë ˆì´ ì—…ë°ì´íŠ¸
        if self.opencv_display and self.opencv_display.is_running():
            self.opencv_display.update_result(result)
            if frame is not None:
                self.opencv_display.update_frame(frame)
    
    def _print_result_summary(self, result: Dict, verbose: bool = False):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        
        if not result.get("success"):
            return
        
        analysis = result.get("multimodal_analysis")
        if not analysis:
            return
        
        # ë””ìŠ¤í”Œë ˆì´ë¡œ ê²°ê³¼ ì „ì†¡
        self._push_to_displays(result, result.get("_frame"))
        
        # ê¸´ê¸‰ ì‹ í˜¸ ì—¬ë¶€
        is_emergency = analysis.get('is_emergency', False)
        
        # í—¤ë” ìƒ‰ìƒ êµ¬ë¶„
        if is_emergency:
            print("\n" + "ğŸš¨" * 50)
            print("ğŸš¨ âš ï¸  âš ï¸  âš ï¸  âš ï¸  âš ï¸  âš ï¸  âš ï¸  âš ï¸  âš ï¸  âš ï¸  âš ï¸  âš ï¸  âš ï¸  âš ï¸  âš ï¸ ê¸´ê¸‰ ìƒí™© ê°ì§€! ğŸš¨")
            print("ğŸš¨" * 50)
        else:
            print("\n" + "=" * 50)
            print("ğŸ“Š ë¶„ì„ ê²°ê³¼")
            print("=" * 50)
        
        # ìŒì„± ì…ë ¥
        text = result.get("transcribed_text", "")
        if text:
            print(f"ğŸ“ ìŒì„± ì…ë ¥: \"{text}\"")
        
        # ìŒì„± íŠ¹ì„± ë¶„ì„
        voice = result.get("voice_characteristics")
        if voice:
            print("\nğŸ¤ ìŒì„± íŠ¹ì„± ë¶„ì„:")
            indicators = voice.get("emergency_indicators", {})
            if indicators.get("high_pitch"):
                print("   - ë†’ì€ í”¼ì¹˜ ê°ì§€ (ê¸´ì¥/ê³µí¬ ê°€ëŠ¥ì„±)")
            if indicators.get("high_energy"):
                print("   - ë†’ì€ ì—ë„ˆì§€ ê°ì§€ (ì†Œë¦¬ ì§€ë¦„/í¥ë¶„)")
            if indicators.get("fast_speech"):
                print("   - ë¹ ë¥¸ ë§ ì†ë„ (ê¸‰ë°•í•¨)")
            if indicators.get("voice_trembling"):
                print("   - ìŒì„± ë–¨ë¦¼ ê°ì§€ (ë¶ˆì•ˆ/ê³µí¬)")
        
        # ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ê²°ê³¼
        print("\nğŸ” ìƒí™© ë¶„ì„:")
        print(f"   - ìƒí™© ìœ í˜•: {analysis.get('situation_type', 'N/A')}")
        print(f"   - ìƒí™© ì„¤ëª…: {analysis.get('situation', 'N/A')}")
        print(f"   - ê°ì • ìƒíƒœ: {analysis.get('emotional_state', 'N/A')}")
        print(f"   - ì˜ìƒ ë‚´ìš©: {analysis.get('visual_content', 'N/A')}")
        
        print("\nâš ï¸  ê¸´ê¸‰ë„ íŒë‹¨:")
        if is_emergency:
            print(f"   - ê¸´ê¸‰ ì—¬ë¶€: ğŸš¨ YES - ì¦‰ì‹œ ëŒ€ì‘ í•„ìš”!")
        else:
            print(f"   - ê¸´ê¸‰ ì—¬ë¶€: âœ… ì•„ë‹ˆì˜¤")
        print(f"   - ìš°ì„ ìˆœìœ„: {analysis.get('priority', 'N/A')}")
        print(f"   - ê¸´ê¸‰ íŒë‹¨ ê·¼ê±°: {analysis.get('emergency_reason', 'N/A')}")
        
        print("\nğŸ¯ ìŒì„±-ì˜ìƒ ì¼ì¹˜ë„:")
        print(f"   - ì¼ì¹˜ ì—¬ë¶€: {analysis.get('audio_visual_consistency', 'N/A')}")
        
        print("\nğŸ’¡ ê¶Œì¥ ì¡°ì¹˜:")
        if is_emergency:
            print(f"   - ğŸš¨ ê¸´ê¸‰: {analysis.get('action', 'N/A')}")
        else:
            print(f"   - {analysis.get('action', 'N/A')}")
        
        if is_emergency:
            print("ğŸš¨" * 50 + "\n")
        else:
            print("=" * 50 + "\n")


# í…ŒìŠ¤íŠ¸ ë° ì‹¤í–‰
if __name__ == "__main__":
    print("=" * 70)
    print("ğŸš€ í†µí•© ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    # ë‹¤ìš´ìƒ˜í”Œë§ ì„¤ì • (ì„±ëŠ¥ ìµœì í™”)
    config = DownsamplingConfig(
        max_image_size=640,
        jpeg_quality=75,
        video_fps=2.0,
        max_video_frames=10,
        video_resolution_scale=0.5,
        video_capture_duration=5.0
    )
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    system = IntegratedMultimodalSystem(
        camera_id=0,
        model="gpt-4o-mini",
        downsampling_config=config
    )
    
    print("\ní…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„ íƒ:")
    print("1. ë‹¨ë°œì„± ë¶„ì„ (í•œ ë²ˆë§Œ)")
    print("2. ì—°ì† ëª¨ë‹ˆí„°ë§ (ë¬´í•œ)")
    print("3. ì—°ì† ëª¨ë‹ˆí„°ë§ (3íšŒ)")
    
    choice = input("\nì„ íƒ (1/2/3): ").strip()
    
    if choice == "1":
        print("\në‹¨ë°œì„± ë¶„ì„ ì‹œì‘...")
        result = system.analyze_once()
        print(f"\nìµœì¢… ê²°ê³¼: {json.dumps(system._make_serializable(result), ensure_ascii=False, indent=2)}")
    
    elif choice == "2":
        system.start_monitoring()
    
    elif choice == "3":
        system.start_monitoring(max_iterations=3)
    
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
