#!/usr/bin/env python3
"""
ë¹„ë””ì˜¤ ìº¡ì²˜ ë° í”„ë ˆì„ ì¶”ì¶œ ëª¨ë“ˆ
ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì—ì„œ í”„ë ˆì„ì„ ì¶”ì¶œí•˜ì—¬ ë©€í‹°ëª¨ë‹¬ ë¶„ì„ì— í™œìš©

ì‚¬ìš©ë²•:
    # ì›¹ìº ì—ì„œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
    monitor = VideoMonitor()
    monitor.start_monitoring(on_frame_callback=your_callback)
    
    # íŠ¹ì • ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ ì¶”ì¶œ
    extractor = VideoFrameExtractor()
    frames = extractor.extract_frames("video.mp4", interval=2.0)
"""

import cv2
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Optional, Callable, List, Tuple
import threading
import time


class VideoFrameExtractor:
    """ë¹„ë””ì˜¤ íŒŒì¼ì—ì„œ í”„ë ˆì„ ì¶”ì¶œ"""
    
    def __init__(self):
        """í”„ë ˆì„ ì¶”ì¶œê¸° ì´ˆê¸°í™”"""
        pass
    
    def extract_frames(
        self, 
        video_path: str, 
        interval: float = 1.0,
        max_frames: Optional[int] = None,
        save_dir: Optional[str] = None
    ) -> List[Tuple[float, np.ndarray]]:
        """
        ë¹„ë””ì˜¤ íŒŒì¼ì—ì„œ ì¼ì • ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ ì¶”ì¶œ
        
        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            interval: í”„ë ˆì„ ì¶”ì¶œ ê°„ê²© (ì´ˆ)
            max_frames: ìµœëŒ€ ì¶”ì¶œ í”„ë ˆì„ ìˆ˜ (Noneì´ë©´ ì œí•œ ì—†ìŒ)
            save_dir: í”„ë ˆì„ ì €ì¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ)
        
        Returns:
            [(timestamp, frame), ...] ë¦¬ìŠ¤íŠ¸
        """
        if not Path(video_path).exists():
            raise FileNotFoundError(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {video_path}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise RuntimeError(f"ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŒ: {video_path}")
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_interval = int(fps * interval)
        
        frames = []
        frame_count = 0
        extracted_count = 0
        
        if save_dir:
            Path(save_dir).mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“¹ ë¹„ë””ì˜¤ ë¶„ì„ ì‹œì‘: {video_path}")
        print(f"   FPS: {fps}, ì¶”ì¶œ ê°„ê²©: {interval}ì´ˆ ({frame_interval} í”„ë ˆì„ë§ˆë‹¤)")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # ì§€ì •ëœ ê°„ê²©ë§ˆë‹¤ í”„ë ˆì„ ì¶”ì¶œ
            if frame_count % frame_interval == 0:
                timestamp = frame_count / fps
                frames.append((timestamp, frame))
                
                if save_dir:
                    filename = f"frame_{extracted_count:04d}_{timestamp:.2f}s.jpg"
                    filepath = Path(save_dir) / filename
                    cv2.imwrite(str(filepath), frame)
                
                extracted_count += 1
                print(f"   âœ… í”„ë ˆì„ {extracted_count} ì¶”ì¶œ (ì‹œê°„: {timestamp:.2f}ì´ˆ)")
                
                # ìµœëŒ€ í”„ë ˆì„ ìˆ˜ ë„ë‹¬ ì‹œ ì¤‘ë‹¨
                if max_frames and extracted_count >= max_frames:
                    break
            
            frame_count += 1
        
        cap.release()
        print(f"âœ… ì´ {extracted_count}ê°œ í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ")
        
        return frames
    
    def extract_key_frames(
        self,
        video_path: str,
        threshold: float = 30.0,
        max_frames: Optional[int] = None,
        save_dir: Optional[str] = None
    ) -> List[Tuple[float, np.ndarray]]:
        """
        ì¥ë©´ ë³€í™”ê°€ í° í‚¤í”„ë ˆì„ë§Œ ì¶”ì¶œ
        
        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            threshold: ì¥ë©´ ë³€í™” ì„ê³„ê°’ (ë†’ì„ìˆ˜ë¡ ë³€í™”ê°€ í° í”„ë ˆì„ë§Œ ì¶”ì¶œ)
            max_frames: ìµœëŒ€ ì¶”ì¶œ í”„ë ˆì„ ìˆ˜
            save_dir: í”„ë ˆì„ ì €ì¥ ë””ë ‰í† ë¦¬
        
        Returns:
            [(timestamp, frame), ...] ë¦¬ìŠ¤íŠ¸
        """
        if not Path(video_path).exists():
            raise FileNotFoundError(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {video_path}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise RuntimeError(f"ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŒ: {video_path}")
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        frames = []
        frame_count = 0
        extracted_count = 0
        prev_frame_gray = None
        
        if save_dir:
            Path(save_dir).mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“¹ í‚¤í”„ë ˆì„ ì¶”ì¶œ ì‹œì‘: {video_path}")
        print(f"   ì„ê³„ê°’: {threshold}")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # ì²« í”„ë ˆì„ì€ ë¬´ì¡°ê±´ ì¶”ê°€
            if prev_frame_gray is None:
                timestamp = frame_count / fps
                frames.append((timestamp, frame))
                extracted_count += 1
                
                if save_dir:
                    filename = f"keyframe_{extracted_count:04d}_{timestamp:.2f}s.jpg"
                    filepath = Path(save_dir) / filename
                    cv2.imwrite(str(filepath), frame)
                
                print(f"   âœ… í‚¤í”„ë ˆì„ {extracted_count} ì¶”ì¶œ (ì‹œê°„: {timestamp:.2f}ì´ˆ)")
            else:
                # ì´ì „ í”„ë ˆì„ê³¼ì˜ ì°¨ì´ ê³„ì‚°
                diff = cv2.absdiff(prev_frame_gray, gray)
                mean_diff = np.mean(diff)
                
                # ì„ê³„ê°’ì„ ë„˜ìœ¼ë©´ í‚¤í”„ë ˆì„ìœ¼ë¡œ ì¶”ì¶œ
                if mean_diff > threshold:
                    timestamp = frame_count / fps
                    frames.append((timestamp, frame))
                    extracted_count += 1
                    
                    if save_dir:
                        filename = f"keyframe_{extracted_count:04d}_{timestamp:.2f}s.jpg"
                        filepath = Path(save_dir) / filename
                        cv2.imwrite(str(filepath), frame)
                    
                    print(f"   âœ… í‚¤í”„ë ˆì„ {extracted_count} ì¶”ì¶œ (ì‹œê°„: {timestamp:.2f}ì´ˆ, ë³€í™”ë„: {mean_diff:.2f})")
                    
                    # ìµœëŒ€ í”„ë ˆì„ ìˆ˜ ë„ë‹¬ ì‹œ ì¤‘ë‹¨
                    if max_frames and extracted_count >= max_frames:
                        break
            
            prev_frame_gray = gray
            frame_count += 1
        
        cap.release()
        print(f"âœ… ì´ {extracted_count}ê°œ í‚¤í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ")
        
        return frames


class VideoMonitor:
    """ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, camera_id: int = 0):
        """
        ë¹„ë””ì˜¤ ëª¨ë‹ˆí„° ì´ˆê¸°í™”
        
        Args:
            camera_id: ì¹´ë©”ë¼ ID (ê¸°ë³¸ê°’: 0)
        """
        self.camera_id = camera_id
        self.is_monitoring = False
        self.cap = None
        self.monitoring_thread = None
        self.frame_callback = None
        self.frame_interval = 1.0  # ì½œë°± í˜¸ì¶œ ê°„ê²© (ì´ˆ)
    
    def start_monitoring(
        self,
        on_frame_callback: Callable[[np.ndarray, float], None],
        frame_interval: float = 1.0,
        show_preview: bool = False
    ):
        """
        ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        
        Args:
            on_frame_callback: í”„ë ˆì„ ì²˜ë¦¬ ì½œë°± í•¨ìˆ˜ callback(frame, timestamp)
            frame_interval: ì½œë°± í˜¸ì¶œ ê°„ê²© (ì´ˆ)
            show_preview: í”„ë¦¬ë·° ì°½ í‘œì‹œ ì—¬ë¶€
        """
        if self.is_monitoring:
            print("âš ï¸  ì´ë¯¸ ëª¨ë‹ˆí„°ë§ ì¤‘ì…ë‹ˆë‹¤")
            return
        
        self.frame_callback = on_frame_callback
        self.frame_interval = frame_interval
        self.is_monitoring = True
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ëª¨ë‹ˆí„°ë§ ì‹¤í–‰
        self.monitoring_thread = threading.Thread(
            target=self._monitoring_loop,
            args=(show_preview,),
            daemon=True
        )
        self.monitoring_thread.start()
        
        print(f"âœ… ë¹„ë””ì˜¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ì¹´ë©”ë¼ {self.camera_id})")
    
    def _monitoring_loop(self, show_preview: bool):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„ (ë‚´ë¶€ ë©”ì„œë“œ)"""
        self.cap = cv2.VideoCapture(self.camera_id)
        
        if not self.cap.isOpened():
            print(f"âŒ ì¹´ë©”ë¼ {self.camera_id}ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            self.is_monitoring = False
            return
        
        fps = self.cap.get(cv2.CAP_PROP_FPS) or 30.0
        frame_skip = int(fps * self.frame_interval)
        frame_count = 0
        start_time = time.time()
        
        print(f"ğŸ“¹ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (FPS: {fps}, ê°„ê²©: {self.frame_interval}ì´ˆ)")
        
        while self.is_monitoring:
            ret, frame = self.cap.read()
            
            if not ret:
                print("âš ï¸  í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
                break
            
            # ì§€ì •ëœ ê°„ê²©ë§ˆë‹¤ ì½œë°± í˜¸ì¶œ
            if frame_count % frame_skip == 0:
                timestamp = time.time() - start_time
                
                try:
                    if self.frame_callback:
                        self.frame_callback(frame, timestamp)
                except Exception as e:
                    print(f"âŒ ì½œë°± ì˜¤ë¥˜: {e}")
            
            # í”„ë¦¬ë·° ì°½ í‘œì‹œ
            if show_preview:
                cv2.imshow(f'Video Monitor (Camera {self.camera_id})', frame)
                
                # 'q' í‚¤ë¡œ ì¢…ë£Œ
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    print("\nğŸ‘‹ ì‚¬ìš©ìê°€ ëª¨ë‹ˆí„°ë§ì„ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤")
                    break
            
            frame_count += 1
        
        self.cap.release()
        if show_preview:
            cv2.destroyAllWindows()
        
        print("âœ… ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ")
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        if not self.is_monitoring:
            print("âš ï¸  ëª¨ë‹ˆí„°ë§ ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤")
            return
        
        self.is_monitoring = False
        
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=3.0)
        
        print("âœ… ëª¨ë‹ˆí„°ë§ ì¤‘ì§€ë¨")
    
    def capture_current_frame(self) -> Optional[np.ndarray]:
        """
        í˜„ì¬ í”„ë ˆì„ ìº¡ì²˜ (ëª¨ë‹ˆí„°ë§ ì¤‘ì¼ ë•Œ)
        
        Returns:
            í˜„ì¬ í”„ë ˆì„ (numpy array) ë˜ëŠ” None
        """
        if not self.is_monitoring or not self.cap or not self.cap.isOpened():
            print("âŒ ëª¨ë‹ˆí„°ë§ ì¤‘ì´ ì•„ë‹ˆê±°ë‚˜ ì¹´ë©”ë¼ê°€ ì—´ë ¤ìˆì§€ ì•ŠìŠµë‹ˆë‹¤")
            return None
        
        ret, frame = self.cap.read()
        if ret:
            return frame
        else:
            print("âŒ í”„ë ˆì„ ìº¡ì²˜ ì‹¤íŒ¨")
            return None


class MotionDetector:
    """ì›€ì§ì„ ê°ì§€ê¸°"""
    
    def __init__(self, threshold: float = 25.0, min_area: int = 500):
        """
        ì›€ì§ì„ ê°ì§€ê¸° ì´ˆê¸°í™”
        
        Args:
            threshold: ì›€ì§ì„ ê°ì§€ ì„ê³„ê°’
            min_area: ìµœì†Œ ì›€ì§ì„ ì˜ì—­ í¬ê¸°
        """
        self.threshold = threshold
        self.min_area = min_area
        self.prev_frame = None
    
    def detect_motion(self, frame: np.ndarray) -> Tuple[bool, List[Tuple[int, int, int, int]]]:
        """
        í”„ë ˆì„ì—ì„œ ì›€ì§ì„ ê°ì§€
        
        Args:
            frame: í˜„ì¬ í”„ë ˆì„
        
        Returns:
            (ì›€ì§ì„ ê°ì§€ ì—¬ë¶€, ì›€ì§ì„ ì˜ì—­ ë¦¬ìŠ¤íŠ¸ [(x, y, w, h), ...])
        """
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜ ë° ë¸”ëŸ¬ ì²˜ë¦¬
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        gray = cv2.GaussianBlur(gray, (21, 21), 0)
        
        # ì²« í”„ë ˆì„ ì €ì¥
        if self.prev_frame is None:
            self.prev_frame = gray
            return False, []
        
        # í”„ë ˆì„ ì°¨ì´ ê³„ì‚°
        frame_delta = cv2.absdiff(self.prev_frame, gray)
        thresh = cv2.threshold(frame_delta, self.threshold, 255, cv2.THRESH_BINARY)[1]
        thresh = cv2.dilate(thresh, None, iterations=2)
        
        # ìœ¤ê³½ì„  ì°¾ê¸°
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        motion_areas = []
        for contour in contours:
            if cv2.contourArea(contour) < self.min_area:
                continue
            
            (x, y, w, h) = cv2.boundingRect(contour)
            motion_areas.append((x, y, w, h))
        
        self.prev_frame = gray
        
        has_motion = len(motion_areas) > 0
        return has_motion, motion_areas
    
    def draw_motion_boxes(self, frame: np.ndarray, motion_areas: List[Tuple[int, int, int, int]]) -> np.ndarray:
        """
        ì›€ì§ì„ ì˜ì—­ì— ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        
        Args:
            frame: ì›ë³¸ í”„ë ˆì„
            motion_areas: ì›€ì§ì„ ì˜ì—­ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ë°•ìŠ¤ê°€ ê·¸ë ¤ì§„ í”„ë ˆì„
        """
        result = frame.copy()
        
        for (x, y, w, h) in motion_areas:
            cv2.rectangle(result, (x, y), (x + w, y + h), (0, 255, 0), 2)
        
        # ì›€ì§ì„ ê°ì§€ í…ìŠ¤íŠ¸
        if motion_areas:
            cv2.putText(result, f"Motion Detected ({len(motion_areas)} areas)", 
                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
        
        return result


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("=" * 70)
    print("ğŸ“¹ ë¹„ë””ì˜¤ ìº¡ì²˜ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    # 1. ì›¹ìº  ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸
    print("\n1ï¸âƒ£ ì›¹ìº  ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸ (5ì´ˆ)")
    
    def on_frame(frame, timestamp):
        print(f"   í”„ë ˆì„ ìˆ˜ì‹ : {timestamp:.2f}ì´ˆ, í¬ê¸°: {frame.shape}")
    
    monitor = VideoMonitor(camera_id=0)
    monitor.start_monitoring(on_frame_callback=on_frame, frame_interval=1.0)
    
    time.sleep(5)
    monitor.stop_monitoring()
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
