"""
KNU-MM í†µí•© ë©€í‹°ëª¨ë‹¬ ê´€ì œ ì‹œìŠ¤í…œ - ë©”ì¸ ì§„ì…ì 

ëª¨ë“  ëª¨ë“ˆ(YOLO, MicArray, STT, ContextLLM, ServerReporter)ì„ í•˜ë‚˜ì˜
íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ í†µí•© ì‹¤í–‰í•©ë‹ˆë‹¤.

ë°ì´í„° íë¦„:
    â”Œâ”€â”€â”€ ì˜ìƒ â”€â”€â”€â”     â”Œâ”€â”€â”€ ìŒì„± â”€â”€â”€â”
    â”‚ CCTV/ì›¹ìº   â”‚     â”‚ MicArray   â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚
     YOLO íƒì§€          DOA ë°©í–¥ ê°ì§€
          â”‚              STT í…ìŠ¤íŠ¸ ë³€í™˜
          â”‚                  â”‚
          â””â”€â”€â”€â”€â”€ ContextLLM â”€â”˜
                (í†µí•© ë¶„ì„)
                     â”‚
              OpenCV í†µí•© í™”ë©´

ì‚¬ìš©ë²•:
    cd integrated_system
    python main.py                          # ê¸°ë³¸ ì‹¤í–‰
    python main.py --no-mic                 # ë§ˆì´í¬ ì–´ë ˆì´ ì—†ì´
    python main.py --no-stt                 # ìŒì„± ì¸ì‹ ì—†ì´
    python main.py --no-llm                 # LLM ë¶„ì„ ì—†ì´
    python main.py --no-display             # í™”ë©´ í‘œì‹œ ì—†ì´
    python main.py --config custom.yaml     # ì»¤ìŠ¤í…€ ì„¤ì •
"""

import os
import sys
import cv2
import time
import math
import signal
import logging
import argparse
import numpy as np
from pathlib import Path

import yaml
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from integrated_system.core.event_bus import EventBus, Event
from integrated_system.core.orchestrator import Orchestrator
from integrated_system.modules.stream_manager import SharedStreamManager
from integrated_system.modules.ptz_controller import UnifiedPTZController
from integrated_system.modules.yolo_detection import YOLODetectionModule
from integrated_system.modules.mic_array import MicArrayModule
from integrated_system.modules.stt_module import STTModule
from integrated_system.modules.context_llm import ContextLLMModule
from integrated_system.modules.server_reporter import ServerReporterModule


def load_config(config_path: str) -> dict:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ (YAML + í™˜ê²½ë³€ìˆ˜ ì˜¤ë²„ë¼ì´ë“œ)"""
    config = {}
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f) or {}

    # .env ë¡œë“œ
    env_path = PROJECT_ROOT / ".env"
    if env_path.exists():
        load_dotenv(env_path)

    contextllm_env = PROJECT_ROOT / "contextllm" / "config" / ".env"
    if contextllm_env.exists():
        load_dotenv(contextllm_env)

    # í™˜ê²½ë³€ìˆ˜ ì˜¤ë²„ë¼ì´ë“œ
    cam = config.setdefault("camera", {})
    cam["rtsp_url"] = os.getenv("RTSP_URL", cam.get("rtsp_url", ""))
    cam["ip"] = os.getenv("CAMERA_IP", cam.get("ip", ""))
    cam["port"] = int(os.getenv("CAMERA_PORT", cam.get("port", 80)))
    cam["user"] = os.getenv("CAMERA_USER", cam.get("user", ""))
    cam["password"] = os.getenv("CAMERA_PASSWORD", cam.get("password", ""))

    return config


def setup_logging(config: dict) -> None:
    """ë¡œê¹… ì„¤ì •"""
    log_cfg = config.get("logging", {})
    level = getattr(logging, log_cfg.get("level", "INFO").upper(), logging.INFO)

    handlers = [logging.StreamHandler()]
    log_file = log_cfg.get("file", "")
    if log_file:
        handlers.append(logging.FileHandler(log_file, encoding='utf-8'))

    logging.basicConfig(
        level=level,
        format="%(asctime)s [%(name)-18s] %(levelname)-7s %(message)s",
        datefmt="%H:%M:%S",
        handlers=handlers,
    )


def build_system(config: dict, args) -> tuple:
    """
    ì‹œìŠ¤í…œ ë¹Œë“œ - ëª¨ë“  ëª¨ë“ˆì„ ìƒì„±í•˜ê³  ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì— ë“±ë¡
    
    Returns:
        (orchestrator, event_bus, stream_manager, ptz, mic_module, stt_module)
    """
    # 1. ì´ë²¤íŠ¸ ë²„ìŠ¤ ìƒì„±
    event_bus = EventBus(max_workers=4, async_mode=True)

    # 2. ê³µìœ  ë¦¬ì†ŒìŠ¤ ìƒì„±
    cam_cfg = config.get("camera", {})
    rtsp_url = cam_cfg.get("rtsp_url", 0)
    
    # ìˆ«ì ë¬¸ìì—´ì„ intë¡œ ë³€í™˜ (0=ì›¹ìº , 1=ë‘ë²ˆì§¸ ì¹´ë©”ë¼ ë“±)
    if isinstance(rtsp_url, str) and rtsp_url.isdigit():
        rtsp_url = int(rtsp_url)
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: rtsp_urlì´ Noneì´ê±°ë‚˜ -1ì´ë©´ í…ŒìŠ¤íŠ¸ ë¹„ë””ì˜¤ ì‚¬ìš©
    if rtsp_url is None or rtsp_url == -1:
        test_video = Path(__file__).parent / "test_video.mp4"
        if test_video.exists():
            rtsp_url = str(test_video)
            logging.warning(f"âš ï¸  í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {test_video.name} ì‚¬ìš©")
        else:
            logging.warning("âš ï¸  í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ë”ë¯¸ ìŠ¤íŠ¸ë¦¼ ì‚¬ìš© (ì˜ìƒ ì—†ìŒ)")
            # ë”ë¯¸ URL - streamì´ None ë°˜í™˜í•  ê²ƒ
            rtsp_url = "test://dummy"
    
    stream = SharedStreamManager(rtsp_url)
    stream.start()

    # PTZ ì»¨íŠ¸ë¡¤ëŸ¬
    ptz_cfg = config.get("ptz", {})
    ptz = UnifiedPTZController({
        "camera_ip": cam_cfg.get("ip", ""),
        "camera_port": cam_cfg.get("port", 80),
        "camera_user": cam_cfg.get("user", ""),
        "camera_password": cam_cfg.get("password", ""),
        "control_mode": ptz_cfg.get("control_mode", "onvif"),
    })
    ptz.initialize()

    # 3. ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ìƒì„±
    orch = Orchestrator(event_bus)

    # 4. ëª¨ë“ˆ ë“±ë¡
    # --- YOLO ---
    yolo_cfg = config.get("yolo", {})
    if yolo_cfg.get("enabled", True) and not args.no_yolo:
        yolo_module = YOLODetectionModule(
            event_bus=event_bus,
            ptz=ptz,
            model_path=yolo_cfg.get("model_path", "yolov8n.pt"),
            confidence=yolo_cfg.get("confidence", 0.3),
            pid_kp=ptz_cfg.get("pid_kp", 0.4),
            dead_zone=ptz_cfg.get("dead_zone_pixels", 50),
            patrol_speed=ptz_cfg.get("patrol_speed", 0.2),
            target_classes=yolo_cfg.get("target_classes"),
        )
        orch.register(yolo_module)

    # --- MicArray ---
    mic_cfg = config.get("mic_array", {})
    mic_module = None
    if mic_cfg.get("enabled", True) and not args.no_mic:
        mic_module = MicArrayModule(
            event_bus=event_bus,
            ptz=ptz,
            agc_max_gain=mic_cfg.get("agc_max_gain", 15.0),
            vad_threshold=mic_cfg.get("vad_threshold", 10.0),
            confidence_threshold=mic_cfg.get("confidence_threshold", 0.6),
            zenith_confidence=mic_cfg.get("zenith_confidence", 0.4),
            zenith_gain=mic_cfg.get("zenith_gain", 10.0),
            history_size=mic_cfg.get("history_size", 10),
        )
        if orch.register(mic_module):
            mic_module.start_monitoring()

    # --- STT (Speech-to-Text) ---
    stt_cfg = config.get("stt", {})
    stt_module = None
    if stt_cfg.get("enabled", True) and not args.no_stt:
        stt_module = STTModule(
            event_bus=event_bus,
            language=stt_cfg.get("language", "ko-KR"),
            energy_threshold=stt_cfg.get("energy_threshold", 400),
            pause_threshold=stt_cfg.get("pause_threshold", 3.0),
            phrase_time_limit=stt_cfg.get("phrase_time_limit", 15.0),
            dynamic_threshold=stt_cfg.get("dynamic_threshold", True),
        )
        if orch.register(stt_module):
            stt_module.start_listening()

    # --- ContextLLM ---
    llm_cfg = config.get("context_llm", {})
    if llm_cfg.get("enabled", True) and not args.no_llm:
        llm_module = ContextLLMModule(
            event_bus=event_bus,
            model=llm_cfg.get("model", "gpt-4o-mini"),
            config_path=llm_cfg.get("config_path", "") or None,
        )
        orch.register(llm_module)

    # --- ServerReporter ---
    srv_cfg = config.get("server", {})
    if srv_cfg.get("enabled", False):
        server_module = ServerReporterModule(
            event_bus=event_bus,
            server_url=srv_cfg.get("url", ""),
            timeout=srv_cfg.get("timeout", 2.0),
        )
        orch.register(server_module)

    # 5. íŒŒì´í”„ë¼ì¸ ì •ì˜
    # â˜… ì£¼ìš” ë³€ê²½: context_llmì€ ì¡°ê±´ ì—†ì´ í•­ìƒ ì‹¤í–‰
    #   (ë‚´ë¶€ì—ì„œ ì‚¬ëŒ ê°ì§€ OR ìŒì„± í…ìŠ¤íŠ¸ ìˆì„ ë•Œë§Œ ë¶„ì„ ì‹¤í–‰)
    orch.define_pipeline("security", [
        {"module": "yolo"},
        {"module": "context_llm"},
        {"module": "server_reporter"},
    ])

    orch.define_pipeline("full_analysis", [
        {"module": "yolo"},
        {"module": "context_llm"},
        {"module": "server_reporter"},
    ])

    return orch, event_bus, stream, ptz, mic_module, stt_module


def run_main_loop(orch: Orchestrator, stream: SharedStreamManager, config: dict, args, stt_module=None) -> None:
    """
    ë©”ì¸ ë£¨í”„ - í”„ë ˆì„ ìˆ˜ì‹  â†’ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ â†’ í†µí•© ì‹œê°í™”

    í™”ë©´ ë ˆì´ì•„ì›ƒ:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  FPS | Mode | Pipeline | ëª¨ë“ˆ ìƒíƒœ           â”‚  ìƒë‹¨ ë°”
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                              â”‚
    â”‚          ì˜ìƒ í”„ë ˆì„ (YOLO ë°•ìŠ¤)              â”‚
    â”‚                                              â”‚
    â”‚  â”Œâ”€DOAâ”€â”€â”                                    â”‚
    â”‚  â”‚  â—€   â”‚  (ë§ˆì´í¬ ë°©í–¥ í‘œì‹œ)                â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  ğŸ¤ STT: "ì¸ì‹ëœ í…ìŠ¤íŠ¸"                     â”‚  í•˜ë‹¨ íŒ¨ë„
    â”‚  ğŸ¤– LLM: [ê¸´ê¸‰ë„] ìƒí™© ì„¤ëª…                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    display_cfg = config.get("display", {})
    pipeline_cfg = config.get("pipeline", {})
    
    show_display = display_cfg.get("show_opencv", True) and not args.no_display
    window_name = display_cfg.get("window_name", "KNU-MM Integrated System")
    process_every = pipeline_cfg.get("process_every_n_frames", 3)
    pipeline_name = pipeline_cfg.get("default", "security")

    if show_display:
        cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)

    frame_count = 0
    fps = 0.0
    fps_frames = 0
    fps_time = time.perf_counter()

    # í‘œì‹œìš© ìƒíƒœ ë³€ìˆ˜
    display_stt_text = ""
    display_stt_time = 0.0
    display_llm = {}
    display_doa_angle = -1

    logger = logging.getLogger("MainLoop")
    logger.info(f"â”â”â” ë©”ì¸ ë£¨í”„ ì‹œì‘ (íŒŒì´í”„ë¼ì¸: {pipeline_name}, ë§¤ {process_every}í”„ë ˆì„) â”â”â”")

    # ëª¨ë“ˆ ìƒíƒœ ì¶œë ¥
    for name, status in orch.list_modules().items():
        icon = "âœ…" if status["initialized"] else "â›”"
        logger.info(f"  {icon} {name}: {'í™œì„±' if status['initialized'] else 'ë¹„í™œì„±'}")

    # STT í…ìŠ¤íŠ¸ ìˆ˜ì‹ ìš© ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
    def on_stt_display(event: Event):
        nonlocal display_stt_text, display_stt_time
        display_stt_text = event.data.get("text", "")
        display_stt_time = time.time()

    def on_doa_display(event: Event):
        nonlocal display_doa_angle
        display_doa_angle = event.data.get("sector_angle", -1)

    event_bus = orch.event_bus
    event_bus.subscribe("stt.text_recognized", on_stt_display)
    event_bus.subscribe("mic.doa_detected", on_doa_display)

    try:
        while True:
            frame = stream.get_frame()
            if frame is None:
                time.sleep(0.01)
                continue

            frame_count += 1
            fps_frames += 1

            # FPS ê³„ì‚°
            now = time.perf_counter()
            if now - fps_time >= 1.0:
                fps = fps_frames / (now - fps_time)
                fps_frames = 0
                fps_time = now

            # Ní”„ë ˆì„ë§ˆë‹¤ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            results = {}
            if frame_count % process_every == 0:
                results = orch.run_pipeline(pipeline_name, {
                    "frame": frame,
                    "timestamp": time.time(),
                    "frame_count": frame_count,
                })

            # LLM ê²°ê³¼ ê°±ì‹ 
            llm_result = results.get("context_llm", {})
            if llm_result.get("analyzed"):
                display_llm = llm_result

            # â”€â”€â”€ í†µí•© ì‹œê°í™” â”€â”€â”€
            if show_display:
                display_frame = frame.copy()
                h, w = display_frame.shape[:2]

                # â”€â”€ 1. YOLO ë°•ìŠ¤ ê·¸ë¦¬ê¸° â”€â”€
                yolo_mod = orch.get_module("yolo")
                yolo_result = results.get("yolo", {})
                if yolo_mod and yolo_result.get("objects"):
                    display_frame = yolo_mod.get_annotated_frame(display_frame, yolo_result["objects"])

                # â”€â”€ 2. ìƒë‹¨ ì •ë³´ ë°” (ë°˜íˆ¬ëª… ê²€ì •) â”€â”€
                overlay = display_frame.copy()
                cv2.rectangle(overlay, (0, 0), (w, 80), (0, 0, 0), -1)
                cv2.addWeighted(overlay, 0.7, display_frame, 0.3, 0, display_frame)

                # FPS
                cv2.putText(display_frame, f"FPS: {fps:.1f}", (10, 25),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

                # YOLO ëª¨ë“œ
                yolo_mode = yolo_result.get("mode", "N/A")
                cv2.putText(display_frame, f"Mode: {yolo_mode}", (10, 55),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0, 255, 255), 2)

                # íŒŒì´í”„ë¼ì¸ í‘œì‹œ
                cv2.putText(display_frame, f"Pipeline: {pipeline_name}", (180, 25),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.55, (200, 200, 200), 1)

                # ëª¨ë“ˆ ìƒíƒœ ì¸ë””ì¼€ì´í„° (ìš°ìƒë‹¨)
                module_status_x = w - 280
                statuses = [
                    ("YOLO", orch.get_module("yolo")),
                    ("MIC", orch.get_module("mic_array")),
                    ("STT", orch.get_module("stt")),
                    ("LLM", orch.get_module("context_llm")),
                ]
                for i, (label, mod) in enumerate(statuses):
                    sx = module_status_x + i * 70
                    is_active = mod and mod.is_ready
                    color = (0, 255, 0) if is_active else (80, 80, 80)
                    cv2.putText(display_frame, label, (sx, 25),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 1)
                    # ìƒíƒœ ì 
                    cv2.circle(display_frame, (sx + 15, 40), 5, color, -1)

                # â”€â”€ 3. DOA ë°©í–¥ í‘œì‹œ (ì¢Œí•˜ë‹¨ ë¯¸ë‹ˆ ì»´í¼ìŠ¤) â”€â”€
                mic_mod = orch.get_module("mic_array")
                if mic_mod and mic_mod.is_ready and display_doa_angle >= 0:
                    _draw_doa_compass(display_frame, display_doa_angle, x=60, y=h - 80, radius=40)
                elif display_doa_angle >= 0:
                    # DOA í…ìŠ¤íŠ¸ë§Œ í‘œì‹œ (ë§ˆì´í¬ ëª¨ë“ˆ ì—†ì–´ë„ ì´ë²¤íŠ¸ ìˆ˜ì‹  ê°€ëŠ¥)
                    cv2.putText(display_frame, f"DOA: {display_doa_angle} deg",
                                (10, h - 90), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 200, 0), 1)

                # â”€â”€ 4. í•˜ë‹¨ íŒ¨ë„ (STT + LLM ê²°ê³¼) â”€â”€
                panel_h = 100
                panel_y = h - panel_h

                # ë°˜íˆ¬ëª… í•˜ë‹¨ ë°°ê²½
                overlay2 = display_frame.copy()
                cv2.rectangle(overlay2, (0, panel_y), (w, h), (0, 0, 0), -1)
                cv2.addWeighted(overlay2, 0.75, display_frame, 0.25, 0, display_frame)

                # STT í…ìŠ¤íŠ¸ í‘œì‹œ (10ì´ˆê°„ ìœ ì§€)
                stt_text_display = display_stt_text if (time.time() - display_stt_time < 10) else ""
                if stt_text_display:
                    # ë§ˆì´í¬ ì•„ì´ì½˜ + í…ìŠ¤íŠ¸
                    cv2.putText(display_frame, "[MIC]", (10, panel_y + 25),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 200, 255), 1)

                    # í…ìŠ¤íŠ¸ê°€ ê¸¸ë©´ ì˜ë¼ì„œ í‘œì‹œ
                    max_chars = w // 12
                    display_text = stt_text_display[:max_chars]
                    if len(stt_text_display) > max_chars:
                        display_text += "..."
                    cv2.putText(display_frame, display_text, (70, panel_y + 25),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.55, (255, 255, 255), 1)
                else:
                    cv2.putText(display_frame, "[MIC] (waiting...)", (10, panel_y + 25),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 100, 100), 1)

                # LLM ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                if display_llm.get("analyzed"):
                    priority = display_llm.get("priority", "LOW")
                    is_emergency = display_llm.get("is_emergency", False)
                    urgency = display_llm.get("urgency", "")
                    situation = display_llm.get("situation_type", "")
                    speech_text = display_llm.get("speech_text", "")

                    # ê¸´ê¸‰ë„ì— ë”°ë¥¸ ìƒ‰ìƒ
                    if is_emergency:
                        llm_color = (0, 0, 255)   # ë¹¨ê°•
                        priority_icon = "[EMERGENCY]"
                    elif priority in ("CRITICAL", "HIGH"):
                        llm_color = (0, 128, 255)  # ì£¼í™©
                        priority_icon = f"[{priority}]"
                    elif priority == "MEDIUM":
                        llm_color = (0, 255, 255)  # ë…¸ë‘
                        priority_icon = f"[{priority}]"
                    else:
                        llm_color = (0, 255, 0)    # ì´ˆë¡
                        priority_icon = f"[{priority}]"

                    cv2.putText(display_frame, "[LLM]", (10, panel_y + 55),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 100, 255), 1)
                    cv2.putText(display_frame, f"{priority_icon} {urgency}", (70, panel_y + 55),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.55, llm_color, 2)

                    # ìƒí™© ìš”ì•½ (2ë²ˆì§¸ ì¤„)
                    if situation:
                        cv2.putText(display_frame, f"Situation: {situation}", (70, panel_y + 80),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, (200, 200, 200), 1)

                    # ê¸´ê¸‰ ì‹œ í™”ë©´ í…Œë‘ë¦¬ ê¹œë¹¡ì„
                    if is_emergency and int(time.time() * 2) % 2 == 0:
                        cv2.rectangle(display_frame, (0, 0), (w - 1, h - 1), (0, 0, 255), 4)
                else:
                    cv2.putText(display_frame, "[LLM] (no analysis yet)", (10, panel_y + 55),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 100, 100), 1)

                # êµ¬ë¶„ì„ 
                cv2.line(display_frame, (0, panel_y), (w, panel_y), (80, 80, 80), 1)
                cv2.line(display_frame, (0, 80), (w, 80), (80, 80, 80), 1)

                cv2.imshow(window_name, display_frame)

                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('p'):
                    # íŒŒì´í”„ë¼ì¸ ì „í™˜
                    pipeline_name = "full_analysis" if pipeline_name == "security" else "security"
                    logger.info(f"íŒŒì´í”„ë¼ì¸ ì „í™˜ â†’ {pipeline_name}")

    except KeyboardInterrupt:
        logger.info("\nì‚¬ìš©ì ì¤‘ë‹¨ (Ctrl+C)")


def _draw_doa_compass(frame, angle_deg: float, x: int, y: int, radius: int = 40):
    """
    DOA ë°©í–¥ ë¯¸ë‹ˆ ì»´í¼ìŠ¤ ê·¸ë¦¬ê¸°

    Args:
        frame: OpenCV í”„ë ˆì„
        angle_deg: DOA ê°ë„ (0=ì „ë°©, ì‹œê³„ë°©í–¥)
        x, y: ì»´í¼ìŠ¤ ì¤‘ì‹¬ ì¢Œí‘œ
        radius: ì»´í¼ìŠ¤ ë°˜ì§€ë¦„
    """
    # ë°°ê²½ ì› (ë°˜íˆ¬ëª…)
    overlay = frame.copy()
    cv2.circle(overlay, (x, y), radius + 5, (0, 0, 0), -1)
    cv2.addWeighted(overlay, 0.5, frame, 0.5, 0, frame)

    # ì™¸ê³½ ì›
    cv2.circle(frame, (x, y), radius, (100, 100, 100), 1)

    # ë°©ìœ„ í‘œì‹œ
    cv2.putText(frame, "N", (x - 5, y - radius - 5),
                cv2.FONT_HERSHEY_SIMPLEX, 0.3, (150, 150, 150), 1)

    # ë°©í–¥ í™”ì‚´í‘œ (angle_deg â†’ ë¼ë””ì•ˆ, OpenCV ì¢Œí‘œê³„)
    # DOA 0Â° = ì „ë°©(ìœ„), ì‹œê³„ë°©í–¥ ì¦ê°€
    rad = math.radians(angle_deg - 90)  # OpenCV: 0Â°=ì˜¤ë¥¸ìª½ì´ë¯€ë¡œ -90
    end_x = int(x + radius * 0.8 * math.cos(rad))
    end_y = int(y + radius * 0.8 * math.sin(rad))

    cv2.arrowedLine(frame, (x, y), (end_x, end_y), (0, 200, 255), 2, tipLength=0.3)

    # ê°ë„ í…ìŠ¤íŠ¸
    cv2.putText(frame, f"{int(angle_deg)}", (x - 12, y + radius + 15),
                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 200, 255), 1)


def main():
    parser = argparse.ArgumentParser(
        description='KNU-MM í†µí•© ë©€í‹°ëª¨ë‹¬ ê´€ì œ ì‹œìŠ¤í…œ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python main.py                      # ì „ì²´ ëª¨ë“ˆ ì‹¤í–‰
  python main.py --no-mic             # ë§ˆì´í¬ ì—†ì´ (YOLO + STT + LLM)
  python main.py --no-stt             # ìŒì„± ì¸ì‹ ì—†ì´ (YOLO + LLMë§Œ)
  python main.py --no-llm             # LLM ì—†ì´ (YOLO + ë§ˆì´í¬ë§Œ)
  python main.py --no-display         # í™”ë©´ ì—†ì´ (ì„œë²„ ì „ì†¡ë§Œ)
  python main.py --config my.yaml     # ì»¤ìŠ¤í…€ ì„¤ì •

ì‹¤í–‰ ì¤‘ í‚¤:
  Q: ì¢…ë£Œ
  P: íŒŒì´í”„ë¼ì¸ ì „í™˜ (security â†” full_analysis)
        """
    )
    parser.add_argument('--config', default=str(Path(__file__).parent / 'config.yaml'), help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--no-mic', action='store_true', help='ë§ˆì´í¬ ì–´ë ˆì´ ë¹„í™œì„±í™”')
    parser.add_argument('--no-stt', action='store_true', help='ìŒì„± ì¸ì‹(STT) ë¹„í™œì„±í™”')
    parser.add_argument('--no-llm', action='store_true', help='ContextLLM ë¹„í™œì„±í™”')
    parser.add_argument('--no-yolo', action='store_true', help='YOLO ë¹„í™œì„±í™”')
    parser.add_argument('--no-display', action='store_true', help='OpenCV ë””ìŠ¤í”Œë ˆì´ ë¹„í™œì„±í™”')
    parser.add_argument('--debug', action='store_true', help='DEBUG ë¡œê¹… í™œì„±í™”')

    args = parser.parse_args()

    # ì„¤ì • ë¡œë“œ
    config = load_config(args.config)
    
    if args.debug:
        config.setdefault("logging", {})["level"] = "DEBUG"
    
    setup_logging(config)
    logger = logging.getLogger("Main")

    logger.info("=" * 60)
    logger.info("  KNU-MM í†µí•© ë©€í‹°ëª¨ë‹¬ ê´€ì œ ì‹œìŠ¤í…œ")
    logger.info("  ì‹œê°(YOLO) + ì²­ê°(MicArray+STT) â†’ LLM í†µí•© ë¶„ì„")
    logger.info("=" * 60)

    # ì‹œìŠ¤í…œ ë¹Œë“œ
    orch, event_bus, stream, ptz, mic_module, stt_module = build_system(config, args)

    # Graceful Shutdown
    def signal_handler(sig, frame):
        logger.info("\nì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹ ...")
        if stt_module:
            stt_module.stop_listening()
        orch.shutdown_all()
        stream.release()
        ptz.shutdown()
        cv2.destroyAllWindows()
        sys.exit(0)

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # ë©”ì¸ ë£¨í”„ ì‹¤í–‰
    try:
        run_main_loop(orch, stream, config, args, stt_module)
    finally:
        logger.info("ì‹œìŠ¤í…œ ì¢…ë£Œ ì¤‘...")
        if stt_module:
            stt_module.stop_listening()
        orch.shutdown_all()
        stream.release()
        ptz.shutdown()
        cv2.destroyAllWindows()
        logger.info("ì‹œìŠ¤í…œ ì¢…ë£Œ ì™„ë£Œ")


if __name__ == "__main__":
    main()
